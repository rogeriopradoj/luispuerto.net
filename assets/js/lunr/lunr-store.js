var store = [{
        "title": "First blog post",
        "excerpt":"This is my very blog post… this post should’ve been a way earlier than it is being, but life is as it is.   I was thinking also into change the “featured image” WordPress is offering but I thought that is fine, since we are in America and this is my first post of my American writing period. The photo is New York City if you haven’t notice, so a really good for an opening post from USA.   This blog has several purposes. I think that the first one is to practice my writing in English and make me more fluent in the Shakespeare language. Yes, I need to improve.   Second one, is try to express myself and to put in black and white my experiences, personal and professional ones. I’m trying to write about myself, my opinions, what I like and what I don’t like (or hate), my hobbies and last but not least my profession. Summing it up, what it seemed as a good idea in the moment I wrote about it, so don’t take it too seriously, perhaps my ideas have already evolve in the moment you’re reading about them.   So, what kind of topics can you expect here? Forestry, GIS, remote sensing, LiDAR, science &amp; research, politics, nature, hiking, photography, technology and probably many many other topics that they will come across my mind the future and they don’t come to my mind right now. I usually like to talk and draw comparisons about the countries where I’ve been or lived. So be prepare to read about Spain, Sweden, Finland and Europe in general, and of course USA, specially Oregon where I live now, and NYC, my favorite city in the world.   My style of writing around here is going to be really informal, but I’ll try to keep my grammar and style in shape. I’m here to look for a honest conversation. First with myself and then with you. Yes, you, because if it were a conversation just with myself this would be a monologue and pretty boring, so please comment.   Since English isn’t my native language, I’m sure I’m going to make a lot of mistakes, typos, grammatical horrors and probably my style is going to leave a lot to be desired. However, you’re there to take care of that, you that hero of the grammar and style. I really encourage you to tell me if you see anything wrong in my writing, Please, I’m here to learn.   Returning to the topic of, why I didn’t write earlier? Well, for a lot of reasons. First, I didn’t feel the need, and second, this blog wasn’t ready. Now, today, in this instant, I’m feeling the need to start blogging again and this is good.   Currently, I’m with my wife Olalla, in Corvallis (OR) and we are going to be here at least until September, learning. Learning about forest, English, american life and getting new experiences. Isn’t life made of experiences?   SEE YOU AROUND!   PS1/ You’ll se more New York in September, when we’ll reach there.   PS2/ Edit: I’ve edited this on 15 May, to fix some typos and grammar mistakes.  ","categories": ["Personal"],
        "tags": ["myself","USA"],
        "url": "https://luispuerto.net/blog/2017/05/15/first-blog-post/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/hwijjf7rwopgej1nb4zb_img_3773.jpg"
},{
        "title": "Back to Joensuu",
        "excerpt":"More or less a month ago we came back from US to Finland. There is no so much to tell about his, but just that the return has been a little bit hard. We returned to an apartment that we have to set in motion again after we were 5 month away. And we have to battle the jet-lag, that it’s really something when you return from US to Europe. For me it wasn’t something new, but that doesn’t make it any easy. We were almost for a week out of the game, not every day, but at least with some minor problem. The third day after the landing for me was the worse, and I even have some dizziness and nausea.   All of these was nothing that we didn’t expect, but on top of these was also something else. It was the return to our little street and our little apartment in Joensuu. Don’t get me wrong, we are really and perfectly happy here, and we missed a lot of things while we were in Corvallis (for starters, our privacy, our bed, our TV and entertaining system, our sofa and our kitchen). But, there is no more drama and craziness, and not more great landscape and diverse people, and no more easy understanding of the people surrounding you or spontaneous talking.   We came straight to the Finnish fall / winter without almost any transition in the middle. Well, it’s true that we were in NYC for almost two weeks and with the best of the companies we could have expected. A really unforgettable trip that I hope to write about in the future and that it was a great colophon for our trip. However, the weather there and the landscape was even more south that it was in Corvallis.  It was hot and nice, even when we were in the beginning of September, and instead of mountains there were big valleys made of glass and steel. Life, urban life.   Now, we are almost in the brink of the dark winter here in Finland, kaamos as Finnish people call it. Not the best to be cheerful and positive about life, since probably until end of November or December we aren’t going to see any snow, which give some bright to the lack of light. The problem for me isn’t the reduce amount of hours of light during the day, but the totally lack of Sun. This country is beautiful when the Sun shines in the sky, even when it’s that low like in this period of the year (it creates a stunning light, really), but the clouds really block almost any hope to see Lorenzo (Lorenzo is the given name we give to the Sun in Spain). This is really depressing, even for me that I come from a really rainy region in Spain.   We have to sum up to this, that these last days have been quite something in the Spanish political scene, being almost in the verge of the its greatest political crisis since the 23-F, that attempt of coup d’état Spain suffered little before I was in this world, in 1981. Even the King was on TV, to the astonishment and confusion of some (they thought that Christmas came earlier this year), to the disappointment of others, and the pleasure of many.   This time the culprits weren’t a handful of militar personnel that have a old and cheesy vision of Spain, but a bunch of politicians that doesn’t have an idea of what they were doing and the guts of take aside what separate them and sit on the table to negotiate. Not to talk about defend they position in a referendum and debating and developing ideas and stating facts.   Anyhow, I think we’ll survive and soon I’ll write more.            ","categories": ["Personal"],
        "tags": ["Finland","Joensuu","myself","thoughts"],
        "url": "https://luispuerto.net/blog/2017/10/12/back-to-joensuu/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_0736.jpg"
},{
        "title": "Blade Runner",
        "excerpt":"Last Friday, it was the World Premiere for Blade Runner 2049 so we decided to rewatch the 2019 one on Friday night to have all the details fresh and on Saturday go to watch the continuation of the story thirty years later on our local cinema here in Joensuu. I decided to watch the 2D version because I don’t see the point of paying a plus for something that in my opinion doesn’t add anything to the cinema experience. What is more, sometimes it makes the experience even worse if you have to read subtitles.   Blade Runner (2019) is one of my favorite movies. It’s a really well told story, that mix a little bit of everything and keep you thinking after you watch it, in other words food for thought. It’s a puzzle, full of details that keep the conversation alive during decades after its release and stirs the discussion up. Over the years, I’ve had several debates with my friends about if Deckard is a replicant or not, the meaning of the unicorn dream and the origami figure, the color and glaze of Deckard eyes in comparison with the other replicants’ eyes…  you can go on and on until you finally realize that is a no point conversation because the ambiguity is there to you enjoy it. It makes you to create your own feelings and connect with the story in a unique and personal way.   The interpretation and themes of the movie are endless, as you can read in the wiki article of the movie, as well as in the specific for the themes the movie touch. Some of those themes are:      Human cloning and the rights of those clones. Since those clones are the intelectual property of a company or individual, as a result of genetic improvement, do they have the same rights as humans?   The previous point can be clone, if you excuse the redundancy, for robots and artificial intelligence. If at any point we’ll be able to get birth to a machine that it’s intelligent enough (in whatever quantity we feel it should be), what are the rights of that machine? Does it have entity?   Empathy… Deckard feels empathy for those he hunts down and probably that was the reason because he left the job in the first place.   Love… interracial love, specifically love between a replicant, a thing without rights, and an “alleged” human. We have to take into account that replicants doesn’t have emotions, and it’s something they develop little by little over their life and for that reason they usually have a really short life. The development of those emotions make them dangerous. Deckard is able to develop that felling on Rachel. Is love that powerful?   The sparsity ephemeral1 or how life is too short and not enough. How replicants life in constant fear of dying since they don’t know their conception or expiration dates. Also, the importance of your.   The notion that we are our memories and experiences, and how your personality is developed through them. How do you know that your memories are yours and not someone’s else.   Philosophical and religious symbolism relating replicants with angels and Roy with Nietzsche’s Superman.   Blade Runner’s World is environmentally destroyed and totally globalized. The climate in Los Angeles has change drastically and the population there is a mixture of cultures and languages. Even some of then speak a kind of neolingua or interligua make of different languages.   Souls, eyes and doves. Eyes in Blade Runner are quite important because they are the way that Blade Runner have to identify replicants using the Voight-Kampff test, due to the replicants lack of emotions. Eyes are windows to those emotions and to the soul, implying that perhaps replicants don’t have soul. However, at the end of the movie there is an allegory of Roy’s soul departing from him in a classical christian way of a dove flying to the sky.   There are other details that make the movie a masterpiece, from my point of view. One of those details is the light. All the time there is a continuous motion of light and darkness, mostly at apartments (J. F. Sebastian’s and Deckard ones) but also in other places, creating something similar to a chiaroscuro. The other great detail is the music, created by Vangelis, that gives perfect touches of new technological, dramatic and dark future as well as delicacy and magic when necessary. Such as in the love moments or in tear in the rain scene.   All of these details make it an almost perfect movie. Perfection doesn’t exist, but some movies as this one are close.            On the contrary, its continuation, Blade Runner 2049, lacks of all of these themes or barely develop or touch them. The only new one developed is the possibility for replicants to bear children, something that would make then more human. The story is more flat or at least, and in my opinion, much more less complex and detailed. No philosophical or religious implications this time, or at least I haven’t perceived them (I can be wrong though).   Don’t get me wrong, I don’t think that the new movie is bad at all, but just another Si-Fi movie. I really think that it’s well made and the story is interesting, with some moment where you can literally blow your imagination. I also recognize that it’s really difficult to create a masterpiece every time you get behind of the camera, they are rare creatures in the history of cinema and you are lucky if you can witness or see one every year. Besides, usually there are few movies (or series) out there where the first one is really good and then the following ones are as good or even better. Perhaps some examples are the Godfather’s Trilogy or Nolan’s Batman Trilogy, where in the latter one the second installment is far superior to any Batman movie out there while the other two are also quite good.   Anyhow… usually magic is difficult to make to happen and this time didn’t happen.                  Update 10th November 2017: When I wrote his post I was looking for this specific work but I could find it… I had it in the tip of my tongue but never came out. Now, it came to my mind like an act of magic. &#8617;           ","categories": ["Reviews","Personal"],
        "tags": ["movies","si-fi"],
        "url": "https://luispuerto.net/blog/2017/10/13/blade-runner/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/blade-runner.jpg"
},{
        "title": "Installing Raspbian in a Raspberry without keyboard and external screen",
        "excerpt":"A few months ago we acquired a Raspberry Pi 3 Model 3 in the hopes to play a little bit around with it to learn some Linux and perhaps to use it as a computer to host rStudio Server and run some scripts on it. But things have been little by little been delayed, mainly because we didn’t have at hand a proper monitor nor USB keyboard.   So… last week I finally decided to do something with the Pi and started to research if it would be possible to do an install without keyboard and screen… and it turned out that it’s totally possible.   First I want to mention that I’m doing this from MacBook Pro with El Capitan installed, and I’m going to install the Stretch version of Raspbian, the last one at the moment. The way we are going to configure Raspbian remotely is connecting to the Pi through SSH (Secure SHell) and we are going to access to the remote desktop using VNC (Virtual Network Computing).   Getting ready   The first thing you need to do is download the OS image from the Raspberry Pi site, where at least in my case I chose the desktop one, and you should too if you want to have a desktop where you log in later, even if it’s just remotely.   To flash the OS in the memory card you can just use Etcher, what makes thinks a little bit easier. You just have drag and drop the unzipped image to the app window and then choose the flash card you want to put the OS in, after you’ve inserted it in the computer. Once you have flashed the card and to make it possible to log in after in the OS from the ssh console you have to create a file named “ssh” without extension in the root of the FAT partition of the card. The FAT partition is the one that you can access from your Mac after you flash OS in the card. You can do it in the terminal using this command:   1 $ touch /Volumes/boot/ssh                        File named ssh with extension on the FAT partition.              From then on, you just have to put the SD card in the Pi and wait for the rest of the installation to finish, which probably won’t take more than a couple of minutes, and connect the Pi through LAN to the same network you are connected. In my case I connected the Pi to one of my Airport Express, but you can plug it in directly to your router.   Connecting to the Pi   Now you just have to open terminal and type   1 $ ssh pi@192.168.1.X   Where ssh is the command in terminal to establish the ssh connection, pi is the default username in  on Raspbian, and 192.168.1.X is the usual IP the router is going to assign to the IP, being the X a number between 1 and 255 (well actually from 2 to 255, because the 1 usually is the router). Please, be aware that your IP could be totally different and depends on how your router is configure.   To find out what are the IP of your Pi you just need to log into your router and check the device list or IP table. Other option is just download an IP scan software (like Angry IP Scanner) and run it on your computer to see the different IPs of the devices on your network. You can also try to establish contact using the hostname instead of the IP, which in this case is like this:   1 $ ssh pi@raspberrypi.local   Then you’ll be faced with a screen similar to this one.                        Login into the Pi using the SSH connection              Where you are going to be asked about adding the key fingerprint of your Pi to your knowhost file of your Mac for secure connections (yes, you want to continue connecting) and then you’ll be prompted for the password of the Pi. The default password is: raspberry.   Configuring the Pi   Now, you are logged on the terminal/shell of your Pi, and you can start configuring using shell commands. First of all is to login as root user to make things a little bit easier and then enter in the configuration wizard (or software configuration tool) using the command raspi-config.   1 2 $ sudo su # if you want to login as root and don't type sudo in every command $ sudo raspi-config                        Raspberry config tool              Now, from here, you can change your password, the hostname, etc as you can see in the screenshot. I recommend you to change your password and leave your hostname as it is, unless you have more than one pi in your network. You should activate the VCN interface, in the Interface Options and set your Localization Options, or at least take a look to check that everything is OK. When you hit Finish.   If you want to connect your Pi to your network via wi-fi, you can also do it. You can find more info in this tutorial, but basically you first scan for wi-fi networks.   1 $ sudo iwlist wlan0 scan   And then you edit the wpa_supplicant.conf configuration file with the info of the network you want to connect.   1 $ sudo nano /etc/wpa_supplicant/wpa_supplicant.conf   Using the previous command you open the config file on nano (a really simple editor) and you add at the end of the file.   1 2 3 4 network={     ssid=\"your wifi network name\"     psk=\"the password for your network\" }   To save you press crtl-o and to exit crtl-x.   Now you can disconnect the LAN cable from your Pi to the router if you wish and restart the SSH connection using the command exit   and reconnect whether using the new IP that was assigned to the wi-fi connection or again the host (raspberrypi.local if you didn’t change it). Sometimes you need to reboot the Pi, to make it work.   You can see the wi-fi config with the following command.   1 $ ifconfig wlan0   Perhaps, it’s also a good idea update and upgrade the system before you continue.   1 2 $ sudo apt-get update $ sudo apt-get dist-upgrade   Installing the VNC Server   When you are done, you can install the VNC server in the Pi. You can find a more detailed tutorial here, but basically you type in the SSH connection:   1 $ sudo apt-get install realvnc-vnc-server realvnc-vnc-viewer   You also have to install the VNC client in your Mac, set it up and then log into the Pi using as username pi and as password raspberry or the one you’ve set up.                        Remote desktop of Raspberry Pi.                                   VNC server dialog              There are further configuration you can do in the server-side of the VNC connection, as it’s explained in the tutorial. I would recommend you to activate the experimental direct capture mode to be able to see apps that are directly render remotely (like Minecraft and I believe some of the settings windows). To do that you go to the VNC server dialog in your Pi and you click on the Menu and go to** **Options &gt; Troubleshooting and select Enable experimental direct capture mode.                        Menu in the VNC server on Pi              Now you can access completely to your Raspberry Pi desktop and you didn’t need and won’t need a keyboard or an additional screen.   Addendum: There is another options to access to the remote desktop of your Pi, that doesn’t need to install any additional software on your Mac, since the connection is carried out by the Finder.app, but they aren’t as optimal as this one that it’s recommended by the official Raspberry Pi documentation.   Additional configuration   Perhaps you are interested into mount NTFS, exFAT and samba volumes, i.e. hard drives that whether you connect to the USB port or you connect through your local network.   The first two things are pretty easy to make it to happen. You just have to type in the SSH console or in the shell on the desktop:   1 2 $ sudo apt-get install ntfs-3g $ sudo apt-get install exfat-fuse                        Mounting a usb stick on the Pi              With this you are going to mount more or less any drive you want. Usually drives mount automatically, when you connect them to the Pi, and you can find them on /home/pi/Media/pi.   Now please, remember that like in Mac you have you unmount your units - volumes before you extract them from the USB port. You do so you can clip on the eject icon on the top right corner of the desktop.   If you wanted to do the same in the terminal or in the SSH console you have to use the command mount  . A little bit more info about how to mount exFAT and NTFS can be found here and here.   1 2 3 4 5 sudo fdisk -f # this is to list all the volumes available mkdir /mnt/usb # directory where you are going to mount sudo mount /dev/sda1 /mnt/usb # mounting the unit sudo ntfs-3g /dev/sda1 /mnt/usb # alternative way of mounting if it's NTFS sudo umount /mnt/usb # to unmount the volume   Now, if you want to mount some samba shares you have connected in your network, like in your router or in the Time Capsule, like it’s my case you have to proceed as follows, being aware that you have to create the mounting (point) directory before.   1 $ sudo mount -t cifs //192.168.1.X/share /mnt/box/   Change the IP for the one of your sharing device and “share” for the path to the volume / hard drive in that device. If the name of the volume you are sharing in that device has a space, you have to introduce \\040 in the spaces. For example is the name of your share is “share with spaces” and you want to mount in the route /mnt/my share you type.   1 $ sudo mount -t cifs //192.168.1.2/share\\040with\\040spaces /mnt/my\\040share/   Now, if you want to mount any those shares on boot, to make it available all the time for other apps, you have to modify fstab  file. To do that you have to run this command on the shell.   1 $ sudo nano /etc/fstab   And now you add to the end of the file:   1 //192.168.1.2/share\\040with\\040spaces /mnt/my\\040share/ cifs user=youruser,pass=yourpassword,rw,uid=1000,iocharset=utf8,sec=ntlm 0 0   And as previously you did in nano, to save you press crtl-o  and to exit crtl-x .   You have to also tell the pi to wait until the network it’s up to boot. For that you have to run again raspi-config.   1 $ sudo raspi-config   And in boot options you select Wait for Network at Boot.   And that’s it, now you have a Raspberry Pi to play around.  ","categories": ["Professional","Technology"],
        "tags": ["how to","linux","raspberry pi","raspbian"],
        "url": "https://luispuerto.net/blog/2017/10/14/installing-raspbian-stretch-in-a-raspberry-without-keyboard-and-external-screen/",
        "teaser":"https://i.imgur.com/b7WS6uG.png"
},{
        "title": "Jääkarhut",
        "excerpt":"Yesterday was the first time in while I’ve been in Jääkarhut. It’s a really especial place and quite singular, or at least it’s what I’ve been told. It’s really true that at least the scenery, if the weather is favorable (I don’t mean necessarily sunny), it’s really awe and jaw-droping, as you can see in the header image of the post or in the image below.                        Sunset from Jääkarhut bay yesterday.              Jääkarhut is a swimming club in Joensuu that is located in the Southwest part of the city in a bay with a quite narrow access to the Pyhäselkä, which make it a really nice and quiet (what is not quiet in Finland?) place to swim because the water it isn’t insanely cold. Ok, sometimes it’s, but not always. I say that it’s a swimming club because it’s how they define themselves, but the reality is that they are more than that.                        Staircase to the avanto in winter.              Jääkarhut means polar bears in Finnish, a name that it’s quite accurate for the club, since the purpose is to swim all year around in the lake, even when the lake is frozen. They even organize competitions. At the end of the jetty there are a couple of hoses connected to a pump that move the water all the time to keep the avanto (a hole in the ice) open during freezing temperatures in winter. You can see jet stream caused by the pump surfacing a little bit in the photo on the right. You have to imagine that temperatures in Joensuu are usually around -10 ºC or -20 ºC in winter months, and they can reach during a couple of weeks less than -30 ºC.   On those months are when more pleasurable the experience is. When temperatures are inferior to -10 ºC the water of the lake is warm in contrast with the outside temperatures. You have to think that as a liquid, with a quite compact structure and high density, water needs to lose a lot of energy from its molecules to freeze and for that reason lakes and other water bodies just freeze a thin crust. I say thin because and some point in winter the ice can have more than 40 cm of thickness, and cars can drive over the lakes. The trade-off here is that being more dense, water makes you lose more energy (temperature) and the cold sensation is much more greater than if you are standing outside, even when the water isn’t usually less than 2 ºC and outside it’s -20 ºC. Something that, if I recall correctly, is related to the heat capacity of the matter.   But… are we crazy or we have been drinking too much vodka to start swimming in the lake in winter? No, the secret is that Jääkarhut is more a sauna club than a swimming club, at least for me and I think for the bast majority if the mortals around here. At the other end of the jetty there is a big cottage with a really nice sauna and big warm stove that usually heat the sauna until 80 ºC. Therefore, the trick is to go to the sauna and stay there enjoying the warm for 5’, 10’, 15’ or whatever you want to stay and then go to the freezing waters and have a quick dip. That is how most of the people survive. Here, it comes one of the biggest differences between the people who goes to the club. Some of them are real Jääkarhut, polar bears, and go to have a dip before they go the sauna, but others, like me, are Saunakarhut, sauna bears, and usually go to the sauna before we have our first dip in the water.                        The entrance to Jääkarhut in winter              Sauna + avanto (or at least a cold shower), is one of the most relaxing things you can experience in your life and I highly recommend it. In Finland, saunas are something really sacred, and almost every building have a sauna, so it’s really customary to have a sauna at least once a week to clean your self. That doesn’t mean that you don’t have a daily shower, as you should, but that you have an extra once a week. However, my biggest reason to go the sauna is the relaxing sensation you have after you enjoy one, even more if you go the avanto and in the particular case of Jääkarhut the social and visual aspects. Usually, you don’t go alone to the sauna, and if you go is perhaps a good place to meet other people and chat. Sauna is a social place for Finnish people as much as the bar, or even more, and it’s where they are more chatty. Besides, Jääkarhut has one of the most stunning views of the sunset and it’s really a pleasure to enjoy it, it the weather permits. I’ll try to write a little bit more about saunas in the future because they a whole universe here in Finland.   Returning to Jääkarhut, it’s not special only for all what I’ve already mentioned, but because as a sauna / swimming club it’s really unique in Finland, and as far as we know there is nothing similar in the rest of country. As I’ve mentioned there are millions of saunas in Finland (private and public) and probably at least a hundred, or more, sauna / ice swimming clubs. I don’t really know about a latter figure, but guess that almost every city has something. Nonetheless, Jääkarhut is the only one open to the public completely and that you don’t need to be member of the club to be able to enjoy it. Usually, in other clubs you need to pay a year membership to enjoy the facilities, or at least you have to attend with a member, as a sample. Here, you just need to buy a ticket in the machine inside of the main house and you are ready to enjoy. The price is quite cheap, for Finland, 6€ and you can swim and use the sauna all your want that day. You’ll  have access to the changing facilities too.  However, if you want to enjoy a hot shower after your sauna and swim you have to pay and extra and insert 1 € per 2 minutes of a hot shower.                        Me swimming in the “fresh” water.              Another great thing of Jääkarhut is the broad operation hours, it’s usually open until 11 at night and you the ticked machine can be operated with cash (coins) or calling from your phone, it’ll charge the amount in your phone bill, bu you have to have a Finnish number. Also, be aware, that from 10 to 11 PM there is usually held the kuumaryhmä or hot group. Nothing sexual here, not worry. It’s usually when the regulars get together to have an extra hot and humid sauna. As much water you pour to the stove (within a reasonable range) the more humid is the sauna and the head is better transmitted to your skin, even to the point that you can feel a little bit of a scorching sensation. If you are new to sauna, don’t go at that time, you aren’t going to enjoy it at all although the people is really friendly and even sometimes they want to mock and kid new participants. If you want to try the kuumaryhmä, a good tactic is go a little bit earlier and enjoy at least half and hour of normal sauna, to then be introduced to the hot session.   Finally, you need to know that the place is not open always. They have an events room in the second floor of the main building that they rent, so check the week before you go just in case there is something going on there. it’s not the first time that I have the dinner of a wedding there or other kind of even. I have to say that be able to hit the sauna after the dinner is a plus.  ","categories": ["Personal"],
        "tags": ["Finland","Finnish life","Joensuu","sauna"],
        "url": "https://luispuerto.net/blog/2017/10/16/jaakarhut/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_4290-e1508086408810.jpg"
},{
        "title": "trash instead of rm",
        "excerpt":"I just discovered that there is a way to send files to the trash instead of just delete them using the shell and I think it’s really useful when you don’t want to totally discard something to the oblivion. I found out about this because I was working on the shell and I needed to remove some files to check if app could work without them. Move or rename them was an option, but I thought that perhaps there were a more efficient way to that and thought that send then the trash was the most efficient way, since from there they could be deleted forever or restored. think   I thought that perhaps the command rm    could have some flag or option that send the files or directories you wish to remove to the trash instead of delete immediately them. However, after read a little bit about the topic, I realized that use rm command is a bad idea and practice to send files to the trash.   You can read a little bit more about it here and here, but to summarize a little bit, it’s basically not safe. You can get use to use rm    to send thing to the trash and if for a moment you are on other computer or with other username you can delete things permanently.   For that reason you can install other commands like trash-cli, and in macOS you can also install rmtrash. On macOS you can install both things using homebrew and in Linux with apt-get  .   1 2 3 4 5 6 # On Mac you can install with Brew $ brew install trash-cli $ brew install rmtrash  # On linux you can install with apt-get $ sudo apt-get install trash-cli   On the previous links about the commands you have all the options you can implement in both commands.   Happy trashing.   PS/ You have to be aware that the shell trash is not the same you can see in your desktop or at least I could find them there. Perhaps, further config is needed.                        Trashing files.             ","categories": ["Professional","Technology"],
        "tags": ["how to","linux","macOS","shell"],
        "url": "https://luispuerto.net/blog/2017/10/19/trash-instead-of-rm/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/mac-trash.jpg"
},{
        "title": "Backing up your Pi's SD card",
        "excerpt":"I’ve been trying make something work in my Pi, but so far I’ve just messing things up and I had to install Raspbian a couple of times from scratch. For that reason I’ve decided to make a copy of the SD card before I repeat the process and try to figure out what is going on.   To make a back up of your SD card using macOS I’ve found two options.      On one hand, you can install dd utility and use a graphical interface to make the backup to happen.   On the other, you can use the dd command [ref.] on the shell to create the backup.   If want to try the dd utility you can download it from the site and normally installing it on macOS or you can install it using Homebrew Cask and the following command.   1 $ brew cask install dd-utility   Backing up   When you executes the dd utility you only have two options: backup or restore a card with an image. Here you don’t have a lot of complication. However, you have to take into account that at least in my case the internal card reader of the Mac doesn’t work with this app in particular, and I need to insert the app in a external usb SD card reader.                        dd utility, backup or restore              I did my backup with the dd utility first, but in the end of the process it gave me an error message. I have the back up file in my hard drive and it works, or at least it unzips and I can mount the image.   To be sure that everything is OK and I I’m not less without a back up, I decided to do it with the shell commands as it’s explained here.   The commands are the following ones:   First we use diskutil [ref.] to see all the volumes connected.   1 $ diskutil list                        List of volumes              Now you just need to choose what are you going to back up and where using the dd command.   1 $ sudo dd if=/dev/diskX of=users/YOURUSERNAME/Downloads/SDCardBackup.dmg   You can see that I haven’t used the same path that they use in the linked instructions because using the diskX that represent your SD card. In my case was the number 4, but in your case could be different.   dd command doesn’t produce any feedback of how much of the process is left. The only indication that is running is that the prompt hasn’t returned to be the normal terminal prompt. Wait until the process finish and don’t close the window or kill he process. It probably going to take time, much more time that with the previous method. At least in my case it started at 17.48 and it’s 19.22 and it hasn’t finished yet for a 32GB SD card. I think that it check all the sectors of the card and copy them to the backup.   EDIT: I got bored of the slowness of the process and I cancel it after a while and try to implement this suggestion about raw volumes. So the code would be like this now:   1 $ sudo dd if=/dev/rdiskX of=users/YOURUSERNAME/Downloads/SDCardBackup.dmg   Restoring   To restore you first have to unmount your SD card.   1 $ diskutil unmountDisk /dev/diskX   To following write the image in the SD card with this command.   1 $ sudo dd if=~/SDCardBackup.dmg of=/dev/diskX   And finally you eject the card with using this.   Once it has finished writing the image to the SD card, you can remove it from your Mac using:   1 $ sudo diskutil eject /dev/rdisk3   So… you are ready now to mess up with your Raspberry Pi without worrying.  ","categories": ["Personal","Professional","Technology"],
        "tags": ["linux","macOS","raspberry pi","raspbian"],
        "url": "https://luispuerto.net/blog/2017/10/19/backing-up-your-pis-sd-card/",
        "teaser":"https://luispuerto.net/assets/images/pages/teaser-default.jpg"
},{
        "title": "Trash location in macOS vs Linux",
        "excerpt":"                     White trash (oh wait!) on macOS              The other day, I explained that there is a way to send files and folders to the trash from the the shell. Now, I just found out that there are differences between where is located the trash (it’s a folder nonetheless) in macOS and Linux.   In macOS, the trash is located in your user’s folder rmtrash   on macOS, since it’s a much nicer and neat command. And easily to understand.   On another hand, in Linux, you can only use trash-cli (you can’t get rmtrash   AFAIK), and I recommend you to install from source, as explained in it’s GitHub site, because if you install using apt-get  you are going to get a some kind of an old version without all the commands (trash-restore  was missing).   In Linux, you have two trashes when you are operating with trash-cli  on shell. One is your user’s trash and you can access to it on this path /home/$USER/.local/share/Trash . There, you can see two folders files and info. I guest that you already know where are the files and where are the info / metadata of the files. However, if you use the command sudo trash-put  you files are going to be moved to /root/.local/share/Trash (obviously, or not that obvious as we’ve seen in macOS).   For the other sudo   before the command.   Now, you are ready to trash whatever you want.                                                                                                                       ","categories": ["Professional","Technology"],
        "tags": ["linux","macOS","raspberry pi","raspbian","shell"],
        "url": "https://luispuerto.net/blog/2017/10/26/trash-location-in-macos-vs-linux/",
        "teaser":"https://luispuerto.net/assets/images/pages/teaser-default.jpg"
},{
        "title": "Installing PGP signing for Git on macOS",
        "excerpt":"                     Only some commits were “verified”              I’ve been committing on Git a lot lately and I’ve been uploading those commits to GitHub. At the same time, I’ve been doing some changes in the repository directly on GitHub and I noticed that the commits that I’ve done in GitHub itself were verified, but the ones that I was uploading from my computer were not. So, I decided to investigated how to “verify” the commits I upload from my computer. Turns out, that GitHub —and I suppose the rest of the online repositories— and Git are able to sign with a PGP key the commits you make to verify your identity against other people. It’s a way to be sure you, and only you, are the one that are committing, thus responsible for the things are doing.   If you what to set up the PGP signing is pretty easy in principle, but could have some caveats. To be honest, I struggled with it on the beginning and every time I committed after I set if up in the beginning I got the following message:   1 2 error: gpg failed to sign the data fatal: failed to write commit object                        Verified Signature in GitHub              You can check the knowledge base that GitHub has about the topic here. However, I found that some of the topics are perhaps a little bit outdated and doesn’t give you clear directions about how you can really do it. I also checked this sources to get my solution post:      Github : Signing commits using GPG (Ubuntu/Mac)   Git error - gpg failed to sign data   Github GPG + Keybase PGP   Automatic Git commit signing with GPG on OSX   Git Tools - Signing Your Work   How I’ve done it   First of all, you need to install pinentry   for mac. I’ve installed all of them using homebrew.   1 $ brew install gpg gpg-agent pinentry-mac    Now, you need to created a PGP key runnnig the command:   1 $ gpg --full-generate-key   You can do it also with:   1 $ gpg --gen-key    However, if you do like the latter command is not going to give you the option to change the key size, as suggested by GitHub.   Answer all the prompted questions and be specially careful with your email since GitHub is going to recognize you by your verify email. You can also create a comment to identify the key e.g. GitHut Key. Finally create a passphrase that you can remember, or note down in a password manager.   Now, you can have to list all your keys with the command:   1 2 3 4 5 6 7 8 9 $ gpg --list-secret-keys --keyid-format LONG  /Users/lpuerto/.gnupg/pubring.kbx --------------------------------- sec   rsa4096/&lt;YOUR_LONG_KEY_ID&gt; 2017-11-04 [SC]       349340ARAFKAJHFA93O8024O02JQ9304OQIRF0QU uid                 [ultimate] Your Name (GitHub Key) &lt;youremail@domine.com&gt; ssb   rsa4096/62E5B29EEA7145E 2017-11-04 [E]    You have to copy to your clipboard &lt;YOUR_LONG_KEY_ID&gt; , with is your key id, and paste in the following commands to configure you Git with your key.   1 2 $ git config --global user.signingKey &lt;YOUR_LONG_KEY_ID&gt; $ git config --global commit.gpgsign true   You can certainly not pass the command   1 $ git config --global commit.gpgsign true   that configures your Git to always sign your commits with your signature and sign just certain commits with adding the flag pinentry  .   You also need to copy your long key id again to get your PGP key with the following command.   1 $ gpg --armor --export &lt;YOUR_LONG_KEY&gt;   Which print you full GPG key, beginning with ——BEGIN PGP PUBLIC KEY BLOCK——  and ending with ——END PGP PUBLIC KEY BLOCK—— . Copy it and now you can paste it on your GitHub following this instructions.   Now, to make it work you need to config pinentry for mac as your dialog to enter your passphrase. To do that you have to use the following command to write pinentry-program /usr/local/bin/pinentry-mac in gpg agent config file.   1 $ echo \"pinentry-program /usr/local/bin/pinentry-mac\" &gt;&gt; ~/.gnupg/gpg-agent.conf   You can also do it manually.   1 $ open ~/.gnupg/gpg-agent.conf   Finally, you need to restart the gpg agent  doing the following. This is really important and it’s was one of the reason because I took me so long to finally configure Git with the PGP. Since I haven’t restarted the gpg-agent it hasn’t pick up the configuration.   1 $ gpgconf --kill gpg-agent   It’s done!   I recommend you to test it doing a test commit. First time it should to prompt you with a dialog like this                        pinentry prompt              Where, if you tick save in keychain , it isn’t going to prompt you again.   Note that if you want to sign commits outside of the shell, not all the apps can sign commits. With my setup I’ve tried Tower and GitHub Desktop and they are able to sign without any problem. Keep  in mind that I’ve committed first in the shell and then I’ve clicked save in keychain  so I don’t know how is going to behave the first time you commit and you haven’t ticked that option or if it’s the first time you sign a commit. In case you had problems with Tower, there are a couple of tutorials out there including Tower:      Signing GitHub Commits   Signed git commits with Tower   Enjoy your Git!   Note: I’ve follow this setup in Mac OS X El Capitan 10.11.6 and with Git 2.15.  ","categories": ["Professional","Technology"],
        "tags": ["coding","git","pgp"],
        "url": "https://luispuerto.net/blog/2017/11/04/installing-pgp-signing-for-git-on-macos/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/11407107023_b52fa108f7_b.jpg"
},{
        "title": "Set rStudio with Homebrew's Git",
        "excerpt":"As I’ll try to explain in the future, I have a full Homebrew R install. I also try to use Homebrew to install as much applications and utilities I can because I think it’s really handy to be able to install or update just with a simple command in the shell. Although sometimes it give you a little bit of a headache, as this time.                        Apple vs Homebrew’s Git version              Apple ships with their systems a really to use version of Git, but it’s usually a bit outdated and you can’t update that version —I don’t really know what version they have shipped with High Sierra1. This is usually not a really big deal, since, as you seen in the left picture, the versions aren’t really far away and most of the people don’t use Git in their everyday lives, but if you really need, or want to be, up to-day you are going to need to do some tweaks here and there.   First of all, you need to install the last Git using Homebrew:   1 $ brew install git   You are probably going to need to relink by force -f your new Git to make the system to use it.   1 $ brew link -f git   Know if you ask the system, your version of Git have to be different —superior—  to the one Apple provides. In my case and under El Capitan is like this:   1 2 3 4 5 6 $ usr/bin/git --version git version 2.10.1 (Apple Git-78) # Apple's version $ /usr/local/Cellar/git/2.15.0/bin/git --version git version 2.15.0 # Homebrew's version $ git --version git version 2.15.0 # System's version, now linked to Homebrew's one.   If you are user of rStudio, you’ll probably know that you can use Git in your rStudio projects, as you should, as in any coding project. rStudio developers, to save us with the fuss of installing and configuring Git —and to save them to explain us how, set up rStudio to use the Apple’s Git by default, pointing to /usr/bin/git  in the user settings                        rStudio’s version control options              If you want to use your own downloaded Git you just have to change that value in the settings and that’s it. However, it’s a little bit more complicated than that in practice. Since we’re using Homebrew to manage our apps the directory of the app changes every time you update it to show the version it’s storing, just in case you wanted to have more than one version. The path to the directory of Homebrew’s Git looks something like this, as you saw before in a code chuck:   1 /usr/local/Cellar/git/2.15.0/bin/git   If you updated Git using Homebrew, the new path would look something like this.   1 /usr/local/Cellar/git/2.XX.X/bin/git   Where the X are the new version numbers.   To make things easier, Homebrew just create symbolic links to the directory /usr/local/bin/  to make the system find the app files. That is what you did when you used the command brew link  and you had to use the flag -f  —force— to overwrite the already existent links to Apple’s Git in usr/bin/git .   So, you’re probably thinking that you just have to set the path to Git executable in rStudio to /usr/local/bin/git  and problem fixed. Yes and not. Yes because that is the path you need to end up in settings, but is something you can’t make to happen using rStudio interface. Every time you change the path to /usr/local/bin/git  using the browse button in the settings you are going to end up with /usr/local/Cellar/git/2.XX.X/bin/git  and in consequence when you update Git using Homebrew rStudio isn’t going to be able to find the Git executable. In consequence, you will end up without Git support in rStudio. This happens because rStudio interface follows the symbolic link instead of stuck with the route you set with the browse button.   Then… What we should do? You need to open the user setting’s file of rStudio located in a hidden directory in your user folder ~/.rstudio/monitored/user-settings/user-settings  with the following command.   1 $ open ~/.rstudio/monitored/user-settings/user-settings                        rStudio user setting’s file.              Towards the end of the file, there is a variable called vcsGitExePath  that set your personalized options for the path of Git executable. With your rStudio closed, you can change it in the file to this:   1 vcsGitExePath=“/usr/local/bin/git\"   and save the file.   If you open again rStudio and go to your user settings, the version control settings have to look like this:                        Version Control Settings on rStudio.              Now you can update without any problem your Git using Homebrew and it will keep working.                        Git’s version in High Sierra                             I just updated to High Sierra, and I can confirm that the Git’s version is a little bit more up to to day, but it isn’t the latest. &#8617;           ","categories": ["Professional","RStats","Technology"],
        "tags": ["git","homebrew","how to","RSoft","RStudio"],
        "url": "https://luispuerto.net/blog/2017/11/05/set-rstudio-with-homebrews-git/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/rstudiobrewgit-1.jpg"
},{
        "title": "Updated to High Sierra",
        "excerpt":"                     High Sierra update on my machine.              Yesterday, after a hard drive update, I updated my machine to macOS High Sierra. When I tried El Capitan, like almost two years ago, I thought that it was going to be the last OS my Mac was going to run. El Cap, as the Apple employees call it, was a really stable and reliable OS. I loved it, and it really run smoothly in my machine, where I had at that moment a custom Fusion Drive —120 GB SSD + 700 GB HDD, latter I upgrade to 1 GB due to a failure of the original hard drive (after 5 years) . At that moment I thought that any further update would create slowness and lagginess in my machine and after I’ve tried how smooth is a computer with an SSD on it,  I really feel aversion to a slow machine. When Sierra went out I tried it, since I have to install the OS again, but didn’t feel right so I stick with El Cap.   Now is the day after I finished the install of High Sierra and the feeling is really nice in general. I think a lot of things are still to settle down —I still have a couple or more mdworkers   hanging around from time to time— so the fans fire up from time to time, but I would say that the felling is even a little bit better than El Cap in some aspects —perhaps I’m subjective and in inside of the new OS hype. It’s true that I am not longer under a custom fusion drive and now I have a full SSD hard drive, and probably that helps, but you have to bear in mind that my machine is a Late 2011 one, 6 years old, and it’s running really really smooth. It’s also true that no longer has the original RAM, but all the rest are the original pieces and still work really well. It seems that even the battery life has improve a little bit after the update. However, I don’t have any stats to prove it.   After I updated, I faced a couple of problems, like is normal in what I would say it a quite customized environment. I have to retouch a little bit my custom keyboard layout to work perfectly with high sierra, and I have some problem with a PDF that I don’t know why preview couldn’t visualize. This latter problem caused the QuickLookUIService   to run wild —even at 400%— when spotlight tried to preview that pdf. So I deleted the pdf —I wasn’t interested— and problem solved.   So far so good! Good job Apple.   PS1/ There were some mdworkers   hanging around consistently even after two days after I’ve installed the new OS —seems that they are not around any more. mediaanalysis   that look what is in your photos to be able to search about them lately. Those processes only work when your computer is idle or you are almost not using it. Interesting.  ","categories": ["Professional","Technology"],
        "tags": ["macOS"],
        "url": "https://luispuerto.net/blog/2017/11/08/updated-to-high-sierra/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/macos_high_sierra_alternative_wallpaper_by_kakoten-dbfgc6m.png"
},{
        "title": "Media Buttons Behavior Has Changed In macOS High Sierra",
        "excerpt":"macOS High Sierra has changed the behavior of the media buttons on macOS —AKA those buttons you use to play/stop music or go forwards or backwards. Before, they were reserved to manage music, whether you where playing in iTunes or Spotify, with iTunes having preference (usually). They also worked with some video player.   Now, Safari has stolen that preference, and if a video or some kind of media is played in Safari —whether in background or foreground, those buttons are going to control it. In other words, you are playing music in iTunes, you find a video in YouTube that you want to watch, so you hit play and push the play/stop key in your keyboard to stop the iTunes music and be able to listen the video audio, but now that is not he behavior. You are going to stop the video, while the music is still playing. And this is not the worse behavior, some people have reported that they are listening to music, receive a call in the iPhone, ringing in macOS —as expected in the last versions of macOS— and when you press play/stop button to stop the music, you answer the call, while the music is still playing. People isn’t happy.                        High Sierra Media Key Enabler for iTunes              Some users have reported that you can fix the issue —or the feature— if you log in your Mac on Safe Mode —pressing shift key when the Mac starts— and use the keys with iTunes and then you return to the normal logging. However, that hasn’t worked for me and I have to say that in my experience iTunes doesn’t behave as they describe. I’m perfectly able to play music on iTunes in safe mode and if I open Safari with a video, the play/stop key behaves in the same way and in normal mode.   What I’ve found helpful is this little app called Media Key Enable that someone recommended in Ask Different. It just turn back the behavior as in earlier releases. You can check the project in GitHub if you want. If you’re a Homebrew user you can install it with Homebrew cask:   1 $ brew cask install highsierramediakeyenabler   If you are as annoyed as the rest of us for the change in behavior, I recommend you to complain to Apple using  Apple Feedback.  ","categories": ["Professional","Technology"],
        "tags": ["high sierra","how to","macOS","tweaks"],
        "url": "https://luispuerto.net/blog/2017/11/10/media-buttons-behavior-changed-high-sierra/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/File-Oct-17-10-34-21.jpeg"
},{
        "title": "The bread story",
        "excerpt":"Anyone close to me, or that’s been hanging out with me for a while, knows that I’m a bakery enthusiast. Ok, perhaps I’m not an enthusiast, but let’s say just an aficionado that like to bake bread and pizza from time to time. This hobby started basically when we arrived to Finland and we brought with us a bread machine that a friend gifted to us. When I was in Spain, and some of my friends and acquaintances were really enthusiastic about bread making and learning about how to make bread, I never understood the point. In general, or at least in my hometown, the bread is really good and quite cheap. I know, really depends where you live and buy your bread and myself have had a really mixed experience related to bread quality in Spain. For me has been quite surprising that in some small villages the bread nowadays is quite industrial and tasteless, where supposedly it has to be more traditional and flavory. I would say that in Ponferrada you can buy a really quite high quality bread almost anywhere, in supermarkets or in bakeries. It’s true that some supermarkets chains prepare themselves the bread using a precooked bread, which give a really fine result few hours after baking it —usually it’s still hot and appealing when people buy is— but after few hours it becomes soggy and unappetizing.                        Lild’s bread machine, quite similar to the one we inherited. Source: Wikimedia.              Anyhow, when we arrived to Finland was really hard for us to find bread with similar standards as the Spanish or South Europeans ones —mainly hard and crunchy crust, and fluffy, full of holes (or eyes as we call them in Spain) and spongy interior although firm. Don’t get me wrong, I don’t think the Finnish, or Nordic, bread is bad, but it lacks of certain characteristics that for me make it soulless —it’s almost impossible to dip with it and the olive oil isn’t as tasty. The bread machine, yet it’s a good beginning to begin to bake, it wasn’t any help to reach the desired standard, since the style you usually get with those machines is similar to the common sliced bread —soft crust and interior, and with no eyes or fluffiness at all. In other words, a soft brick. It’s true that you can do a lot of variants with those machines, and add mix different flours or ingredients, but it can’t deliver what we wanted.                        Ken’s Book. Source: Amazon.              Therefore, I begin a search to mimic the style my baker deliver to me in Spain. In that journey I came across the Ken Forkish’s book, “Flour Water Salt Yeast: The Fundamentals of Artisan Bread and Pizza”, which I liked from the very beginning, because even the title sounded pretty basic and I wasn’t looking for something really fancy. I just wanted to make simple but tasty bread. Through that book, I learned the basic bread making technique, the basic instruments I needed and from there on I developed my own technique and recipe, more simpler than the one he teaches in the book. Ken uses mainly a dutch oven to bake the bread at home and to mimic the moist created in an industrial oven that provide the bread its hard crust characteristics of the South European breads.   Although I really liked the result with the dutch oven —you are going to get a really a perfect boule that cracks from the top, which is really nice— it’s really more complicated than what I wanted. You have to warm up the dutch oven in the oven for around an hour before you want to start baking the bread in it. Then, manage the really hot dutch oven to put the dough in it and reintroduce the whole thing in the oven again. You can burn your hand if you aren’t careful and adds quite a complication to the process.   Anyway, I really recommend Ken’s book to begin to bake and learn about the process of making bread. It’s really simple and enjoyable to read, and with tons of diagrams and images to explain to you how to do things. With him, I learned about the different kinds of flours, how to mix the dough with one hand, about the autolyse, the stretch and the folding of the dough to improve the formation of gluten networks, the pincer method of mixing, to measure everything always in weight and in percentages of the whole amount of flour, and or course about my beloved Maillard reaction that is in almost all the things I bake and cook. Because if your bread isn’t brow, I would say dark brown, in other words well baked, you are doing it WRONG. And I can’t stress it more. If your loaf doesn’t have the Maillard reaction isn’t bread, is other thing, probably just harden dough. It isn’t a mater of your taste, but a matter of correct taste. Bread should have a hard and crunchy dark brown crust to give to it the different levels of complex flavor that a perfect bread must possess. I’m not kidding and it’s probably a matter of education to learn how appreciate bread like that.                        Ken’s sandwiches. D E L I C I O U S !              When I started to use Ken’s book, four years ago, I couldn’t imagine that at some point I would be enjoying Ken’s products in his bakery in Portland’s Alphabet Historic District.  I never thought that in the future I would be living for a while in the States, and even less in the West Coast, and more specifically in Oregon. I even didn’t realize that his bakery was there until we arrived to Oregon, and thought… “wait a minute, isn’t here were this guy from the book has his bakery?” Besides, and as I said, I’ve never been a crazy guy about baking, and I bake more for necessity than for the pleasure of baking. However, I really like to do something right when I do it, and I can become really obsess with it at some point. Taste matters. When I visited Ken’s place I realized what is perfection. I would say it’s the best bread I’ve ever tried, and it’s well over the average loaf you can find in Spain, and probably in France and Italy. Really high quality and for a really good price, tacking into account you are in the United States.                        Me at Ken’s. On the left after we bought a couple of loaves and in the right at the door of the venue.              Sometimes one just can’t be but really surprised of the turns that life takes and you never know where you are going to buy your next loaf of bread, eat you next sandwich or drink you next beer.   I’m really proud of my bakery abilities. My grandpa was as professional baker and, although I never baked with him, I think about him almost every time I bake.      You always have to bake very well the bread (get a really good Maillard effect) and the bottom of the loaves and boules have to be also really well-baked. Move them to bake the bottom because the oven get cold under them.    In the following days I will try to post my recipes for bread and pizza. I’ll try to be as detailed as possible, but as I said, they are just simplified versions from Ken’s book ones. I really recommend to get Ken’s book to get the full technique and recipes, you are going to learn a lot. Ken has a YouTube channel where he explain different techniques also.   See you by the heat of the oven.  ","categories": ["Personal","Trips"],
        "tags": ["bread","eating","Oregon","Portland","trips","USA"],
        "url": "https://luispuerto.net/blog/2017/11/12/the-bread-story/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_0631.jpg"
},{
        "title": "The preliminaries for bread and pizza making",
        "excerpt":"When I started to write this post, it was going to be just my bread recipe, as promised in my previous post. However, as I dug down little my little explaining the basics of bread making, I realize that I’d be better to make a post just explaining the basic of bread and pizza making and then in two other different post explain the recipes. Again, I can’t stress more that if you really want to learn how to bake, you should take a look to Ken’s book, where you are going to be able to learn the bread method as a whole and read the complete recipes.   Ingredients &amp; tools   What are we going to need to bake bread? Ok, let’s start with the basic tools and ingredients you are going to need.                        Tools for baking              Essential tools      An oven —:grimacing: just in case isn’t something obvious or you don’t live in Iceland— and an oven tray. It’s better to use a swallow one.   Oven mittens or something to pick up hot things. You aren’t probably superman or a replicant, so you are going to get your hands burn without them.   A kitchen scale. All the measurements are in grams. No volumes. I’ll explain later.   A big bowl. Thing that the dough usually rises like the double or the triple of the original volume. Better transparent, so you can see what is going on inside. I use a big salad bowl from Ikea when I bake a 500 gr loaf, and a really big kitchen pot when I’m making a kilo.   A container to put water inside the oven. I use a small pyres tray but something made of metal or ceramic also works. You need this to create a sauna in the oven.   A timer or a clock… you need to measure times quite precisely.   Recommended tools      A precision scale. This isn’t an essential, but I would say it’s highly recommended. At some point you are going to need to measure quite tiny amounts of salt, and even tinier of yeast. Something in the range from 0.10 gr. to 5 gr. Kitchen scales usually don’t go down to less than 5 gr, and sometimes they lose precision under 100 gr. You don’t need anything fancy, just something than can go under 1 g. —with a readability of around 0.01 gr.— is going to be fine. I have this one now, but you can find tons of then in Amazon or eBay for less money. If you don’t want to have more than one scale, or you want to buy just one  —something understandable— you can just buy the precision scale. Yet, precision scales can’t measure more than 500 gr. —or at least I haven’t found any— which can be fine if you use a container really light to measure the flour. You can also take more than one measure. In other words, if you need to measure 600 gr of flour, you can measure 300 + 300 gr. Anyway, I have two scales, kitchen and precision one because it’s handy.   Parchment paper or what some of us know as oven paper —it is’t the first time I buy paper to wrap sandwiches thinking that it’s oven paper, so be careful, they are pretty similar, but sandwich paper get stuck to the bread and create a mess. It’s not an essential because you can sprinkle some flour over the tray to try to avoid bread to stick to it. However, it’s easier with the paper and It’s easy to find it in any grocery store —in USA was pretty difficult and I ended ordering it on Amazon. If you are thinking to use a baking mat I wouldn’t recommend since the maximum temperature those mats can bear is around 250 ºC, which is the basic temperature you are going use to bake your bread.   A scraper to cut and handle dough and which is really cheap. With it is usually easier to take out the dough from the container where it’s fermenting and if you want to cut the dough to make different loaves or boules it’s going to do the job better than anything.   A kitchen thermometer. The Ikea one is just fine. In the beginning is good to understand about the temperatures, but it isn’t essential. You can use your hand, but not in the oven.   Proofing baskets or bannetons. These are the baskets where you leave the dough when it’s already shaped and undergo its final rise, or proofing.  In general, it is good to have some container where you can leave the dough resting during the proofing, mainly to keep the dough safe —you don’t move or pick up the dough like slime— to save space in your kitchen and help them to reach the desired shape. You can leave your dough resting on the counter over some sprinkled flour or use some container you already have where you’ve sprinkled some flour, if you don’t want to invest in these baskets. However, the final result it’s usually a little bit better if you use a banneton. You can find them on eBay or Amazon. I got mines in eBay and they are just fine.   A rolling-pin or something to flat the dough when you are making pizza. I have a nice wooden rolling-pin in Finland, but if you don’t have one and you don’t want to buy it, it’s fine. You can use your hands to flat the pizza, if you are brave enough twirl it in the air like a real pizzaiolo, or use a bottle of wine or a big bottle of beer. While I was in the USA, I was using a long and straight bottle of beer I thoroughly cleaned and removed the label. It isn’t as perfect as the rolling-pin, but it’ll do the job.   A couple of small or middle size —a liter is enough— jugs to measure water. You can use whatever you want, but I recommend you to use something with a handler and a mouth made to pour liquids if you don’t wan to create a mess. You are going to measure water in gr. over a scale, so you need a container in scale, and a container to pour the water over the container in the scale and  measure the amount. Then you are going to need to pour that water over the flour.   Ingredients      Flour — The basic ingredient.   Water — The basic matter of life and the universal dissolvent.   Salt — To make the bread savory and tasty.   Yeast — I normally use natural yeast, but you can use dry instant yeast if it’s easier for you to find it.   Olive oil — Just to make pizza dough. I use Spanish 🇪🇸 extra-virgin olive-oil for almost all my cooking —this isn’t an exception.   Ok, as you see the ingredients aren’t really complicated and they are basic things that almost anyone has in their kitchens. Perhaps the most complex thing is the yeast, but you’ll see it isn’t that complicated.   Measurements in weigh                        Measuring for a pizza              As Ken explains in his book, everything in bakery is measured in mass or weight, and grams are usually the most handy unit. You don’t use volumes because usually are a tricky business even more when you are dealing with something like flour, salt or yeast. Flour can be compress and have less volume for the same amount of matter or mass, in other words more dense. Water can vary its volume depending on the temperature also, and, although the variation usually is really small, you are already using a scale for the flour. You are going to be really precise if use the scale instead of the marks in the water container —if it has any. Salt and yeast, are also tricky things to measure in volume, even more if you use natural yeast, which is like plastic.   When I’ve been baking with some people, they usually are really surprised that I try to be as precise as possible working with the scale. So if the recipe call for 500 gr. of flour, I go for 500.00 gr., or if the recipe says 345 gr. of water I measure 345.00 gr. I don’t go a gram up or down and I try to be as precise as possible. The kitchen is a lab, and you should treat your ingredients as chemistry reactives.   Baker’s Percentages   Everything is also stated in percentages, or as they are called, baker’s percentages. This has more sense that you think and it’s going to be really useful. However, it’s also going to be really confusing at the beginning because often most recipes on internet aren’t stated like that. The reason for that is the amounts of some ingredients, like salt and yeast, are normally so tinny that is handier to use volumes unless you have a precision scale. In spite of that, I recommend you to stick with weights since in the long run the results are going to be better.   You have to think that the main ingredient here is flour, whether it is just one kind or a mix of flours. For that reason, all the rest of the ingredients are just expressed as a proportion of that main ingredient, so the mix of the dough is going to reach always the same exact result. OK, this is not always true, and the kind or flour or mixture of flours you are using has a lot to do with your final result. Let’s try to keep things simple for now, and I’ll try to explain a little bit about that later.   So, let’s say in our recipe we are using 500 gr. of flour that is our 100%. The rest of the ingredients are just a proportion:      100% of flour: 500 gr. x 100% = 500 gr.   72% of water: 500 gr. x 72% = 360 gr.   10% of salt: 500 gr. x 10% = 10 gr.   0.72% of yeast: 500 gr. x 0.72% = 3.6 gr.   Now imagine that for any reason you wanted to make a loaf of 700 gr. instead of 500 gr. Easy peasy:      100% of flour: 700 gr. x 100% = 700 gr.   72% of water: 700 gr. x 72% = 504 gr.   10% of salt: 700 gr. x 10% = 14 gr.   0.72% of yeast: 700 gr. x 0.72% = 5.04 gr.   As you see percentages makes your life a little bit easier, and gives you flexibility.   Sometimes you are going to want to mix flours to give different flavors and consistency to your bread. Usually those recipes state the amount of flour of each type also in percentages For example. 60% white flour and 40% whole wheat. Then, if you want a 700 gr. loaf you have to proceed as following:      100% of flour: 700 gr. x 100% = 700 gr.            60% white flour: 700 gr. x 60% = 420 gr.       40% whole wheat flour: 700 gr. x 40% = 280 gr.           72% of water: 700 gr. x 72% = 504 gr.   10% of salt: 700 gr. x 10% = 14 gr.   0.72% of yeast: 700 gr. x 0.72% = 5.04 gr.   Easy, isn’t it? Anyhow, if you want to learn a little bit more about the baker’s percentage, check Ken’s book or the wikipedia article about the topic.   Flour                        Finnish flour              I think you have to take into account when you are mixing flour and water is you are going to get different dough consistency —thus different results— depending on the amount of water you are  adding and the kind of flour —or the mix— you are using. It’s not the same to use white fine flour than whole wheat or rye flour. Usually, white fine flours need less water than whole wheat or rye flours, mainly because they absorb less water. Of course, if you mix flours you are going to need something in between depending on the percentage of each flour.   I normally use wheat-white-fine-unbleached flour for my baking, which in Finland is called puolikarkea vehnäjauho. I use this kind, perhaps, because in Spain the most widely used flour is the wheat-white-fine-bleached one since most of the people consume white bread, although things are changing. I decided to go with non-bleached because I prefer the taste —less industrial— and the color of the final product —close to the real color of the wheat, white creamy —almost nothing is pure white in the nature. You can go with whatever flour, or mix of flours you want, but keep in mind that if you decided to mix or use other flour than the one stated in the recipe you are going probably to need a little bit of adjustments. In any case, the adjustment could be unavoidable along the road, since even if you are using the same kind of flour, but different brand —or  even vintage, the flour could be different and have slightly different properties.   In the past, I used and mixed different kinds of flours, like rye or whole wheat with really good results, but nowadays I mostly use just white non-bleached for bread and pizza. If you want to learn more about flour, I again, recommend Ken’s book. If you live in the Nordic Countries, specifically in Finland, I recommend you this guide that will help you a little bit in your trip to the supermarket. Please, keep in mind that the more gluten content of the flour, the better, since it make the dough more elastic, it can rise better and keep the bread structure more firmly. The problem is that usually the gluten content isn’t stated in the package. I think you can also read the wikipedia article about wheat flour, which is quite informative. Learn as much as possible about the flour you can find where you live; it’s the basic element for baking and that information it’s going to be helpful.   Water and temperature   Almost any water good for drinking is going to be good for baking. However, take into account that if you use water that has flavor, it’s going to be added to the bread. In Spain, for instance, there are a lot of places where the water is hard —high content in minerals, specially calcium— and they give to the water sometimes unpleasant flavor.   Another variable you have to take into account is temperature. Mainly the temperature of the water and the room where the dough is fermenting / raising. For best results during fermentation you usually needs to have everything a little bit over room temperature (21 ºC), about 26 ºC. This means that you have to use a little bit of warm water, like 35 ºC to warm things up when the reaction starts. Here is where you use the the kitchen thermometer, if you have one, but if you don’t, you can use your hand. Remember that your body is around 36 ºC, so if you dip your hand in the water and it feels comfortable for a bath, that means temperature is more or less right.   Since I use fresh yeast, I dissolve it in the water although you can mix the yeast and the salt with the dough after you do the autolyse, as Ken’s explain in his book. Keep in mind that as higher the temperature, the yeast reaction is going to be faster, as temperature activate the yeast. So, if you use warmer water, the temperature of the dough is going to be higher, the yeast if going react faster and the total time of fermentation be reduced. However, keep in mind that around ~43 ºC the yeast starts dying. For that reason if the water is too hot for you, it’s going to be probably too hot for the yeast.   Yeast                        Finnish fresh yeast. Source Wikimedia.              Yeast is the most complex ingredient after flour —or even more complex than flour in some aspects— and one that is quite mystical, since it provide the magic of rising the bread. I don’t going to tell you too much about yeast in the technical side —I’m not an expert— and you can know more, as always, in wikipedia. I think that what you just need to know is that basically yeast is something alive —even when you use dry yeast, when you hydrate it comes to life— and they are from the kingdom of fungi —the same as mushrooms. They usually eat sugars and transform them into other things. In our case mostly gas —carbon dioxide, but if you leave them enough time yeast can also produce alcohol.   I normally use** natural fresh yeast** for my baking here in Finland, but perhaps where you live is more difficult to find. Where I was living in USA, I used instant dry yeast since I could find it in almost any grocery store. Later, I found out where was the fresh yeast my grocery store, but was more expensive and less convenient than the instant dry yeast. It’s up to you what yeast you use.   Whether you uses fresh yeast, or instant dry, you have to know that:   3 gr. of fresh yeast ≈ 1 gr. of instant dried yeast   So, you can convert one kind of yeast into another easily and use the one you have at hand regardless of the kind is mentioned in your recipe.   When you increase or decrease the percentage of yeast in the mixture, you usually increase or decrease the fermentation time, in other words, the time you have to wait until the dough fully rises. To calculate this time, you have to take into account also the temperature or the room and temperature of the water you uses for mixing the dough. However, let’s suppose that we work always in standard conditions (21 ºC of room temperature and 36 ºC for the water). In consequence, you can use the amount of yeast you use to adjust the time of fermentation to your necessities. However, you have to be careful and don’t add an incredible amount of yeast to produce a fast or instant bread because your bread could end with too much yeast flavor.   The Tables of Bread &amp; Pizza   “The Lord Jehovah has given unto you these tables of bread and pizza commandments! For all to obey!” Ok… perhaps not Jehovah.   Ken uses 5 hours of fermentation for straight doughs bread which it’s a nice beginning point. However, there are moments when you don’t have the time to wait for a 5 hours fermentation, or that it would suit you better if you can extend the fermentation a little more. For instance, if you want to mix the dough just before you go to bed and bake in the morning.   For that reason I developed the below tables. Bear in mind that they are really far from being truly  accurate and they are just a rough extrapolation, but they help me to create an idea of how much yeast I need for the time I want to rise my dough.     You can use then, and perhaps you can even develop yours. You can download in pdf in the link above or here. I created them in Numbers App on Mac, you can download the original here. If you want an Excel version you can download it here, but take into account that I just converted them using Numbers so bugs galore.   Perhaps, at some point I should try to create a regression equation or something.   The technique   The technique I use is a little bit different than the one Ken uses; I tried to simplify because I usually shot of time. However, and as previously, I recommend you check his book to a full explanation of the technique. I’m going to try to explain here, and with words, some parts of the technique that I use and some others that I find interesting but I don’t normally use.   The autolyse   The autolyse it’s just to mix the water and the flour alone and leave like that for about 20 or 30 minutes. The reason to do it is to increase the absorption of water by the flour, making it easier to shape and work with it and prevent the bread from bleach from the action of atmospheric oxygen.   I usually don’t autolyse, unless I really have the time. For example, if I going to make an overnight fermentation I sometimes mix the water and the flour a little bit before going to bed. Then, just before going to bed I add the salt and the yeast for a 8 hours fermentation. Another reason because I don’t autolyse is I don’t use dry yeast that it’s easier to add after, since you just sprinkle over the dough. Natural yeast need to be dissolved.   The pincer method   It’s a way of mixing the dough using your thumb and index fingers to cut the dough several times and then remix it again. You usually do that a couple of times. You can see how Ken do himself in this video.   The folding   After you have divided the dough with the pincer and mixed it again, you use the folding after the dough has rested a little bit. The folding is basically stretch the dough, without tearing it, to fold it over itself. You usually do that several times and then leave the dough rest for 15 minutes to repeat it again. You do this like 3 or 4 times during the first hour of fermentation. It helps to build up the gluten to provide the necessary strength to support the bread structure. In other words, it helps to make a bread with volume. Again you can see Ken do it himself in this video. I usually don’t do it as thorough and I just do the folding at the end of my dough mixing. It’s up to you if you want to do it or not during the first hour.   The rising   There isn’t a lot to say here, but just that in the time when the yeast is reacting, and producing the fermentation, so the dough rises. Try to keep the dough at room temperature (21 ºC) for the whole time and remember to cover the dough to avoid the development of a dried crust in the top of the dough. If you aren’t using a container for rising with a lid, like me, you can use a dish to cover because it’s easy to clean in case the dough rises enough to touch the covering —I use a big dish from Ikea and perfectly fits on the opening of the bowl. There are multiple options for covering, from the traditional kitchen towel to the even better with plastic wrap. I think plastic wrap is better because you are trying to avoid losing as humidity as possible from the dough during the rising period, so plastic is a better sealant than a piece of cloth. However, I understand that plastic is not the best for the environment and for that reason I suggest a dish.   Shaping and proofing                                                                                                                                 After the dough has risen —it’s fully fermented— you can remove it form the container where it has been rising and put it on the counter over some sprinkled flour, where you are going to shape it.  Just remember to cover your hands with four to avoid the dough stick to your hands when you are handling it. It’s usually really handy to sprinkle some flour in the the border of the dough to remove the it from the container, where you are going to introduce your hand or the scraper to detach it from the container. With the scraper it’s usually a lot of cleaner and easier than with your hands.   The process of shaping the dough isn’t really complicated. I usually pick up each of the corners, after I’ve cleaned any residual flour on the dough, and fold them to the center of the blob. Next, I turn it over and create a ball with the seam on the bottom using my hands to push the edges of the ball under it and stretch the surface of the ball. If I’m going to bake a boule I leave like that, but if I’m baking a loaf I squeeze and stretch the ball to make it cylindrical, but keeping the seam at the bottom. You can see how Ken shape the dough here. After than, I leave it resting in the banneton to undergo the final rise for proofing. The proofing is the way your loaves are going to get their full potential and volume. However, you have to be careful, if you overproof, your loaves are going to collapse over themselves, and if you underproof they aren’t going to develop their full volume.  You can see Ken here checking his proofed loaves.   I usually don’t proof the bread too much and perhaps it’s something I should do during more time. Normally I get impatient and I don’t proof more than 20 or 30 minutes —the time the oven gets warm. However, for straight doughs it’s recommended to proof for around an hour. This is really up to you and how much time you have or want to invest in this. You have to mind too, that if you are going to leave your loaf proofing for about an hour, you should keep it inside of plastic bag or cover it in some way, to avoid it to dry and create a hard crust.                        Proofing in the banneton.              Almost every good baked —and happy— bread has to have what we call in Spain a smile,_ _or in other words a crack or split. To make that happen, usually you use a razor —or a really well sharpened knife— just before you are going to introduce your loaf in the over to make one or more cuts in the upper part of the loaf. This way, the the loaf rises in the oven it create beautiful splits and cracks.   However, I usually don’t score then and I try to leave the seam —the one I made when I shaped the loaf and I left under the loaf while proofing— in the upper part when I put the loaf in the oven. This way, usually —and I say usually because it doesn’t happen always— the loaf creates a natural crack where the seam is when it rises in the oven.   It’s up to you so score or not your bread before you bake it.   The sauna oven :finland:                        Water in a container inside the oven.              If you want to have a crispy, toothy and hard crush in your bread you need to put it on a sauna for the at least the first 20 minutes. OK, wait a minute, bright back that loaf to the kitchen because I didn’t mean literally in the sauna. What I mean is you have to create a lot of humidity inside of the oven. To make that happen I usually introduce in oven, while it’s warming, a heat resistant container with water. This way the oven is really humid when you introduce the bread. There are other several techniques and some of then have been tested here. You can do whatever you want, but the point is to bring humidity inside. I usually remove the container from the oven after 20 minutes if there is any water left on it, so the environment is totally dry towards the end of the baking and you extract all the excess of humidity from inside of your loaves. You don’t want the crumb of your bread soggy.   When I started to bake bread I used Ken’s method with the dutch-oven, which doesn’t need to add any water to the oven since you use the own humidity of the dough —the dutch oven is really a small place and with the humidity from the loaf is enough. However, as I’ve explained, in my opinion it’s too complex for a regular baking, and has another drawbacks . So I tried to mimic a professional oven and started to spray water inside with a manual spray from Ikea. The results where really good. However, I felt that if you put too much water over your loaf it loses the capacity to rise. Besides, you have to open the door almost every 3-5 minutes to re-spray and keep the oven humid, in consequence you lose heat every time you open the oven door.   However, I think that the optimal approach has to be something closer to what you have in the Finnish 🇫🇮 stove in sauna.  In other words, a container filled with rocks of other similar elements that has a lot of edges, jagged sides and crevices that increase the hot surface exponentially —Iban Yarza says that a deep metallic oven tray filled with screws and nuts could do the job too. Nevertheless, I would also have a container filled with water from the very beginning, to introduce the bread in an already humid environment.   Maillard reaction                        Maillard reaction in my bread              As I mentioned in my previous post, you have to bake your bread until it’s dark brown because if you don’t, you are doing something wrong. You have to get in your bread what is know as Maillard reaction. Maillard reaction is a well known reaction that doesn’t only take place in your bread, but in almost everything you eat and enjoy, or at least you should, and has been cook. When you create a Maillard reaction what you are doing is adding flavors to your bread due to the degradation of proteins in the outside of the loaf. Besides of adding flavor, you are also adding a different texture to your bread, which your taste is probably going to appreciate.   I know that there are people out there that _don’t like dark brown bread _and they prefer to eat white blunt one. As anything in taste, dark brown bread if something you will end up learning how to enjoy, in the same way you learn to enjoy a good wine or beer, or a well made piece of meat. It’s just a matter of education to learn to enjoy and appreciate those new flavors and textures.   Remember, if you don’t bake your bread until dark brown, you aren’t baking bread, you are baking other thing. Perhaps, just dried dough.   Post baking   As you probably know, you don’t try to eat or cut the bread directly after you bake it. You have to leave the loaf resting in racket for at least 30 minutes and I even recommend a little bit more. If you open up your bread too soon you are going to mess up with the last part of the baking and probably the crumb is going to get ruined. You have to think that when you cut the bread and it’s still hot you are exposing the hottest and most humid part of your bread to the outside environment without the protection of the crush. In consequence, it’s going to lose it’s water content quickly and going to get dry and deformed.   Don’t open out your bread too soon and be patient. While you wait, you can listen the sound of the cracking crust. Pure music, your bread is singing for you.   Final comments   This post has been finally much more longer than I wanted and I’m sorry for it, but it’s really difficult to summarize all you have to know in a more brief way. Perhaps you are thinking that you just want a recipe and you don’t need to know how it works. I understand, but as everything in life if you really what to do it right you have to at least have an idea of what is going on. With this post, I’ve tried to do that.   You also have to keep in mind that this directions and explanations, are mainly valid for the straight dough kind of baking. If you want to explore other styles like biga or poolish, things are going to be quite different.   Finally I can’t but again recommend you to check Ken’s book, which I think it’s perfect to start baking, have a deeper understanding of all the processes, learn about another styles of doughs and to have a really simple recipes from where you can create your own. As Ken recommend in the book, I also encourage you to experiment and try to change little by little the proportions and add different flours.   If you speak Spanish, and you are interested in the Spanish style bread, I recommend you Iban Yarza’s book, which is a classic among the baking enthusiast in Spain. Almost anyone that makes his own bread in Spain has this book in their bookshelf.   I promise that my next post is going to be the recipe, and shorter.  ","categories": ["Personal","Recipes"],
        "tags": ["bread","how to"],
        "url": "https://luispuerto.net/blog/2017/11/16/the-preliminaries-for-bread-and-pizza-making/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_4380.jpg"
},{
        "title": "My bread recipe",
        "excerpt":"Ok… as I promised in my previous post, here it’s my bread recipe for straight dough. I’m not going to fully explain the techniques, since they are already explained in the earlier post. There, you can also find what tools you need and, in case you want to change the ingredients quantities, the tables for bread and pizza.   For this recipe, I’m going to make a 4 hours rise / fermentation bread, but as you know, you can change the proportion of yeast in your dough to change the rise time. The proofing time is around 40 minutes and the total baking time is around 40 minutes. This gives us around 6 hours in total of preparation, if we take into account the mixing and handling of the dough. The temperature of baking is 250-270 ºC —depending on the oven and how brown you want to your bread.   Ingredients      Flour: 500 gr. 100% of wheat-white-fine-unbleached flour or puolikarkea vehnäjauho in Finland.   Water: 360 gr. 72%   Salt: 10 gr. 2%   Yeast: 6.00 gr. 1.20% of natural fresh yeast for 4 hours fermentation.      Directions      Measure 500 gr. of flour and put them in your mixing bowl with the 10 gr. of salt.   Measure the 360 gr. of warm water (~36ºC) and dissolve on it the 6 gr. of fresh yeast with a spoon.   When the yeast is fully dissolved, you can pour the water on the flour.   You can start mixing, using the pincer method and when the ingredients are fully mixed you can proceed with the folding.   Leave the dough fully rise for 4 hours in a covered container.   When the dough is fully risen, you can use the scraper to remove it from the container and drop it over the counter where you previously have sprinkled some flour.   Shape the dough as explained in the previous post.   Leave for proofing for at least 20 — 30 minutes, but remember that the recommended time is about an hour. Don’t forget to put into a plastic bag or cover it if you are going to leave it for an hour.   Remember to preheat the oven with the water container inside it before your loaf if fully proofed.        When the dough has finally proofed you can drop the loaf over the oven tray with the parchment paper and introduce it in the oven.         Keep the water container inside of the oven the next 20 minutes.   After 20 minutes, remove the water container if there is any water left in it and bake for another 10 min.   I usually at this point take the loaf out of the oven and flip it over quickly down side up and reintroduce it in the over for at least another 20 min. I want the bottom my loaves really well-baked. This way to get a really uniform dark brown color across all the loaf. Remember to pick up your loaf with the kitchen mittens if you don’t want to burn your hands. The loaf is really hot.   After 10 min. —or when you have reached the desired dark brown color— take out of the oven and put over a rack, if you have one, or something that allow the heat and the humidity to disperse. If you haven’t bake the loaf with the rack of the oven inside, that rack is usually fine.   After 30-40 min you can enjoy your bread.                        Final result              ","categories": ["Personal","Recipes"],
        "tags": ["bread","how to"],
        "url": "https://luispuerto.net/blog/2017/11/17/my-bread-recipe/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_4384.jpg"
},{
        "title": "Pizza recipe",
        "excerpt":"Since it’s Sunday let’s have some pizza for dinner. Here is the pizza recipe I usually make at home, and what I would call our standard pizza. It’s really rare that we don’t bake one once per week or at least two weeks.   In the same way as with the bread, you can read about the basics of the bread technique and find out a little bit more about how to bake in this post. Just remember that in addition to the normal baking tools and ingredientes you are going to need olive oil and a rolling ping to flatten and stretch the dough. The latter isn’t really necessary, but if you like really thin pizza you are going to need something —anything— to flatten the dough. Besides that, you are going to need whatever you want to put as toppings.   However, you have to keep in mind that you shouldn’t put a lot of topping over the dough for two reasons. First, because the principal ingredient here should be the dough. The final flavor has to be a combination of everything you put over the dough plus the dough, not just the toppings. If you can’t taste the dough you are doing something wrong and you’ve put too much toppings. Second, because if you load the dough with too much toppings you aren’t going to be able to bake the dough. Anything that you put in the oven is going release water, in the case of vegetables, or fat, in the case of meats. All of that released water is going to end up in the dough and it’s going to slow down the baking and if you increase the time of baking you can end up burning everything. It isn’t the first time I get too excited about to put my favorite ingredients with the result of cooked ingredients but a half backed dough. You also have to know that pizza shouldn’t be baked too much —just a little bit of Maillard reaction in the crust and that’s it, the rest of the pipe has to be soft. Pizza isn’t supposed to be hard like a cookie.                        Pizza in a traditional oven. Source Wikimedia              Pizza usually is traditionally baked a really high temperature (~500 ºC), much higher that the bread, and in short time (90 seconds) —at least the Neapolitan pizza. The key is to get baked the base, but not cook to much the toppings, to keep the moist on them and in the base. Yet, usually commercial home ovens aren’t able to reach that high temperature. I usually set the over for around 275 ºC that it’s more or less the maximum. In my oven the maximum is 300 ºC but I’ve put an oven thermometer inside once and it wasn’t able to reach that temperature and it stabilize around 275 ºC. So with that temperature we are going to need to bake our pizza for around at least 9-10 min. It really depends in the amount of toppings you put and what kind. I usually bake it for around 12 minutes. The key is to have a crispy and brown crust and a melted and a little bit brown in some spots mozzarella. Also, is good to have a fully baked base with some brown spots on it. Anyhow, use your judgement to get the result you really want.   Some people like to bake their pizzas at home in a pizza stone or similar. They also bake bread there. I plan to make a small note in the future about my experience baking in a stone, but to sum it  up a little bit, it makes things a lot of more complicated and the final results aren’t as better than when you bake your pizza of bread in a regular metal tray. I wouldn’t recommend it, and even less in the beginning.   So, to wrap it up. We are going to mix pizza dough and we are going to leave it rise for around 4 hours. Then, we are going to proof it for around 20 minutes and we are going to bake it for around 10-12 minutes at 275 ºC. This gives around a total of 5 hours if we take into account the mixing, the flattening of the dough and the topping.   Ingredients   Dough for the base      Flour: 160 gr. 100% of wheat-white-fine-unbleached flour or puolikarkea vehnäjauho in Finland. In the case of the four for pizza the general rule is as finer the better and it’s commonly said that you have to use double cero “00” flour. Anyhow, and again, is a mater of taste and where you live. I’ve tried the pizza flours you can buy here in Finland, or at last in my local grocery store, and I have to say they aren’t anything fancy, more the contrary. As a rule of thumb, I would say that any really fine and white flour is going to do the job for you.   Water: 112 gr. 70%   Salt: 3.2 gr. 2%   Yeast: 1.44 gr. 0.9% of natural fresh yeast for 4 hours of fermentation.   Olive oil:  A dash… I don’t have a defined amount and I just eyeball the thing.   Toppings      Tomato sauce: I use normal tomato sauce to which I add oregano and basil, but you can use whatever you want.   Oregano and basil: to add to the tomato sauce.        Fresh mozzarella cheese:  ~100 gr. I usually pick up a block of fresh mozzarella for pizza  of around 400gr. and I cut in four. I use one of the pieces and I freeze the rest of the pieces wrapped in aluminium foil, since mozzarella is something that easily get spoil. To use the frozen pieces I usually thaw them slowly, or if I’m in a hurry I put them in warm water. They are usually ready in an hour. You can use the already cut one if you prefer, I sometimes use that because it’s handy. Also keep in mind that although you can use fresh mozzarella for salad, the results aren’t going to be as good as if you use the special for pizza’s one.       White mushrooms: ~250 gr. Perhaps a little bit less. Keep in mind that if you put too much they are are going to release a lot of water. Be cautious.   Bacon: 4 slices, it’s more than enough. Think in the same way as with mushrooms, if you put too much, there is going to be a lot of fat and the dough isn’t going to get cooked.   Olives: I love olives so I put some if I have them at hand.   Big capers: The same with caper, I really like them, but I usually prefer the big ones instead of the small. Somehow they taste different to me.                        Big and small capers.              Directions      As in the same with the bread, we measure the flour (160 gr.) with scale and put in on the container where the rising in going to happen.   Measure the salt (3.2 gr.) and put with the flour.   Measure the water warm water (112 gr. at ~36 ºC) and dissolve the yeast on it (1.44 gr.).   Pour the water with the yeas with on the flour with the salt and add a dash of olive oil.           Mix everything thoroughly and leave it rising for 4 hours at room temperature (21 ºC). If you have your mozzarella frozen, it’s a good moment to take it from the freezer and thaw it. Remember to cover the dough while is rising.               When the dough is fully risen, you take it from the container and drop it over the counter where you have sprinkle some flour. There, in the same way as with the bread, you shape a ball and leave it proofing over the counter for around 20 min. You don’t need to cover them since it’s going to be just 20 minutes. However, if you are going to make more than one pizza, perhaps is going to be worth to take the third and the fourth ball to the fridge to slow a little bit the proofing and to keep the moisture. The best is to put them in a container, cover them and put in the fridge.          While the ball is proofing is the moment to prepare your toppings and warm up the oven. Slide your mushrooms as fine as possible and take your mozzarella and slide then too, if you are using fresh mozzarella in a block. I usually cut the bacon in four pieces and cut the big capers in four pieces too.   When your ball is fully proofed you take it and turn around and push it with your fist on the seam. Now you take your rolling pin, over which you have also sprinkled some flour, and begin to flatten the dough. I usually try to make it square, since my tray is square and I want to use the full tray, but you can shape it round if you prefer something classic. Use as much flour you need to avoid your dough to get stick to the rolling pin and to the counter. I usually apply the rolling ping a couple of times and then put more flour on the dough to turn it around apply more flour and the other side and then use the rolling pin in that side. I do that until the dough has been stretched and flatten enough that it’s slightly bigger than my tray.        When the dough is stretched enough I pick it up with my hands and put in on the tray with the parchment paper. There, I extend to the full size of my tray and with the scrapper I cut the parts that overflow the edges of the tray. You can use a scissors if you prefer, but I think with the scrapper is easier.             Now, you can begin you apply your toppings. First, a really thin, but really thin, layer of tomato sauce. Then, I apply the oregano and the basil. Next, the mozzarella slices, and after that the mushrooms. Finally, I put the bacon over the layer of mushrooms and I add the olives and the capers.             When you are happy with your toppings, you introduce your pizza in the oven for about 10-12 minutes. Towards the end, you should assess if you want to take it earlier from the oven or if you want to leave it a couple of minutes more. But, be really careful. At 270 ºC things go really fast and in a matter of a couple of minutes things can get scorched easily. If there is a lot of moist over the dough, you are right, you put too much toppings.             When you are happy how your pizza look, take it out and place it over a wooden cutting table, or somethings like that to proceed to cut it. Don’t leave on the tray because you want it to stop baking and don’t cut it in the tray because you don’t want to scratch your tray.              Enjoy your pizza!                        Pizza ready to be eaten              This is just an idea for a basic pizza with a basic toppings, but you can be as creative your want. I sometimes make a smoked salmon one with pesto and my mom likes a lot to put seafood like mussels, claps and shrimps. The only forbidden ingredient is pineapple.  ","categories": ["Personal","Recipes"],
        "tags": ["bread","how to","pizza"],
        "url": "https://luispuerto.net/blog/2017/11/19/pizza-recipe/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_4377.jpg"
},{
        "title": "About baking stones",
        "excerpt":"As I promised, here is a small note about my experience using baking stones and some things I think you should take into account if you decide to use one.   First of all, I think, if my memory doesn’t fail me, that I have used just one baking stone and it broke as a cause of the heat —while we were baking a pizza some pieces of the stone started to fall from the rack, creating a interesting scorched mess in the bottom of the oven composed of stone pieces, pizza dough, tomato and mozzarella. Until it broke, I use it extensively to bake pizza and bread.   Now, what do you have to take into account when you are baking over a stone, and what are the pros and the cons?   Pros      I think that the only pro you have when you are baking in a stone is the final result. Usually the stone is somehow porous and it pulls the moist of your dough which allow you to have a crispier results in your bread and pizza. Even more, in a place quite complicated as the bottom of the pizza or the loaf that it isn’t in direct contact with the hot air of the oven. Probably the results are going to be closer to a professional oven .   Cons      Warm up the stone is going to take more time that just warm up your oven. A minimum of 30 minutes, ranging as a normal time almost 1 hour. By no means you should put your dough over a not-enough-hot stone because you’re going to get the opposite effect. The bottom of your pizza or loaf if going to be under-baked.   You no longer going to be able to work on tray and you need to work on your pizza over a pizza / baking peel. Perhaps it’s something appealing for you, since you are going to look like a real baker, oh yeah! But it’s something that takes practice to master. Besides, it really limits the size fo your pipes and loaves. Usually the peels you are going to find aren’t going to be as big as your tray. The size of your pizzas is going to be reduced.   In the same way that you need to warm up your stone, you need to cool it down. If you take your stone from the oven too soon if can break due to thermal shock. Even when you are careful, your stone can break, like it happened to me. This cripples a little bit the capacity operation of your oven, extending even in two hours your normal baking time.   The results are good, but they aren’t, at least in my opinion, incredible good that justify all the fuss and the price. I get really good results with my regular metal tray, and in some cases, I would say that even better than with the stone, since I have more experience that before. Before you decided if you want to give a try to the stone, I would recommend to master your technique.   Although the purpose of the stone is to get more crispier result, since the stone pulls the moist of the dough, it going to get colder than the rest of the oven by the effect of the dough. In other words, where you put the loaf to get baked, it’s going to get colder. to avoid that, my grandpa —who was a professional baker— always told me that you have to move your loaves towards the end of the baking to a spot where there wasn’t a loaf previously. I guess that in the modern industrial ovens it’s less of problem since those are more powerful —and electric— than the previous and lumber heated ones that my grandpa was using. Anyhow, you are going to probably find this problem too in your home oven, and this is the reason because I take out the loaf and turn it over bottom up for the last 10 minutes of baking. I doubt you have a stone and oven so big that you are able to move your loaf around in the oven, so you are going to probably do as me. The stone is going to make this process slightly more difficult.   It’s somehow dirtier. You aren’t going to be able to use parchment paper anymore. I’ve seen some setups that use it in combination with the stone, but in my opinion you can’t use both. The parchment paper will stop the stone to take the moist of your dough while baking. If you bake just bread probably it’s going to be ok, but it’s quite often that you some ingredients fall over the stone when you are baking it. Even more if you need to use the peel to take it in and out.   Considerations   As you see… I’m not a big fan of baking stones although I recognize its appealing since they make things closer to the real thing. They aren’t cheap either and good ones could be really expensive, around 50 to 100 €. Besides, they are really heavy and delicate. In my opinion they are something that if you decide to use better to be a good on, thus you are going to expend money on it. I recommend you to think it over really carefully.   If you really want to give it a try, I think that the stone as thicker the better —it takes also more time to warm up and cool down— and that the tiles are a better solution than a really big stone since they more adaptable, easy to keep and probably as a result of their size some less dilatation and contraction, thus they be less prone to break. However, take into account that I never ever I’ve used them, so it’s just a guessing.   That’s my two cents about baking stones.  ","categories": ["Personal","Recipes"],
        "tags": ["bread","opinion"],
        "url": "https://luispuerto.net/blog/2017/11/19/about-baking-stones/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Pizza_on_stone.jpg"
},{
        "title": "Homebrew",
        "excerpt":"In some of my previous post [e.g. 1, 2, 3 or 4 ] I’ve been using Homebrew to install some pieces of software in my Mac using the terminal, or shell. But, perhaps you’ve been wondering what is exactly Homebrew? It defines itself as “the missing package manager for macOS”. Yet… what is a package manager? A package manager is a small piece of software that helps you to manage other software —packages— in your computer, or in other words, to install, update, setup and uninstall software. Package managers have been a classic in most linux distributions and most linux users are accustomed to the idea to install software through them, whether on a GUI (graphical user interface) or just on a CLI (command line interface). You as a macOS user probably are also used to a GUI package manager, the Mac App Store, that allow you to install Apps easily and keep it updated. It doesn’t uninstall it for you, or clean the config files, though.   However, you usually need more software that the one you can find in the Mac App Store, and even more if you are a bit of an advance user. For example, you probably need to install Java, Flash, Git, R (the stats software) or other apps that aren’t available on the Mac App Store for different reasons. To manage that software and apps Homebrew was created.                        List of all my installed formulas              Homebrew is a really simple thing, it’s just a Git repository full of Ruby scripts that you download to your machine and they install, update or uninstall the software with a given set of parameters or options that user can set. Those scripts are called formulas in the Homebrew terminology —as you soon will discover, Homebrew terminology is everything about beer. You can easily install Homebrew using your terminal with the following command.   1 $ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"   From there on, you can start to use Homebrew in the same way as any other command on terminal. To use Homebrew you just have to type info if you want more info about it. Homebrew install itself on the folder /usr/local/Homebrew and it’s going to install your software in the folder /usr/local/Cellar.                        The Cellar of Homebrew              When you install a formula with Homebrew, it usually creates symbolic links to the folder /usr/local —usually to the /usr/local/bin. but it could create more links to other folders there— to make that software available to the whole system. However, there are some formulas that aren’t linked to /usr/local and they are considered keg-only, or in other words, they are there just to be used for other formulas as dependencies. You can change that, though, and link it manually with brew link. Sometimes when you install a formula with Homebrew, you need to link it manually and/or force the linking, just because your system has already a symbolic link on /usr/local/bin for that same software but not manage by Homebrew. Like with the Git case.   1 $ brew link -f formula-name # -f is a flag to force the linking in case there were already a symbolic link on /usr/local/bin   What are the advantages of using Homebrew? Mainly that you can forget about download any software yourself and keep it updated manually. Now, you just can download and install your software with a simple command on your terminal, and update or uninstall it. After you install Homebrew you just need to type this in your shell to install a formula:   1 $ brew install formula-name   But you probably are wondering… How can I know the name of the formula I want to install? It’s quite easy, most of them are really intuitive, like for example Git one.   1 $ brew install git   Some other aren’t that easy, but Homebrew has a search engine included.   1 $ brew search formula-name   And of course, you can look on internet for them. There are even some websites that are specialized on formulas, like http://brewformulas.org.   One of the advantages of Homebrew is you can install more than one formula at once, and you can even create shell scripts to install, uninstall or configure several formulas.   If you want to install more than one formula, it’s pretty easy.   1 $ brew install formula-name-1 formula-name-2 formula-name-n ...   Another advantage of Homebrew is you can also choose how to install your software and you can even compile yourself from source code. If possible, Homebrew is going to install in your system a bottle by default, the binaries of the already compiled software. Yet, you can ask Homebrew to download the source code and compile it in your computer. Compiling has advantages and disadvantages. The main disadvantage is that it takes time, for some packages up to half an hour or more to compile, really depends on your machine. Also takes resources. On the other hand you can really personalize your install and the software, giving you fully control of the software you install in your computer, giving extras from the original or standard install. Even you can install software that is under development or it isn’t available for Mac yet.   For example, I have a full Homebrew R install that allow me to have the last version of R before is available for Mac in CRAN. Since I’ve compiled it myself, I’ve compiled with OpenBLAS which make R a little bit faster. I’ll try to make a post soon about my installation of R and how to do yourself one.   Finally one of the best things about Homebrew is you can update all the software installed with it just with one command. Ok, the true is you need two, but you can chain them on just one.   1 $ brew update &amp;&amp; brew upgrade   The first command is to update the list of formulas you have available in the Homebrew repository and download to your computer. It’s basically a upgrade, what you are doing is comparing the formulas you have installed with the Homebrew repository ones. If there is new versions of the formulas you have, Homebrew will installed them for you.   Besides of formulas, kegs and bottles, Homebrew has also taps. Taps are third-party repositories additional to the core Homebrew. I think that the most famous one is Homebrew-Cask caskroom/cask. Cask allows you to install GUI apps that are installed in /Applications folder of your Mac. To install Apps with cask is very simple.   1 $ brew cask install cask-name   There are several other taps you can add and most of them are to add functionalities or are thematic, like for example homebrew/science for formulas related to science packages or osgeo/osgeo4mac for formulas related to geography and GIS. As you’ve noticed the name of the taps are somethings like this: some-name/some-name. The reason for that is because taps are GitHub repositories by default although you can tap any git repository, even local ones, so the tap names correspond to github-username/repository. Therefore, and through this simple mechanism, you can create your own taps and share them with anyone, and, conversely, add anyone taps.   For example I’ve installed a tap that adds functionalities to Homebrew-Cask, since the update capabilities aren’t as great as with homebrew/core. The name of this tap is buo/cask-upgrade and you can check the repository here. With the original Cask functionalities you can only know which casks are outdated using brew cask outdate and force brew to “update” each of them with:   1 $ brew cask install -f cask-formula-name # -f to force to force the install of the new version   However if you use buo’s tap you just need to type:   1 $ brew cu   and that’s it. You can even add the flags -a and -y to autoupdate and to not ask questions. Yet, you have to be aware that some people on the team of homebrew cask doesn’t like buo’s tap or at the very least they think it has some caveats. So be careful when you implement it.                        brew cu on my machine              There are more commands you can use inside of Homebrew and Hombre-Cask. You can find more documentation about Homebrew here, and about Hombrew-Cask here.   I don’t want to finalize without sharing with you the line I use to update and upgrade my Homebrew.   1 $ brew update &amp;&amp; brew upgrade &amp;&amp; brew cleanup &amp;&amp; brew prune &amp;&amp; brew cu -ay &amp;&amp; brew cask cleanup   You already know the purpose of prune deletes the unnecessary symbolic links.   Happy brewing :beers:!  ","categories": ["Professional","Technology"],
        "tags": ["homebrew","how to","macOS"],
        "url": "https://luispuerto.net/blog/2017/11/21/homebrew/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/homebrew.jpg"
},{
        "title": "First Heavy Snowfall",
        "excerpt":"Last Sunday we had the first heavy snowfall of this winter season in Joensuu. As you can see in the photos in flickr gallery below, the landscape was really wonderful… quite like a postcard. Olalla and I decided to go outside after our late brunch and take a walk over the fresh snow to shoot some photos. It was a quite nice walk, also rather cold. The temperature was around zero, nothing compare with what we have ahead of us (probably -30 ºC). We welcomed the snow, not only because it’s beautiful, but because in November it’s quite a blessing. It makes everything brighter in the darkness. Bear in mind that we are in the middle of Kaamos, or Polar Night, and not until 9 a.m. there is some light to begin to fade away around 3 p.m. —or even earlier. We still have three weeks ahead until the trend is reversed and the light increases again. I’ll try to write about Kaamos in a future post.   Although we welcomed and enjoyed the snow on Sunday, the problem came on Monday.  Since the temperature is all the time around zero, snow is in quasi-liquid state, or in other words it’s really humid since it’s almost thawing and freezing all the time. This makes the floor really slippery and dangerous. It’s really madness to drive, even with studded tires, since it makes the car behave more like a boat than really a car. The car is skidding almost all the time, and you have to be really carefully shifting lanes or turning on the streets. Even more keeping a safe distance from the car in front of you because the braking distance doubles or triples. The problem is that snow and ice doesn’t stick to the tarmac until it’s around -6 or -10 ºC. Then the ice stinks to the road and the sudden tires are able to get grip on the ice, not as good as in dry conditions, but I would say that surprisingly good. Anyhow, I’m not going to deny that sometimes I enjoy a little bit the skid the situation, but most of the times one have to be really careful.              As you can see in the pictures above, the parking lot of the university wasn’t that fancy on Monday as our Sunday photos.   Let’s hope that the temperatures decrease a little bit and stabilize around -6 ºC, which becomes to be a really great temperature to enjoy the snow and drive safely.                        Joensuu’s weather forecast for the following days. Source FMI              PS/ Meanwhile we’ll try to enjoy the views.                        View from my window… only on the light hours, of course. Then is like this              ","categories": ["Personal"],
        "tags": ["Finland","Finnish life","Joensuu","snow"],
        "url": "https://luispuerto.net/blog/2017/11/29/first-heavy-snowfall/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/DSC8753.jpg"
},{
        "title": "My MacBook Pro late 2011's discrete graphics card said \"ciao\" 👋🏻 —again",
        "excerpt":"Last Sunday wasn’t really a pleasant day. On Saturday late night, or rather around Sunday 00.30 am, my MacBook Pro late 2011’s discrete graphic card begin to fail to in the end not being able to boot it properly. The computer was working just fine, it wasn’t even using the discrete card, connected to the external screen as I’ve been doing lately, or any doing any other intensive task. I just rebooted it and 5’ after loading the desktop a solid gray screen appear that allowed to do nothing. After I forced reboot pushing the on/off button, the normal loading gray screen had glitches as thin-horizontal-weird lines. After 3 or 4 boots into the desktop and then the gray screen of dead the computer begin to load directly just to the gray screen of death. Nothing could be done to load the computer normally. I tried safe mode, pressing shift key on boot, and nothing, just the same gray screen of death. Restore mode, alt + R on boot, also the gray screen of death. So, I decided to left the issue to sleep —it was around 1.30 am in the morning, and led the computer to make a hardware test, pressing D key on startup.   It’s not the first time that the discrete graphic card fails in my laptop. It’s a malaise that occurs to almost any 2011 Macbook Pro’s computer. I think there is two kind of machines out there, the ones where the issue already happened and the ones where it’s going to happen. My entire logic board was changed on July 2014 and 3 years and a half down the road it failed again last Sunday. Again, the same faulty chip. This is not proper quality Apple (AMD / Nvidia are also culprits here), and I don’t know whose idea was to mount those logic board / chips on this Mac model, but I think it wasn’t the brightest idea ever.   So, in Sunday morning, after I slept on the issue and I had a proper coffee and breakfast, I got my hands on to try to fix the problem. I begun with searching on internet about the problem and some people suggested that it was a RAM problem. Well, it could have been. The last time I did a hardware test, when I changed my hard drive in the US, it yielded a RAM problem, and this time threw me the same error.                        Error 4MEM/66/40000000: 0x846b3798              So I decided to give it a try to this guy’s suggestion and I even switched the RAM for the original ones, since I upgraded 4 years ago to two 8 GB modules. However, the result was always the very exact one. So I decided to ditch the idea that the RAM was the cause of the grey screen of death, reinstalled the RAM and looked for another solution.   Finally, I found these solutions (1, 2, 3 and 4), which were quite recent. It seems that a lot of boards are dying all over again. The solution is quite simple, since what is failing is the discrete graphic card and you usually don’t use it in your daily life, you just have to stop using that card at all, and use all the time the integrated one. Problem solved. But, how do you make that happen when the only thing you are able to see is the grey screen of death? Basically you make the computer to not boot using the drivers for the discrete graphic car. The solution works, but there is some caveats, under High Sierra. One is that your Mac is not longer going to sleep properly, but it can be fixed just changing the sleep mode to hibernation. And that the bright control is not longer going to work, at least at the moment. I can live with that.   I’m going to summarize here what I’ve done to fix it under High Sierra macOS 10.13.1 on my Late 2011 MacBook Pro.   The solution   First of all, I have to say that I begun with the solution number 1 and then I switch to the 2, 3 and 4 ones. Probably, you are fine to proceded from the solution number 2 onwards. To make make those solutions work in the long run, you are going to need a USB stick in order to have a way of booting your Mac, from the very beginning, or when you update your system and the fix stop working as a consequence of the update. You don’t need a very big USB stick. What you are going to store on it it’s not going to more than 10 mb.   Moving the kexts   This step isn’t really necessary, and you can jump to the next step. However, this is the exactly how I did things, since first I found one solution and then the others. Also, if you have trouble booting with the other solutions, perhaps this part is going to help you.   You need to boot on single user mode (press and hold `Cmd + S + R ) and run the following commands.   1 2 3 4 5 6 7 8 9 $ fsck -fy # to check a disk $ mount -uw / # mount a root filesystem with read/write permissions $ sudo mkdir /AMD_Kexts/ # make a directory to store the AMD drivers in case you'll need them in future $ sudo mv /System/Library/Extensions/AMD*.* /AMD_Kexts/ # move the AMD drivers $ sudo rm -rf /System/Library/Caches/com.apple.kext.caches/ # remove the AMD drivers cache $ sudo mkdir /System/Library/Caches/com.apple.kext.caches/ # just in case OS X will be dumb and will not recreate this directory, I am creating it for OS X $ sudo touch /System/Library/Extensions/ # to update the timestamps so that new driver caches - without AMD drivers - will be definitely rebuilt $ sudo umount / # umount a partition to guarantee that your changes are flushed to it $ sudo reboot   However, in the same way as the solution’s poster, when I tried to delete the kext the Mac was throwing me the error operation not allowed or something similar. Probably because in the same way as s/he, I have my disk locked as “read-only” after too many attempt of booting. Lucky, I didn’t need to mount my disk on Linux, as s/he did. I just took my disk out of my Mac and put it in a USB enclosure that I connected to another Mac with High Sierra. I have High Sierra installed in my machine with the new APFS, so that means that my disk in only readable by other Macs with High Sierra installed. Dangerous, yes, but I wanted to take advantage of the new features. Backups were invented for some reason.   From there, I just needed to performed the same commands but a little bit different. You have to take into account that in macOS your hard drive is going to mount automatically, so it wasn’t necessary to mount it like before, and you just have to run the rest of the commands with the proper path and the name of your drive. In my case my hard drive name is Macintosh SSD, and in this Mac there is also a Macintosh SSD, so when it mounted my hard drive macOS renamed it to to Macintosh SSD 1, In the shell you have to proper indecate the blank spaces on name and paths using the backslash symbol \\, therefore I could access to my hard drive using the name Macintosh\\ SSD\\ 1. Mind the name of your hard drive (usually Macintosh HD) and change the path in the commands in consequence.   1 2 3 4 5 6 $ sudo mkdir /Volumes/Macintosh\\ SSD\\ 1/AMD_Kexts/ # make a directory to store the AMD drivers in case you'll need them in future $ sudo mv /Volumes/Macintosh\\ SSD\\ 1/System/Library/Extensions/AMD*.* /AMD_Kexts/ # move the AMD drivers $ sudo rm -rf /Volumes/Macintosh\\ SSD\\ 1/System/Library/Caches/com.apple.kext.caches/ # remove the AMD drivers cache $ sudo mkdir /Volumes/Macintosh\\ SSD\\ 1/System/Library/Caches/com.apple.kext.caches/ # just in case OS X will be dumb and will not recreate this directory, I am creating it for OS X $ sudo touch /Volumes/Macintosh\\ SSD\\ 1/System/Library/Extensions/ # to update the timestamps so that new driver caches - without AMD drivers - will be definitely rebuilt $ sudo umount /Volumes/Macintosh\\ SSD\\ 1/ # umount a partition to guarantee that your changes are flushed to it&lt;br&gt;   Now, you can take your disk, reinstall it in your Mac and begin from there. I booted to something like this1:                        Booting without kexts.              Using solutions 2, 3 and 4   That’s beginning… at least now I knew that my computer can be booted. Then, I decided to switch solutions and continue with the fix explained in 2, which is fully explained in 3. The reason… because seemed more recent and better explained, and more stable in the long term. So, as it’s detailed in that solutions, first reset the SMC and the NVRAM. Then, boot your Mac on recovery single user mode (pressing and holding Cmd + S + R) and run the following commands.   1 2 3 $ nvram fa4ce28d-b62f-4c99-9cc3-6815686e30f9:gpu-power-prefs=%01%00%00%00 $ csrutil disable $ reboot   Now, and since you moved the GPU kext from their original location you are going to boot to something like this.                        Booting normally              Hey!! you probably are thinking… I’ve done it, I’ve fixed. it. Yes &amp; not. Since you’ve moved the kexts from their original place things are working more or less correctly, however, if you updated the system, you are going to probably have to move your kext and apply the solution again. For that reason, they decided to created a nicer solution implementing GRUB on your start up disk and blocking the loading of those kexts on booting. GRUB is the same system you put in place when you are installing Linux in a computer and you want to have more than one OS in the machine. This solution allow us to keep the kexts in place and boot without loading them. The solution isn’t perfect, but at least is easy to implement and in case you install a system update you can easily reimplement.   At this point, I recommend to move the kexts to the original location /System/Library/Extensions/.                        Moved kexts              You can do it dragging and dropping those back to its original location (it’s going to ask for your password), or you just can move then with terminal:   1 $ sudo mv /AMD_Kexts/*.* /System/Library/Extensions/   Getting a GRUB   Now, to implement the complete solution you have to download ubuntu to take the GRUB from there. I’ve downloaded Ubuntu 17.102, as it’s specified in the fix. When you’ve downloaded the .ISO, you have to attach and mount it, so assuming that you have the ISO in downloads:   1 $ hdiutil attach -nomount ~/Downloads/ubuntu-17.10-desktop-amd64.iso   Which probably turns something like this:   1 2 3 /dev/disk2              Apple_partition_scheme /dev/disk2s1            Apple_partition_map /dev/disk2s2            Apple_HFS   The disk number could be different, for example in my case the first time was disk3, but mounted a second time and was disk2. Depends on how many disks have you mounted before.   Now you can finally mount the ISO with the following commands:   1 2 3 $ mkdir /tmp/ubuntu $ mount -t cd9660 /dev/disk2 /tmp/ubuntu/ $ open /tmp/ubuntu/shell   Preparing the USB stick and editing GRUB file   You have to format you USB stick to FAT32 and it’s recomendable to name it boot folders from the ISO to your USB stick root.                        Folders you have to copy              When you have those folders on your USB stick, you have to edit the file /RESCUE/boot/grub/grub.cfg. I like Atom to edit, but perhaps you don’t have it installed so if you type:   1 $ open /Volumes/RESCUE/boot/grub/grub.cfg   Your default text editor will open. In case you want to be sure and open with Text Edit:   1 $ open -a TextEdit /Volumes/RESCUE/boot/grub/grub.cfg   Then you have to delete all the file content and paste the following.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 if loadfont /boot/grub/font.pf2 ; then     set gfxmode=auto     insmod efi_gop     insmod efi_uga     insmod gfxterm     terminal_output gfxterm fi  set menu_color_normal=white/black set menu_color_highlight=black/light-gray  set timeout=0 menuentry \"macOS\" {     outb 0x728 1     outb 0x710 2     outb 0x740 2     outb 0x750 0     outb 0x714 0xFF     exit }   Take into account that is a specific GRUB configuration for High Sierra and with a single OS installed. If you have more that one OS or you are under other OS check this. Note that I’ve added a line to the original proposed GRUP file at almost at the end. This is because someone suggest that line on this thread. I’ve also set the timeout variable to zero, since I don’t want to push enter to continue or wait 10 seconds. I don’t have anything to chose from.   Save the file and let’s copy now to you Mac. Or perhaps, you can test if this works properly rebooting using the USB stick. For that you have to reboot and then push alt after you hear the chime, introduce the USB stick and choose it on the menu. You should be able to reboot normally.   Making it permanent   Now you can make this permanent and without need the USB. You run on terminal the following commands with your USB still plugged and assuming that you named it RESCUE.   1 2 3 4 5 6 7 8 9 $ cd /Volumes $ sudo mkdir efi $ sudo mount -t msdos /dev/disk0s1 /Volumes/efi $ sudo mkdir /Volumes/efi/boot $ sudo mkdir /Volumes/efi/EFI/grub $ sudo cp -R /Volumes/RESCUE/boot/ /Volumes/efi/boot $ sudo cp -R /Volumes/RESCUE/EFI/boot/ /Volumes/efi/EFI/grub $ sudo bless --folder=/Volumes/efi --file=/Volumes/efi/EFI/grub/grubx64.efi --setBoot $ sudo bless --mount=/Volumes/efi --file=/Volumes/efi/EFI/grub/grubx64.efi --setBoot   Now… you can unmount the USB and boot without it.   Preventing GPU from waking up from sleep   When you are under High Sierra, the next step doesn’t really work, and even installing this kext is going to return to a black screen when you return form sleep. To prevent this, you can just change the way your Mac sleep and make it hibernate. If you have a SSD in place, like I have, the difference on time between waking up from a normal sleep than from hibernation is going to be neglectable.  The only real difference is, your computer isn’t going to wake up when you lift the lid and you have to push the on/off button to wake your Mac up. To set this up you have to run on terminal:   1 $ sudo pmset -a hibernatemode 25   If you want to return to the normal sleep mode you set in the following way:   1 $ sudo pmset -a hibernatemode 3   Anyhow, I decided to apply the solution as it’s explained here and I even created a kext myself for High Sierra. Just in hopes that in a close future things improve I can normally sleep. If you download the kext you just have to unzip it and then copy to /Library/Extensions and run the following commands.   1 2 3 $ sudo chmod -R 755 /Library/Extensions/AMDGPUWakeHandler.kext $ sudo chown -R root:wheel /Library/Extensions/AMDGPUWakeHandler.kext $ sudo touch /Library/Extensions   And reboot.   Now, you are done.   Reimplementing the fix   If you need to reimplement the solution because you updated the system you are going to need to just bless GRUB disk again —make it a bootable disk. You just boot from the rescue USB stick and run the following commands on terminal3   1 2 3 4 5 $ cd /Volumes $ sudo mkdir efi $ sudo mount -t msdos /dev/disk0s1 /Volumes/efi $ sudo bless --folder=/Volumes/efi --file=/Volumes/efi/EFI/grub/grubx64.efi --setBoot $ sudo bless --mount=/Volumes/efi --file=/Volumes/efi/EFI/grub/grubx64.efi --setBoot   Final note   As I’ve mentioned earlier, there are only two caveats to this solution if you are under High Sierra. One is that you lose the ability to sleep you computer normally and you have to hibernate. Nothing really serious if you have an SSD. The other is more serious… or at least is going to affect you more in your daily basics. You are not going to be able to control the brightness of your computer anymore. It’s something that MacRumors community is trying to fix.   Anyhow, the weir way that High Sierra manages the bright isn’t something new. And during the betas, some people commented about this on the developers forum as well, here and here.   Why this happens?   You’re probably wondering yourself why this is happening to your computer although if you have researched a little bit about the problem you probably already know what is the problem. Really bad quality graphic chips that due to heat in the computer finally fails. You can learn a little bit in this video.            To sum it up… AMD / Nvidia chips mounted in logic board of this model are really bad and I guess most of them are faulty. The lack of proper ventilation on Macs, specially in this models, doesn’t improve things and make then even more prone to this kind of issues. There is a technique called reballing, which basically consist repair the littles balls that the chip uses to connect to the logic board. However, this is just a temporal solution since the problem is in the chip itself. Reballing works just because when you heat up the chip to reball it you mess with some of the internal parts of the chip —I think they call this reflow— and that makes it to work properly again, but after a while the problem is going to return. I’ve read that some people just put the board in the oven at around 200ºc to fix the issue, but I guess doesn’t last a lot either.   The only long working solution is just get another board, or just another chip, and pray that this time it isn’t faulty. Change the board isn’t complicated, at least not incredible. But invest 400€, or even more, in a 6 years old computer is something you really have to ponder.   Anyway, the only ones here to blame is AMD / Nvidia and or course Apple, they knew the issue all this time and they’ve done nothing to fix it. And for that reason Apple is being sued.   It’s time to think if it’s really worth to spend the money Apple tags their computers. I’m really willing pay, but I expect a top-notch silent computer and service.  Besides, it seems that Apple has forgotten that professional customers really need professional computers and the last batch seems that isn’t up to the mark and Apple has received a lot of backlash for it. So big has been the adverser reaction that Apple has to even make an statement addressing it.   Related issues?   While I was in the US we got a beautiful 24” cinema display that I connected to my machine and we enjoyed a lot. At that moment, I was concerned by the extensive use I was doing of the dGPU when connected to the external screen and I asked in the Apple communities about the issue. They replied that it was the intended way to use the computer and I shouldn’t worry. I don’t blame then, because what they told me, was true, it was/is the normal way to use the computer and how it was designed.  Perhaps bad designed, or at least not really thoroughly thought. When you connect the computer to a external screen you trigger the use of the dGPU, not because you really need it —you can perfectly work with the integrated as other models do, but because the thunderbolt connection where you connect the external screen leads directly to the graphic card there is no way to avoid it’s use. This cause an increase of temperature of more or less 20º from the normal operation, just for connecting a external display, and  in my humble opinion there is nothing of extensive use in connecting it to a external display.   In the same way… An almost month ago I installed High Sierra and I noticed a weird error related to graphics on the console:                        [ERROR] — Unknown CGXDisplayDevice: 0x41dcd00              The error is still there right now, and I told Apple about it when I noticed. They contacted me and collected some data. They never return to me about this issue. Now, I wondering if this two issues are connected and High Sierra accelerated the degradation process of the Nvidia / AMD chip.                  To be entirely honest I don’t remember if after deleting the kexts I was able to boot directly to the striped desktop or I needed to run the commands in the beginning of solution 2. &#8617;                  Previusly I downloaded a spefific release of Ubuntu. I’ve changed to the last one. &#8617;                  Today, 8th of December 2017, I decided to install the update to 10.13.2. It worked really well, and even booted without needed to apply the fix again. In other words, the discrete GPU was working. However, after a while fail, and took me more than what I wanted to reestablish everything. So my recommendation is, if you update, apply the fix as soon as possible, but cause sooner or later things are going to go south and it’s going to take you even more time to fix it. In my case I needed to apply the solution almost from the very beginning and a NVRAM reset was necessary to be able to operate again the computer. Good luck! &#8617;           ","categories": ["Personal","Professional","Technology"],
        "tags": ["dGPU","graphic card","high sierra","how to","macOS"],
        "url": "https://luispuerto.net/blog/2017/12/05/my-macbook-pro-late-2011s-discrete-graphics-card-said-ciao-again/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/grey_screen.jpg"
},{
        "title": "Disconnecting the dGPU in a late 2011 MacBook Pro —another way",
        "excerpt":"As I’ve told in the previous post, the discrete graphic car of my MacBook Pro late 2011 is faulty and can be trusted any more. So I decided to disconnect it.   Yesterday, I updated the system to 10.13.2 and although in the beginning everything was working fine without enforcing the dGPU disconnection, the graphic card later failed and I had to apply the fix again. For some reason the fix didn’t working as well as it was working before so after too much booting and trying I decided to take the middle way. Besides, the GRUB soliton was nice, but it made the booting much more slower.   What I’ve done basically is move the kexts from the extensions folder to other place and apply the wake up handle.   In other words this are the steps. Please keep in mind that I’m running High Sierra in my machine.           Reset the SMC and the NVRAM.            Boot your Mac on recovery single user mode (pressing and holding Cmd + S + R) and run the following commands.       1 2 3  $ nvram fa4ce28d-b62f-4c99-9cc3-6815686e30f9:gpu-power-prefs=%01%00%00%00  $ csrutil disable  $ reboot           You are going to reboot to your normal desktop. Now you can move the GPU kexts to other place:     1 2 3 4 5  $ sudo mkdir /AMD_Kexts/ # make a directory to store the AMD drivers in case you'll need them in future  $ sudo mv /System/Library/Extensions/AMD*.* /AMD_Kexts/ # move the AMD drivers  $ sudo rm -rf /System/Library/Caches/com.apple.kext.caches/ # remove the AMD drivers cache  $ sudo mkdir /System/Library/Caches/com.apple.kext.caches/ # just in case OS X will be dumb and will not recreate this directory, I am creating it for OS X  $ sudo touch /System/Library/Extensions/ # to update the timestamps so that new driver caches - without AMD drivers - will be definitely rebuilt                Take the AMDGPUWakeHandler.kext and copy it to &lt;span /Library/Extensions then run the following commands       1 2 3  $ sudo chmod -R 755 /Library/Extensions/AMDGPUWakeHandler.kext  $ sudo chown -R root:wheel /Library/Extensions/AMDGPUWakeHandler.kext  $ sudo touch /Library/Extensions                Make sure to have change the way the system sleeps:       1  $ sudo pmset -a hibernatemode 25           Now you can reboot.        Perhaps it’s recomendable to re-enable the SIP. To do that just boot your Mac on recovery single user mode (pressing and holding Cmd + S + R) and run:       1 2  $ csrutil enable  $ reboot           This solution perhaps it’s a little bit easier, but has the disadvantage that you need to apply it all every time you update the system.   Anyway, I recommend to read the previous post to understand fully what is going on.  ","categories": ["Personal","Technology"],
        "tags": ["dGPU","graphic card","high sierra","how to","macOS"],
        "url": "https://luispuerto.net/blog/2017/12/08/disconnecting-the-dgpu-in-a-late-2011-macbook-pro-another-way/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/grey_screen.jpg"
},{
        "title": "Disconnecting the dGPU in a late 2011 MacBook Pro —third way",
        "excerpt":"Update 2017.12.12-14.20 EET: whether or not I use the AMDGPUWakeHandler and whether I sleep or hibernate I can’t wake up of the hibernation/sleep. Depending on if I have AMDGPUWakeHandler on or off I get different outputs, but none of those end in a successful wakeup.   Update 2017.12.13-09.08 EET: After checking about the wake up problem after sleeping / hibernating with the people form MacRumors, I reached some conclusions.      You don’t use AMDGPUWakeHandler with this solution, since it could create a kernel panic… so for that reason doesn’t work.   pmset gpuswitch option can help, but it’s really undocumented officially, so we really don’t know what the values for 0, 1 and 2 stand for, and can vary from machine to machine I guess, or at least to macOS version to version.   gfxCardStatus can help since the problem after wake up is the dGPU activates and the computer freezes.   I’ve updated steps 11 and 12 in consequence.   Update 2017.12.18-14.42 EET: I’ve tried to wake up from hibernation without gxfCardStatus and it worked pretty well I didn’t have any issue, so if you don’t want to have it installed or at least running in the background I think it’s OK.   Update 2018.01.09-21.35 EET: After install the security update to mitigate the effects of Spectre, I have to apply the fix again as explained here. Everything worked fine, but on wake up of hibernation I got a black screen a couple of times. Also the computer didn’t turn off and got stuck in a black screen. I really don’t know what is the reason, but seems it’s related to the gpuswitch parameter. I changed to 0 and then to 2 again, and seems that everything is normal again. But I don’t know if it’s really that or it’s other thing.   Update 2018.01.21-09.20 EET: I just installed macOS update 10.13.3 and after testing a little bit I got to the conclusion that what work best for me is to set pmset -a gpuswitch 1     Ok!!!!!! There is a third, and I think final, solution to totally deactivate the dGPU. Till this moment this is my favorite solution and I even have the brightness back to my computer. Also it sleeps correctly. You can check my previous post also 1 &amp; 2.   We all have to thank to MacRumors community that all of them have been working really hard to create a workable solution to all of us. This guide is almost a exact copy of the one posted by MikeyN here. I’ve just changed somethings and added the AMDGPUWakeHandler to manage the sleep.   The fix   Let’s explain how it’s done:      As always you have to reset SMC and PRAM/NVRAM before you do anything else.            SMC: shutdown, unplug everything except power, now hold leftShift + Ctrl + Opt/Alt + Power for about 10” and release at the same time.       PRAM/NVRAM: with the power cord on, power on and immediately later and before the chime hold cmd + Opt/Alt + P + R at the same time until you hear the chime for the second time. Try to do the following step just right after, so you don’t let the computer to load —and fail.                Now, boot into recovery single user mode by holding: cmd + R + S. When finish to load, you run:       1 2 3 4 $ csrutil disable # to disable SIP $ nvram fa4ce28d-b62f-4c99-9cc3-6815686e30f9:gpu-power-prefs=%01%00%00%00 # to disable the dGPU on boot. $ nvram boot-args=\"-v\" # Load in verbose mode $ reboot                Reboot in single user mode holding on boot cmd + S            Now we are going to mount the hard drive and move the driver of the dGPU AMDRadeonX3000.kext out of the drivers folder.       1 2 3 4 5 $ /sbin/mount -uw / # mount root partition writeable $ mkdir -p /System/Library/Extensions-off # make a kext-backup directory $ mv /System/Library/Extensions/AMDRadeonX3000.kext /System/Library/Extensions-off/ # only move ONE offending kext out of the way $ touch /System/Library/Extensions/ # let the system update its kextcache $ reboot                Now you’re going to be able to load your desktop normally, but with an accelerated iGPU display. However, the system doesn’t know how to power-management the failed AMD-chip, so you are going to need to load it manually.       1 $ sudo kextload /System/Library/Extensions-off/AMDRadeonX3000.kext                You can automate the loading with the doing the following:       1 2 $ sudo mkdir -p /Library/LoginHook # Creating a folder to store the script. $ sudo nano /Library/LoginHook/LoadX3000.sh # Creating the script.                On nano you type/paste:       1 2 3 4 #!/bin/bash kextload /System/Library/Extensions-off/AMDRadeonX3000.kext # pmset -a gpuswitch 0 # to prevent to switch to the dGPU exit 0          * I’ve decided to comment the line 3 since I’m not sure that 0 is the correct value. Besides, in the step 12 I set gpuswitch 2.            You make it executable active:       1 2 $ sudo chmod a+x /Library/LoginHook/LoadX3000.sh $ sudo defaults write com.apple.loginwindow LoginHook /Library/LoginHook/LoadX3000.sh                This is what I like the most. You create a script in the root of your hard drive to automate the process in case of an update.       1 $ sudo nano /force-iGPU-boot.sh # Creates the script in the root          With the following content.       1 2 3 4 #/bin/sh sudo nvram boot-args=\"-v\" sudo nvram fa4ce28d-b62f-4c99-9cc3-6815686e30f9:gpu-power-prefs=%01%00%00%00 exit 0                Now you make it executable and I hide to avoid delete it:       1 2 $ sudo chmod a+x /force-iGPU-boot.sh # make ir executable $ sudo chflags hidden /force-iGPU-boot.sh # hide the file.          In the future if you reset SMC and PRAM/NVRAM you just have to load in single user mode holding cmd + S and run:       1 $ sh /force-iGPU-boot.sh           Then, you can copy the AMDGPUWakeHandler, or the one I created AMDGPUWakeHandler.kext, to /Library/Extensions and run the following commands:     1 2 3 $ # sudo chmod -R 755 /Library/Extensions/AMDGPUWakeHandler.kext $ # sudo chown -R root:wheel /Library/Extensions/AMDGPUWakeHandler.kext $ # sudo touch /Library/Extensions                This time you don’t need to change the way the machine sleeps. And you can reboot       I recommend you the way the machine sleeps to hibernate, but I haven’t tested if it can sleeps normally after we apply the following step       1 $ sudo pmset -a hibernatemode 25           All the fuss about the wake up after sleep / hibernate is related to when the computer wake ups checks the GPUs and somehow it gets stuck to dGPU. For that reason some people has changed the variable gpuswitch in pmset. Nevertheless, this variable is really undocumented and you find explanations to what the values to that variable (0, 1 and 2) do on internet. At this moment I have it set as default 2 I’ve decided to change to 1.       1 $ sudo pmset -a gpuswitch 1           Which I thing it’s the default value. The default value is 2.       You can try the different values, reboot and then close the lid and wake up and see the results.       What has worked for me is leave it in 1 and install gfxCardStatus, and every time I boot change to integrated only. But still testing. I’ve tried to wake up from hibernation without gfxCardStatus and it also worked, so I guess it’s optional. gfxCardStatus can help, but at least I’m not using it right now. I wake up from hibernation without it perfectly.            After you reboot and if everything goes smoothly, perhaps you wan to return to the normal boot mode. You can room in terminal:       1 $ sudo nvram boot-args=\"\"           Now I just going to copy paste from MikeyN’s post      This setup has now one kext in a place Apple’s installers do not expect. That is why in this guide SIP has not been reenabled. If an update that contains changes to the AMD drivers is about to take place it is advisable to move back the AMDRadeonX3000.kext to its default location before the update process. Otherwise the updater writes at least another kext of a different version to its default location or at worst you end up with an undefined state of partially non-matching drivers.     After any system update the folder /System/Library/Extensions has to be checked for the offending kext. Its presence there will lead to e.g. a boot hang on Yosemite and Sierra, an overheating boot-loop in High Sierra.     Further: this laptop is overheating, no matter what you do. The cooling system is inadequate and the huge number of failing AMD chips are just proof of that.    In case you have to update   So, before you update the system, please remember to run:   1 $ sudo cp -r /System/Library/Extensions-off/AMDRadeonX3000.kext /System/Library/Extensions/   Then you update. If you can’t normally load your computer, you can hold cmd + S and run:   1 $ sh /force-iGPU-boot.sh   Then you can reboot again on single user mode holding cmd + S and then run   1 $ sudo mv /System/Library/Extensions/AMDRadeonX3000.kext /System/Library/Extensions-off/   To move again the kext. Keep in mid that the other one still there do you are going to probably rename it in this fashion:   1 $ sudo mv /System/Library/Extensions/AMDRadeonX3000.kext /System/Library/Extensions-off/AMDRadeonX3000-1.kext   Checking that everything is OK   If you run in terminal   1 $ kextstat | grep AMD   You have to get something similar to this:   1 2 3 4 111    2 0xffffff7f82da8000 0x122000   0x122000   com.apple.kext.AMDLegacySupport (1.6.0) 3BE3756A-6D69-3CD0-B18A-BC844EE2A4DF &lt;105 12 11 7 5 4 3 1&gt; 130    0 0xffffff7f83631000 0x12e000   0x12e000   com.apple.kext.AMD6000Controller (1.6.0) DC45A18B-6F81-38D5-85CB-06BFBD74B524 &lt;111 105 12 11 5 4 3 1&gt; 146    0 0xffffff7f83126000 0x22000    0x22000    com.apple.kext.AMDLegacyFramebuffer (1.6.0) 5F948DD4-8D1E-31BD-A7EE-C44254CBA506 &lt;111 105 12 11 7 5 4 3 1&gt; 174    0 0xffffff7f83ab3000 0x56c000   0x56c000   com.apple.kext.AMDRadeonX3000 (1.6.0) 7E721EBE-AD4B-3C53-A70A-1FFF3C231968 &lt;173 147 105 12 7 5 4 3 1&gt;   In the beginning I wasn’t getting any of these and the system just loaded AMDRadeonX3000. That resulting in a little bit of overheating in the dGPU and I wasn’t able to sleep the computer. The reason for the system to not load the kext was I moved them that much that I changed the ownership of the files. If that is your case you can run in terminal:   1 2 $ sudo chown -R root:wheel /System/Library/Extensions/AMD*.* $ sudo chown -R root:wheel /System/Library/Extensions-off/AMD*.*   to return the ownership to the System / Root   Let’s hope that everything goes smoothly from now on. You have to see the bright side of life, now you have a quite cold running Mac since the dGPU is totally deactivated. After a while your temps have to be something similar to this:                        GPU diode and GPU proximity are the dGPU sensors. GPU PECI is the iGPU sensor.             ","categories": ["Personal","Technology"],
        "tags": ["dGPU","graphic card","high sierra","how to","macOS"],
        "url": "https://luispuerto.net/blog/2017/12/11/disconnecting-the-dgpu-in-a-late-2011-macbook-pro-third-way/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/grey_screen.jpg"
},{
        "title": "Happy New Year 12018 &#x1f389;",
        "excerpt":"I want to wish you all a   Happy New Year   12018   🎉   And remember that:   Then                 And Now                 The Sky Is The Limit            There Are Always Reasons   To Be Optimistic            And That You Get   “More Bonus Points If You Help To Build A Galactic Human Empire”.  ","categories": ["Personal"],
        "tags": ["New Year"],
        "url": "https://luispuerto.net/blog/2017/12/31/happy-new-year-12018/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/happy_2018.jpg"
},{
        "title": "iTerm2 + Oh My Zsh + Powerlevel9k + Monaco Nerd Complete Font",
        "excerpt":"In general, I don’t use my Mac’s Terminal app. Instead, I use iTem2 with a special configuration, that doesn’t use Bash, but Oh My Zsh as a shell, that is a framework to manage Zsh configuration as your shell. This framework allows you to install plugins or configure your prompt, among other cool things. I’ve also configured iTerm2 to work with a patched Monaco1 font with the complete collection of nerd glyphs. The result is more of less what you can see in the featured image in this post, a beautiful and elegant shell that you can configure and enjoy use.   How you can get something similar? Reach this configuration is quite easy. These are the directions:   Install iTerm2   I would begin installing iTerm2. iTerm2 is just an app similar to Terminal, but with steroids. It has far more options and even have mouse support.   To install iTerm we are going to use Homebrew:   1 $ brew cask install iterm2   Now that you have iTerm2 you have to install Oh My Zsh.   Install Oh My Zsh   To install Oh My Zsh you need to have installed in your system Git. Usually that is not a problem because Mac comes with its own Git, but remember that you can update to the last version easily using Homebrew.   However, you can’t install Oh My Zsh itself using Homebrew, but you can use cURL or Wget, which probably you have already installed in your system. If you don’t have any of those, you can install them through Homebrew. To install Oh My Zsh you can run the following commands:   1 $ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"   or   1 $ sh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\"   Oh My Zsh has a autoupdate feature, so don’t worry about update. From time to time, it’s going to ask you to check the repo where it’s stored for updates.   Installing Powerlevel9K theme   Powerlevel9K is a Oh My Zsh external theme that gives it that awesome look and the capacity to configure the prompt, yet keep it light. There are literally dozens of themes, whether included in the Oh My Zsh repo or external ones, and Powerlevel9K is one of the external ones, so you have to download (clone the repo) and store it on the custom part for the Oh My Zsh configuration folders. To do so, just run the following command in your terminal.   1 $ git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k   Configuring Oh My Zsh &amp; PowerLevel9K   When you have Oh My Zsh installed you can begin to configure. In order to do that you have to open the configuration file, which is located in your user folder, with your favorite text editor. In my case I like to use Atom, so I run the following command in the terminal.   1 $ atom ~/.zshrc   Bellow you can see my configuration file. The important lines are highlighted.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 # Setting language and localization variables export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8  # Setting $JAVA_HOME export JAVA_HOME=\"$(/usr/libexec/java_home)\"  # Setting $PATH ## If you come from bash you might have to change your $PATH. export PATH=$HOME/bin:$PATH export PATH=/usr/local/sbin:$PATH # export PATH=/usr/local/bin:$PATH ## Path for LLVM install export PATH=/usr/local/opt/llvm/bin:$PATH ## Path for Golang export GOPATH=$HOME/golang export GOROOT=/usr/local/opt/go/libexec export PATH=$PATH:$GOPATH/bin export PATH=$PATH:$GOROOT/bin  # Setting R variables export R_LIBS_USER=$HOME/Library/R/3.x/library  # Path to your oh-my-zsh installation. export ZSH=~/.oh-my-zsh  # Set name of the theme to load. Optionally, if you set this to \"random\" # it'll load a random theme each time that oh-my-zsh is loaded. # See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes # ZSH_THEME=\"agnoster\"  ZSH_THEME=\"powerlevel9k/powerlevel9k\" # POWERLEVEL9K_MODE='nerdfont-complete' POWERLEVEL9K_MODE='awesome-fontconfig' # This is another way # POWERLEVEL9K_MODE='awesome-patched' # This isn't working  # Disable dir/git icons POWERLEVEL9K_HOME_ICON='' POWERLEVEL9K_HOME_SUB_ICON='' POWERLEVEL9K_FOLDER_ICON=''  # DISABLE_AUTO_TITLE=\"true\"  POWERLEVEL9K_VCS_GIT_ICON='' POWERLEVEL9K_VCS_STAGED_ICON='\\u00b1' POWERLEVEL9K_VCS_UNTRACKED_ICON='\\u25CF' POWERLEVEL9K_VCS_UNSTAGED_ICON='\\u00b1' POWERLEVEL9K_VCS_INCOMING_CHANGES_ICON='\\u2193' POWERLEVEL9K_VCS_OUTGOING_CHANGES_ICON='\\u2191'  POWERLEVEL9K_VCS_MODIFIED_BACKGROUND='yellow' POWERLEVEL9K_VCS_UNTRACKED_BACKGROUND='yellow' # POWERLEVEL9K_VCS_UNTRACKED_ICON='?' # POWERLEVEL9K_SHOW_CHANGESET='true'  # Root indicator config POWERLEVEL9K_ROOT_INDICATOR_BACKGROUND=\"red\" POWERLEVEL9K_ROOT_INDICATOR_FOREGROUND=\"white\" POWERLEVEL9K_ROOT_ICON='\\u26A1'  # Prompt elements POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(status os_icon root_indicator context dir vcs) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(background_jobs virtualenv rbenv rvm time)  POWERLEVEL9K_SHORTEN_STRATEGY=\"truncate_middle\" POWERLEVEL9K_SHORTEN_DIR_LENGTH=4  POWERLEVEL9K_TIME_FORMAT=\"%D{%H:%M:%S \\uf073 %d.%m.%y}\"  POWERLEVEL9K_STATUS_VERBOSE=false  export DEFAULT_USER=\"$USER\"   # Uncomment the following line to use case-sensitive completion. # CASE_SENSITIVE=\"true\"  # Uncomment the following line to use hyphen-insensitive completion. Case # sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\"true\"  # Uncomment the following line to disable bi-weekly auto-update checks. # DISABLE_AUTO_UPDATE=\"true\"  # Uncooment the following line to autoupdate without prompt DISABLE_UPDATE_PROMPT=\"true\"  # Uncomment the following line to change how often to auto-update (in days). # export UPDATE_ZSH_DAYS=13  # Uncomment the following line to disable colors in ls. # DISABLE_LS_COLORS=\"true\"  # Uncomment the following line to disable auto-setting terminal title. # DISABLE_AUTO_TITLE=\"true\"  # Uncomment the following line to enable command auto-correction. # ENABLE_CORRECTION=\"true\"  # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\"true\"  # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\"true\"  # Uncomment the following line if you want to change the command execution time # stamp shown in the history command output. # The optional three formats: \"mm/dd/yyyy\"|\"dd.mm.yyyy\"|\"yyyy-mm-dd\" # HIST_STAMPS=\"mm/dd/yyyy\"  # Would you like to use another custom folder than $ZSH/custom? # ZSH_CUSTOM=/path/to/new-custom-folder  # Which plugins would you like to load? (plugins can be found in ~/.oh-my-zsh/plugins/*) # Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/ # Example format: plugins=(rails git textmate ruby lighthouse) # Add wisely, as too many plugins slow down shell startup. plugins=(git colored-man colorize github jira vagrant virtualenv pip           python brew osx zsh-syntax-highlighting) source $ZSH/oh-my-zsh.sh source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh source /usr/local/share/zsh-autosuggestions/zsh-autosuggestions.zsh # source ~/.fonts/*.sh  # User configuration  # export MANPATH=\"/usr/local/man:$MANPATH\"  # You may need to manually set your language environment # export LANG=en_US.UTF-8  # Preferred editor for local and remote sessions # if [[ -n $SSH_CONNECTION ]]; then #   export EDITOR='vim' # else #   export EDITOR='mvim' # fi  # Compilation flags # export ARCHFLAGS=\"-arch x86_64\"  # ssh # export SSH_KEY_PATH=\"~/.ssh/rsa_id\"  # Set personal aliases, overriding those provided by oh-my-zsh libs, # plugins, and themes. Aliases can be placed here, though oh-my-zsh # users are encouraged to define aliases within the ZSH_CUSTOM folder. # For a full list of active aliases, run `alias`. # # Example aliases alias zshconfig=\"atom ~/.zshrc\" # alias ohmyzsh=\"mate ~/.oh-my-zsh\"   As you can see I have a lot the lines commented with #, since I don’t want to use that config, but I didn’t lose them. From line 28 to 73, it’s basically the configuration of the prompt. There are literally dozens of ways to configure the prompt, and you can see some of them here. Mine is quite similar to Falkor’s one, but I’ve edited it a little bit. You can find out more about how to stylizing your prompt and how the configuration variables work here and here.   Don’t forget to set your theme as Powerlevel9k —line 33 ZSH_THEME=\"powerlevel9k/powerlevel9k\"— and also the Powerlevel mode —line 35. The Powerlevel Mode define the type —or the style— of glyphs than are shown.   You can see also that in the lines 118 — 122 are the plugins I’m using and that in the the line 151 I establish a shortcut to access to the configuration through atom just typing zshconfig.   Configuring iTerm2   Finally, you have to configure iTerm2 to use your patched font if you want the glyphs to shown in your prompt                        iTerm2 font configuration.              If you don’t want to patch any font, you can download any of the prepatched fonts, and I recommend do it using Hombrew.   1 2 $ brew tap caskroom/fonts $ brew cask install font-meslo-nerd-font #if you want to install Meslo font                        Color configuration in iTerm2              Finally you can configure the colors in iTerm2. Usually people use of of the presets iTerm have, or the ones you can download. But I have tweaked a little bit the colors and I have my own configuration.   Now you are ready to use iTerm2 with your new configuration.   Setting Zsh as your default shell   First you need to check what version, if any, of Zsh you have installed.   1 $ zsh --version   in my case and right now my version is 5.4.2, but you can check which is the last version in the wikipedia page.   Now you have to check that you have Zsh in your list of authorized shells. You have check running opening the file /etc/shells with atom:   1 $ atom /etc/shells   You have to see something similar to this.   1 2 3 4 5 6 7 8 9 10 11 # List of acceptable shells for chpass(1). # Ftpd will not allow users to connect who are not using # one of these shells.  /bin/bash /bin/csh /bin/ksh /bin/sh /bin/tcsh /bin/zsh /usr/local/bin/bash   If you don’t have the line 10, add it and save the file.   Now you run the following command to make Zsh your default shell:   1 2 $ chsh -s $(which zsh)   Keeping Terminal with the previous config   Since I have iTerm2 with Oh My Zsh, I like to keep Terminal with Bash. Since we’ve set as a default terminal Zsh we need to set up manually to use Bash.   Open Terminal and launch the options screen Cmd + ;. In the General tap you can find which shell use terminal. Choose command and type /bin/bash.                        Terminal with Bash              Done, now you can enjoy the best of the two worlds. Enjoying a new, more flexible and customizable shell as default, while keeping your old one, just in case you felt nostalgic.                  I was about to upload my patched Monaco font, but then I realize that I can’t post any modification of the Monaco font since it’s copyrighted by Apple. However, you can easily patch your copy of the font for your personal use with the script provided by Nerd Fonts. I faced some problems when I tried to patch it myself, basically related to the height of the patched font, which ended up different than the original font. If this is your case, you can just download FontForge —brew cask install fontforge— and modify those parameters to be equal to the original ones &#8617;           ","categories": ["Professional","Technology"],
        "tags": ["how to","macOS","shell"],
        "url": "https://luispuerto.net/blog/2018/01/09/iterm2-oh-my-zsh-powerlevel9k-monaco-nerd-complete-font/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Screen-Shot-2017-12-19-at-11.20.07.png"
},{
        "title": "Install R 100% Homebrew Edition With OpenBlas & OpenMP — My Version",
        "excerpt":"Update Friday, 10th of May 2018: If you want to install R with all the capabilities you need to read this post too, and perhaps this one too.   Update Tuesday, 27th of March 2018: I just found out that seems you don’t just need to run sudo R CMD javareconf to configure Java an R, at least with the versions of Java 9.0.4 and R 3.4.4.   Update Thursday, 22nd of March 2018: I have to add -fopenmp to both clang and clang++ variables in my makevars to be able to build data.table package correctly. This is not exactly what the Data Table wiki recommends. I update the section about the Data Table Package accordingly.     As you know I’m a big fan of Homebrew as a manager of part of the software of my Mac, since it make things easier. There are a lot of guides out there about how to have a R installation 100% Homebrew and some people, like me, like to have this kind of setup because it’s convenient and for the sake of lear a little bit more about how R works in more detail. However, Homebrew setup isn’t officially supported by the R Core Team, so if you find problems with your R installation you aren’t going to get support from them. Nevertheless, you are going to be able to get support from Homebrew and of course, from the regular channels to get help for R, like the mail list.   The biggest advantage, besides of the regular advantages of installing something with HomeBrew, is you can create your own version of R, you can compile it, therefore you can compile it with steroids, so you can take advantage of the OpenBlas and OpenMP libraries.   OpenBLAS &amp; OpenMP   OpenBLAS is a open implementation of the BLAS (Basic Linear Algebra Subprograms) API. Basically, it optimizes your processor when you are doing mathematical operations, like when you are using R. It’s usually a huge leap in performance when you begin to make complex mathematical operations.   OpenMP is a library for Open Multi-Processing, or in other words, be able to use all the cores of your processor when you are compiling C, C++, and Fortran. If also make R to process faster since some packages are able to use all the cores of your computer after you compile them with OpenMP.   In other words, you are going to increase your performance a lot with this setup, as Mauricio Vargas demonstrate in his last two post (Why is R slow? some explanations and MKL/OpenBLAS setup to try to fix this and Is Microsoft R Open faster than CRAN R?).   We also did a small test since we wanted to get this setup in one of our computers, a MacBook Air from 2013. So, we used the MicroBenchmarks package with the below script (from Alexej Gossmann’s Blog) and we got the following results.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 library(microbenchmark)  set.seed(2017) n &lt;- 10000 p &lt;- 100 X &lt;- matrix(rnorm(n*p), n, p) y &lt;- X %*% rnorm(p) + rnorm(100)  check_for_equal_coefs &lt;- function(values) {   tol &lt;- 1e-12   max_error &lt;- max(c(abs(values[[1]] - values[[2]]),                      abs(values[[2]] - values[[3]]),                      abs(values[[1]] - values[[3]])))   max_error &lt; tol }  mbm &lt;- microbenchmark(\"lm\" = { b &lt;- lm(y ~ X + 0)$coef },                \"pseudoinverse\" = {                  b &lt;- solve(t(X) %*% X) %*% t(X) %*% y                },                \"linear system\" = {                  b &lt;- solve(t(X) %*% X, t(X) %*% y)                },                check = check_for_equal_coefs)  mbm   We got this results on a Macbook Pro 2010:                 1 2 3 4 5 6 7 # Base R benchmarks results  Unit: milliseconds           expr      min       lq     mean   median       uq      max neval             lm 299.5747 319.5861 339.4531 324.8694 331.7090 521.0330   100  pseudoinverse 326.1059 344.3830 358.1566 351.7829 359.7690 508.8802   100  linear system 199.0780 206.6064 218.7704 210.2198 218.2907 327.6886   100   1 2 3 4 5 6 7 # R with openblas and LLVM  Unit: milliseconds           expr       min        lq      mean    median        uq      max neval             lm 262.22400 272.39873 287.72378 277.65483 286.07772 361.0826   100  pseudoinverse  60.50899  62.65356  82.10815  70.40881  75.11090 169.7922   100  linear system  38.01025  39.48672  52.82579  45.81922  49.36025 121.0351   100   I really think the results speak for themselves.   Caveats   Of course there are some problems when you have this kind of install. The first one is the complication of the install process. If it were as simple as install R binaries from CRAN I wouldn’t be doing this guide. The second one and more important, you are going to need to compile the packages you install from now on, without exception. You aren’t going to be able to install the binaries of the packages anymore. This has advantages and disadvantages. The main advantage is that they are going to make use of the libraries you have installed in your your system like OpenBLAS, OpenMP or LLVM, to mention some. However, this means that you are going to need some other libraries to compile and you have to have them correctly linked, like Java or libxml2 or some of the packages aren’t going to compile and you aren’t going to be able to have it on your system.   In case you get any problem internet is your friend. You can look for the error R is returning when it tries to compile. If you are the first one to get that error you can ask in communities like Stackoverflow or the mail list for R help. All of these is going to make you understand R much better and your are going to be a better R user. So take it with patience and consider it like an advance course for R.   Take into account that sometimes even the CRAN install binaries pose problems, mostly with it’s link to Java. Before I decided to have this kind of install with R I had in the past multiple problems with Java and rJava package. So nothing is perfect, but you didn’t decided to use R because it was simple, did you?   How to install?   I’ve used as inspiration for this guide mainly two main sources. On one hand, Bhaskar Karambelar’s installation guide, and on the other Mauricio Vargas’ one. Bhaskar’s one was the first I used, more than 6 months ago, while we were in the United Stated, and really worked well in that moment. Problem with it is, it installs a lot or libraries to program in C/C++ what unless you are a C/C++ programmer you aren’t going to use, although you never know. At that moment, I installed everything due to lack of knowledge, but probably right now I wouldn’t. It’s up to you if you want to install those libraries and programing languages. However, I have more than enough space in my hard drive and I don’t mind to have then, perhaps they are going to to be useful in the future. Besides, this has been a way to discover then and know more about C/C++ programing. Mauricio’s guide goes more to the point and it just helps you to install a really fast and quick version of R that use OpenMP and OpenBlas.   Through this guide I just want to try to show you how I ended with my installation, that is an updated mixture of both guides.  However, take into account that mine guide is going to be a little bit different, even more taking into account that I use Zsh as my shell.   Homebrew   You probably have Homebrew already installed, if you don’t, please, install it. Then, I recommend you to connect to the cask tap if you haven’t done it already:   1 $ brew tap caskroom/cask # Tap to install regular app with user interface (GUI)   As you probably you’ve noticed, I don’t tap a lot of repos that Bhaskar taped. This is mainly because those taps are deprecated and its formulae are now included in the Homebrew Core. I decided not to tap other repos because I’m not going to use them.   I recommend to add the following lines in your Zsh and/or Bash profiles running the following:   1 2 3 4 5 6 7 8 9 # For zsh echo '# Setting language and localization variables export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8' &gt;&gt; ~/.zshrc  # For bash echo '# Setting language and localization variables export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8' &gt;&gt; ~/.bash_profile   Uninstalling previous R install   If you already have R installed I recommend you to uninstall R completely. Before you do it, you perhaps want to do a copy of your installed packages, just make a list because you are going to need to compile all of them with this homebrew install. You can run the following code in R to make a copy of your packages.   1 2 3 4 5 6 7 8 9 10 11 12 13 # How to list installed packajes  package_matrix &lt;- installed.packages()  package_df &lt;- data.frame(package_matrix)  package_list &lt;- package_df[is.na(package_df$Priority), \"Package\"]  packages &lt;- as.character(package_list)  write(packages, file = \"packages\")  save(packages, file = \"packages.RData\")   Now you are going to have a file in your working folder packages.RData that is going to store a variable with a list of all your packages. To reinstall all the packages you just need to load that file in R and run:   1 install.packages(packages)   Now that you have a list of your installed packages you can delete R from your system. Run the following on terminal:   1 $ sudo rm -rf /Library/Frameworks/R.framework /Applications/R.app /usr/local/bin/R /usr/local/bin/Rscript   XCode Command Line Tools   You need to have installed the Command Line Tools for XCode. Please be aware that if you already has installed, XCode you probably still need to install the CLT. The best way to know is running the following command in terminal:   1 $ xcode-select --install   C/C++ Compilers and Libraries   Now, you need to install the C/C++ necessary compilers and other useful libraries.   1 $ brew install gcc ccache cmake pkg-config autoconf automake   You can   1 2 3 4 5 6 7 $ cd /usr/local/bin $ ln -s gcov-7 gcov $ ln -s gcc-7 gcc $ ln -s g++-7 g++ $ ln -s cpp-7 cpp $ ln -s c++-7 c++ $ cd ~   You can ask fo the versions to check if everything is correctly installed. You have to get something similar to this:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ gcc --version gcc (Homebrew GCC 7.2.0) 7.2.0 Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  $ gfortran -v Using built-in specs. COLLECT_GCC=gfortran COLLECT_LTO_WRAPPER=/usr/local/Cellar/gcc/7.2.0/libexec/gcc/x86_64-apple-darwin17.2.0/7.2.0/lto-wrapper Target: x86_64-apple-darwin17.2.0 Configured with: ../configure --build=x86_64-apple-darwin17.2.0 --prefix=/usr/local/Cellar/gcc/7.2.0 --libdir=/usr/local/Cellar/gcc/7.2.0/lib/gcc/7 --enable-languages=c,c++,objc,obj-c++,fortran --program-suffix=-7 --with-gmp=/usr/local/opt/gmp --with-mpfr=/usr/local/opt/mpfr --with-mpc=/usr/local/opt/libmpc --with-isl=/usr/local/opt/isl --with-system-zlib --enable-checking=release --with-pkgversion='Homebrew GCC 7.2.0' --with-bugurl=https://github.com/Homebrew/homebrew-core/issues --disable-nls --with-native-system-header-dir=/usr/include --with-sysroot=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk Thread model: posix gcc version 7.2.0 (Homebrew GCC 7.2.0)  $ ccache --v ccache version 3.3.4   You can check also if the OpenMP from GCC is working running the following on terminal:   1 2 3 4 5 6 7 8 9 10 $ cat &gt; omp-test.c &lt;&lt;\"END\" #include &lt;omp.h&gt; #include &lt;stdio.h&gt; int main() {     #pragma omp parallel     printf(\"Hello from thread %d, nthreads %d\\n\", omp_get_thread_num(), omp_get_num_threads()); } END gcc -fopenmp -o omp-test omp-test.c ./omp-test   And you should get something similar to:   1 2 3 4 5 6 7 8 Hello from thread 1, nthreads 8 Hello from thread 6, nthreads 8 Hello from thread 4, nthreads 8 Hello from thread 2, nthreads 8 Hello from thread 5, nthreads 8 Hello from thread 0, nthreads 8 Hello from thread 3, nthreads 8 Hello from thread 7, nthreads 8   Miscellaneous graphical libraries -optional   1 $ brew install freetype fontconfig pixman gettext   Some of these libraries aren’t strictly necessary for R, but they are to install other related apps like QGIS, GRASS or PostGIS. I think that if you don’t want to install then you don’t need to do it right now, since that software install its on dependencies   SSL/SSH Libraries -optional   If you already have Git you probably have OpenSSL, the other two are optional.   1 $ brew install openssl libressl libssh2   1 2 3 4 5 $ /usr/local/opt/openssl/bin/openssl version OpenSSL 1.0.2n  7 Dec 2017  $ /usr/local/opt/libressl/bin/openssl version LibreSSL 2.2.7   Libxml2   It’s highly recomendable to install this library since it’s somehow necessary to install some packages depending on the version of your macOS system. It’s really small (10 mb) so you are losing nothing installing it.   1 2 $ brew install libxml2 $ brew link libxml2 --force   Boost -optional   Boost is one of those libraries that you only install if you program in C/C++. If you want to install it you need to have Libxml2 installed and then proceed as following:   1 2 $ brew install icu4c libiconv libxslt $ brew install boost --with-icu4c --without-single   Then you can test if it’s correctly installed   1 2 3 4 5 6 7 8 9 10 11 12 $ cat &gt; first.cpp &lt;&lt;END #include&lt;iostream&gt; #include&lt;boost/any.hpp&gt; int main() {     boost::any a(5);     a = 1.61803;     std::cout &lt;&lt; boost::any_cast&lt;double&gt;(a) &lt;&lt; std::endl; } END clang++ -I/usr/local/include -L/usr/local/lib  -o first first.cpp ./first   1 1.61803   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cat &gt; second.cpp &lt;&lt;END #include&lt;iostream&gt; #include &lt;boost/filesystem.hpp&gt; int main() {     boost::filesystem::path full_path( boost::filesystem::current_path() );     if ( boost::filesystem::exists( \"second.cpp\" ) )     {         std::cout &lt;&lt; \"Found second.cpp file in \" &lt;&lt; full_path &lt;&lt; std::endl;     } else {         std::cerr &lt;&lt; \"Argh!, Something not working\" &lt;&lt; std::endl;         return 1;     } } END clang++ -I/usr/local/include -L/usr/local/lib  -o second second.cpp \\     -lboost_filesystem-mt -lboost_system-mt ./second   1 Found second.cpp file in \"/Users/brewmaster\"   GPG &amp; Git   I’ve already explained how to install GPG in a previous post to use it with Git. How to install Git was also explained.   X-Server   You are going to probably need X-Server down the road.   1 $ brew cask install xquartz   Latex   Latex is a set of applications and libraries to be able to write beautiful mathematical formulas and documents, mainly. But can be use to write any kind of documents.   1 $ brew cask install mactex   Java   If you don’t have Java installed it’s a good moment to do so and to do it with Homebrew.   1 $ brew cask install java   1 2 3 4 $ java -version java version \"9.0.1\" Java(TM) SE Runtime Environment (build 9.0.1+11) Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode)   Python   It’s recommended to install Python 2 and 3 as a complement to R although R itself doesn’t use it.   1 2 3 4 5 6 $ brew install python $ sudo easy_install pip $ pip install --upgrade pip setuptools $ pip install markdown rpy2 $ python -V # checking the version Python 2.7.10   rply2 is probably to give you an error untill you install R. You can try to install it right now and if it give you the error install again lately.   1 2 3 4 $ brew install python3 $ pip3 install --upgrade pip setuptools wheel $ python3 -V # Checking the version Python 3.6.4   R &amp; related   We are going to install some things before we install R itself. Pandoc is really useful when you have R to convert documents in different formats. Cairo is a graphical library that can be use for in R and it’s need for QGIS. Libsvg and librsvg are optional   Important!: If you want to have R with all the capabilities you need to install Cairo with the instructions in this post.   1 $ brew install pandoc cairo libsvg librsvg   OpenBLAS   Let’s install OpenBLAS, this is one of the key pieces of this installation.   1 $ brew install openblas --with-openmp   Now you can test if OpenBlas has been correctly installed.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ cat &gt; test-openblas.c &lt;&lt;\"END\" #include &lt;cblas.h&gt; #include &lt;stdio.h&gt;  void main() {   int i=0;   double A[6] = {1.0,2.0,1.0,-3.0,4.0,-1.0};   double B[6] = {1.0,2.0,1.0,-3.0,4.0,-1.0};   double C[9] = {.5,.5,.5,.5,.5,.5,.5,.5,.5};   cblas_dgemm(CblasColMajor, CblasNoTrans, CblasTrans,       3,3,2,1,A, 3, B, 3,2,C,3);    for(i=0; i&lt;9; i++)     printf(\"%lf \", C[i]);   printf(\"\\n\"); } END  clang -L/usr/local/opt/openblas/lib \\     -I/usr/local/opt/openblas/include \\     -lopenblas -lpthread \\     -o test-openblas test-openblas.c  ./test-openblas   1 11.000000 -9.000000 5.000000 -9.000000 21.000000 -1.000000 5.000000 -1.000000 3.000000   Armadillo and other libraries -optional   Now, you can also install, if you want, Armadillo, which is other library that it’s useful if you program in C/C++ and take advantage of OpenBLAS.   1 2 $ brew install eigen armadillo v8-315 $ brew link v8-315 --force   You can test Armadillo with the following code since the new Armadillo doesn’t provide examples, or at least I haven’t found them.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 $ cat &gt; example1.cpp &lt;&lt;END  #include  #include \"armadillo\"  using namespace arma;  using namespace std;  int main(int argc, char** argv)    {    cout &lt;&lt; \"Armadillo version: \" &lt;&lt; arma_version::as_string() &lt;&lt; endl;    // directly specify the matrix size (elements are uninitialised)    mat A(2,3);    // .n_rows = number of rows    (read only)    // .n_cols = number of columns (read only)    cout &lt;&lt; \"A.n_rows = \" &lt;&lt; A.n_rows &lt;&lt; endl;    cout &lt;&lt; \"A.n_cols = \" &lt;&lt; A.n_cols &lt;&lt; endl;    // directly access an element (indexing starts at 0)    A(1,2) = 456.0;    A.print(\"A:\");    // scalars are treated as a 1x1 matrix,    // hence the code below will set A to have a size of 1x1    A = 5.0;    A.print(\"A:\");    // if you want a matrix with all elements set to a particular value    // the .fill() member function can be used    A.set_size(3,3);    A.fill(5.0);    A.print(\"A:\");    mat B;    // endr indicates \"end of row\"    B &lt;&lt; 0.555950 &lt;&lt; 0.274690 &lt;&lt; 0.540605 &lt;&lt; 0.798938 &lt;&lt; endr      &lt;&lt; 0.108929 &lt;&lt; 0.830123 &lt;&lt; 0.891726 &lt;&lt; 0.895283 &lt;&lt; endr      &lt;&lt; 0.948014 &lt;&lt; 0.973234 &lt;&lt; 0.216504 &lt;&lt; 0.883152 &lt;&lt; endr      &lt;&lt; 0.023787 &lt;&lt; 0.675382 &lt;&lt; 0.231751 &lt;&lt; 0.450332 &lt;&lt; endr;    // print to the cout stream    // with an optional string before the contents of the matrix    B.print(\"B:\");    // the &lt;&lt; operator can also be used to print the matrix    // to an arbitrary stream (cout in this case)    cout &lt;&lt; \"B:\" &lt;&lt; endl &lt;&lt; B &lt;&lt; endl;    // save to disk    B.save(\"B.txt\", raw_ascii);    // load from disk    mat C;    C.load(\"B.txt\");    C += 2.0 * B;    C.print(\"C:\");    // submatrix types:    //    // .submat(first_row, first_column, last_row, last_column)    // .row(row_number)    // .col(column_number)    // .cols(first_column, last_column)    // .rows(first_row, last_row)    cout &lt;&lt; \"C.submat(0,0,3,1) =\" &lt;&lt; endl;    cout &lt;&lt; C.submat(0,0,3,1) &lt;&lt; endl;    // generate the identity matrix    mat D = eye&lt;mat&gt;(4,4);    D.submat(0,0,3,1) = C.cols(1,2);    D.print(\"D:\");    // transpose    cout &lt;&lt; \"trans(B) =\" &lt;&lt; endl;    cout &lt;&lt; trans(B) &lt;&lt; endl;    // maximum from each column (traverse along rows)    cout &lt;&lt; \"max(B) =\" &lt;&lt; endl;    cout &lt;&lt; max(B) &lt;&lt; endl;    // maximum from each row (traverse along columns)    cout &lt;&lt; \"max(B,1) =\" &lt;&lt; endl;    cout &lt;&lt; max(B,1) &lt;&lt; endl;    // maximum value in B    cout &lt;&lt; \"max(max(B)) = \" &lt;&lt; max(max(B)) &lt;&lt; endl;    // sum of each column (traverse along rows)    cout &lt;&lt; \"sum(B) =\" &lt;&lt; endl;    cout &lt;&lt; sum(B) &lt;&lt; endl;    // sum of each row (traverse along columns)    cout &lt;&lt; \"sum(B,1) =\" &lt;&lt; endl;    cout &lt;&lt; sum(B,1) &lt;&lt; endl;    // sum of all elements    cout &lt;&lt; \"sum(sum(B)) = \" &lt;&lt; sum(sum(B)) &lt;&lt; endl;    cout &lt;&lt; \"accu(B)     = \" &lt;&lt; accu(B) &lt;&lt; endl;    // trace = sum along diagonal    cout &lt;&lt; \"trace(B)    = \" &lt;&lt; trace(B) &lt;&lt; endl;    // random matrix -- values are uniformly distributed in the [0,1] interval    mat E = randu&lt;mat&gt;(4,4);    E.print(\"E:\");    cout &lt;&lt; endl;    // row vectors are treated like a matrix with one row    rowvec r;    r &lt;&lt; 0.59499 &lt;&lt; 0.88807 &lt;&lt; 0.88532 &lt;&lt; 0.19968;    r.print(\"r:\");    // column vectors are treated like a matrix with one column    colvec q;    q &lt;&lt; 0.81114 &lt;&lt; 0.06256 &lt;&lt; 0.95989 &lt;&lt; 0.73628;    q.print(\"q:\");    // dot or inner product    cout &lt;&lt; \"as_scalar(rq) = \" &lt;&lt; as_scalar(rq) &lt;&lt; endl;    // outer product    cout &lt;&lt; \"q*r =\" &lt;&lt; endl;    cout &lt;&lt; q*r &lt;&lt; endl;    // multiply-and-accumulate operation    // (no temporary matrices are created)    cout &lt;&lt; \"accu(B % C) = \" &lt;&lt; accu(B % C) &lt;&lt; endl;    // sum of three matrices (no temporary matrices are created)    mat F = B + C + D;    F.print(\"F:\");    // imat specifies an integer matrix    imat AA;    imat BB;    AA &lt;&lt; 1 &lt;&lt; 2 &lt;&lt; 3 &lt;&lt; endr &lt;&lt; 4 &lt;&lt; 5 &lt;&lt; 6 &lt;&lt; endr &lt;&lt; 7 &lt;&lt; 8 &lt;&lt; 9;    BB &lt;&lt; 3 &lt;&lt; 2 &lt;&lt; 1 &lt;&lt; endr &lt;&lt; 6 &lt;&lt; 5 &lt;&lt; 4 &lt;&lt; endr &lt;&lt; 9 &lt;&lt; 8 &lt;&lt; 7;    // comparison of matrices (element-wise)    // output of a relational operator is a umat    umat ZZ = (AA &gt;= BB);    ZZ.print(\"ZZ =\");    // 2D field of arbitrary length row vectors    // (fields can also store abitrary objects, e.g. instances of std::string)    field&lt;rowvec&gt; xyz(3,2);    xyz(0,0) = randu(1,2);    xyz(1,0) = randu(1,3);    xyz(2,0) = randu(1,4);    xyz(0,1) = randu(1,5);    xyz(1,1) = randu(1,6);    xyz(2,1) = randu(1,7);    cout &lt;&lt; \"xyz:\" &lt;&lt; endl;    cout &lt;&lt; xyz &lt;&lt; endl;    // cubes (\"3D matrices\")    cube Q( B.n_rows, B.n_cols, 2 );    Q.slice(0) = B;    Q.slice(1) = 2.0 * B;    Q.print(\"Q:\");    return 0;    }  END  clang++   -O2   -o example1  example1.cpp  -larmadillo -framework Accelerate  ./example1   You are going to get something like:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 Armadillo version: 8.300.3 (Tropical Shenanigans) A.n_rows = 2 A.n_cols = 3 A:   1.8965e-256  6.9531e-310  6.9531e-310   6.9531e-310  4.9407e-324   4.5600e+02 A:    5.0000 A:    5.0000   5.0000   5.0000    5.0000   5.0000   5.0000    5.0000   5.0000   5.0000 B:    0.5560   0.2747   0.5406   0.7989    0.1089   0.8301   0.8917   0.8953    0.9480   0.9732   0.2165   0.8832    0.0238   0.6754   0.2318   0.4503 B:    0.5560   0.2747   0.5406   0.7989    0.1089   0.8301   0.8917   0.8953    0.9480   0.9732   0.2165   0.8832    0.0238   0.6754   0.2318   0.4503  C:    1.6679   0.8241   1.6218   2.3968    0.3268   2.4904   2.6752   2.6858    2.8440   2.9197   0.6495   2.6495    0.0714   2.0261   0.6953   1.3510 C.submat(0,0,3,1) =    1.6679   0.8241    0.3268   2.4904    2.8440   2.9197    0.0714   2.0261  D:    0.8241   1.6218        0        0    2.4904   2.6752        0        0    2.9197   0.6495   1.0000        0    2.0261   0.6953        0   1.0000 trans(B) =    0.5560   0.1089   0.9480   0.0238    0.2747   0.8301   0.9732   0.6754    0.5406   0.8917   0.2165   0.2318    0.7989   0.8953   0.8832   0.4503  max(B) =    0.9480   0.9732   0.8917   0.8953  max(B,1) =    0.7989    0.8953    0.9732    0.6754  max(max(B)) = 0.973234 sum(B) =    1.6367   2.7534   1.8806   3.0277  sum(B,1) =    2.1702    2.7261    3.0209    1.3813  sum(sum(B)) = 9.2984 accu(B)     = 9.2984 trace(B)    = 2.05291 E:    7.8264e-06   5.3277e-01   6.7930e-01   8.3097e-01    1.3154e-01   2.1896e-01   9.3469e-01   3.4572e-02    7.5561e-01   4.7045e-02   3.8350e-01   5.3462e-02    4.5865e-01   6.7886e-01   5.1942e-01   5.2970e-01  r:    0.5950   0.8881   0.8853   0.1997 q:    0.8111    0.0626    0.9599    0.7363 as_scalar(r*q) = 1.53501 q*r =    0.4826   0.7203   0.7181   0.1620    0.0372   0.0556   0.0554   0.0125    0.5711   0.8524   0.8498   0.1917    0.4381   0.6539   0.6518   0.1470  accu(B % C) = 20.9962 F:    3.0479   2.7206   2.1624   3.1958    2.9261   5.9957   3.5669   3.5811    6.7118   4.5424   1.8660   3.5326    2.1213   3.3968   0.9270   2.8013 ZZ =         0        1        1         0        1        1         0        1        1 xyz: [field column 0]    0.6711   0.0077     0.3834   0.0668   0.4175     0.6868   0.5890   0.9304   0.8462   [field column 1]    0.5269   0.0920   0.6539   0.4160   0.7012     0.9103   0.7622   0.2625   0.0475   0.7361   0.3282     0.6326   0.7564   0.9910   0.3653   0.2470   0.9826   0.7227    Q: [cube slice 0]    0.5560   0.2747   0.5406   0.7989    0.1089   0.8301   0.8917   0.8953    0.9480   0.9732   0.2165   0.8832    0.0238   0.6754   0.2318   0.4503  [cube slice 1]    1.1119   0.5494   1.0812   1.5979    0.2179   1.6602   1.7835   1.7906    1.8960   1.9465   0.4330   1.7663    0.0476   1.3508   0.4635   0.9007   You can also test V8   1 2 $ echo 'quit()' | v8 V8 version 3.15.11.18 [sample shell]   R   Important!: If you want to have R with all the capabilities you have to follow the instructions in this post, then you can continue.   Let’s finally install R.   1 $ brew install R --with-openblas --with-java   Then if you are using english (american english) as your main language I recommend you to run the following:   1 $ defaults write org.R-project.R force.LANG en_US.UTF-8   Java9+R   First you have to insert the following line in your Zsh and/or Bash profiles.   1 2 3 4 5 6 7 # For zsh echo '# Setting $JAVA_HOME export JAVA_HOME=\"$(/usr/libexec/java_home)\"' &gt;&gt; ~/.zshrc  # For bash echo '# Setting $JAVA_HOME export JAVA_HOME=\"$(/usr/libexec/java_home)\"' &gt;&gt; ~/.bash_profile   And then run the following command in the terminal:   1 $ sudo R CMD javareconf JAVA_CPPFLAGS='-I/System/Library/Frameworks/JavaVM.framework/Headers -I/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/' # this is a specific command for Java 9.0.1   It seems that with Java 9.0.4 and R 3.4.4 you can run instead just:   1 $ sudo R CMD javareconf   or perhaps:   1 $ sudo R CMD javareconf JAVA_CPPFLAGS='-I/$JAVA_HOME'   You have to get something similar to this:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /usr/local/Cellar/r/3.4.3/lib/R/bin/javareconf: line 66: -I/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/: No such file or directory Java interpreter : /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/bin/java Java version     : 9.0.1 Java home path   : /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/bin/javac Java headers gen.: /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/bin/jar Non-system Java on macOS  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang -I/usr/local/Cellar/r/3.4.3/lib/R/include -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/Cellar/r/3.4.3/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.4.3/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.4.3/lib/R Done.   Folder for R Packages   Let’s create our own folder to store the installed packages for R. This way R, or us, doesn’t have to move all the packages every time we install a new R version. Run the following in terminal.   1 2 3 4 $ mkdir -p $HOME/Library/R/3.x/library $ cat &gt; $HOME/.Renviron &lt;&lt;END R_LIBS_USER=$HOME/Library/R/3.x/library END   You should also add this variable to your zsh and/or bash profiles.   1 2 3 4 5   # For zsh echo 'export R_LIBS_USER=$HOME/Library/R/3.x/library' &gt;&gt; ~/.zshrc  # For bash echo 'export R_LIBS_USER=$HOME/Library/R/3.x/library' &gt;&gt; ~/.bash_profile   LLVM   LLVM or Low Level Virtual Machine is a library that allow us to compile faster some R packages using OpenMP and also make that those packages use OpenMP when we are normally using R. To install it you run on your terminal the following:   1 $ brew install llvm   Then insert the LLVM location to your path in your Zsh and/or Bash profiles:   1 2 3 4 5 # For zsh echo 'export PATH=/usr/local/opt/llvm/bin:$PATH' &gt;&gt; ~/.zshrc  # For bash echo 'export PATH=/usr/local/opt/llvm/bin:$PATH' &gt;&gt; ~/.bash_profile   Data Table Package   The package Data Table need a specific makevars file. Makevars file is the file that tells R how and with what libraries it has to compile the packages we download from source. So we are going to install Data Table first, with that specific configuration and then set the final makevars file.   1 2 3 4 5 6 7 8 $ mkdir ~/.R $ echo \"CC=/usr/local/opt/llvm/bin/clang -fopenmp CXX=/usr/local/opt/llvm/bin/clang++ -fopenmp # -O3 should be faster than -O2 (default) level optimisation .. CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe LDFLAGS=-L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib CPPFLAGS=-I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include\" &gt;&gt; ~/.R/Makevars   Now we can install Data Table package on terminal. To do so just run on terminal:   1 2 3 4 $ R --vanilla &lt;&lt; EOF install.packages('data.table', repos='http://cran.us.r-project.org') q() EOF   Setting the final Makevars   First, we delete the previous makevars file.   1 $ rm ~/.R/Makevars   Set the final Makevars file.   1 2 3 4 5 6 7 $ echo \"CC=/usr/local/opt/llvm/bin/clang CXX=/usr/local/opt/llvm/bin/clang++ # -O3 should be faster than -O2 (default) level optimisation .. CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe LDFLAGS=-L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib CPPFLAGS=-I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include\" &gt;&gt; ~/.R/Makevars   As you probably have noticed the change is just the -fopenmp flag on the second line. In case you have to reinstall, or update Data Table, you just have to add that flag and then delete it. Really easy and you can even do it from RStudio.   RStudio   When you install R from Homebrew and you compile it, you don’t have anymore the R shell as an application on your Applications folder. But you can install any other graphical interface like RStudio. To do it you just run in your terminal:   1 $ brew cask install rstudio   Usually RStudio is able to recognize R install and you don’t need to do anything else.   Additional related languages -optional   You can also install some additional related languages like:   Node.js   From Wikipedia:      Node.js is an open-source, cross-platform JavaScript run-time environment that executes JavaScript code outside the browser.    To install it just run:   1 $ brew install node phantomjs casperjs   Scala   From Wikipedia:      Scala (/ˈskɑːlɑː/ SKAH-lah)[9] is a general-purpose programming language providing support for functional programming and a strong static type system. Designed to be concise,[10] many of Scala’s design decisions aimed to address criticisms of Java.[8]    To install it just run in your terminal:   1 $ brew install scala   golang   From Wikipedia:      Go (often referred to as Golang) is a programming language created at Google[12] in 2009 by Robert Griesemer, Rob Pike, and Ken Thompson.[10] Go is a statically typed, compiled language in the tradition of C, with memory safety, garbage collection, structural typing,[3] and CSP-style concurrency.[13]    To install it just run in your terminal:   1 $ brew install golang   You need to modify your zsh and/or bash profile like the following   1 2 3 4 5 6 7 8 9 10 11 12 13 # For zsh echo '## Path for Golang export GOPATH=$HOME/golang export GOROOT=/usr/local/opt/go/libexec export PATH=$PATH:$GOPATH/bin export PATH=$PATH:$GOROOT/bin' &gt;&gt; ~/.zshrc  # For bash echo '## Path for Golang export GOPATH=$HOME/golang export GOROOT=/usr/local/opt/go/libexec export PATH=$PATH:$GOPATH/bin export PATH=$PATH:$GOROOT/bin' &gt;&gt; ~/.bash_profile   Some GIS Libraries &amp; Soft -optional   You can also install some GIS libraries. This libraries could be mandatory if you are going to install geographical packages:   1 2 3 4 $ brew tap osgeo/osgeo4mac # Tap for geospatial software $ brew install postgresql geos proj $ brew install gdal2 --with-complete --with-opencl --with-armadillo --with-unsupported --with-libkml --with-postgresql $ brew install postgis --with-gui   Shell Profiles   You’ve been adding things to your Zsh and/or Bash profiles. I recommend you to make those profiles tidy, it’s going to be easier to modify things in the future.   This is how I have then:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Setting language and localization variables export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8  # Setting $JAVA_HOME export JAVA_HOME=\"$(/usr/libexec/java_home)\"  # Setting $PATH ## If you come from bash you might have to change your $PATH. export PATH=$HOME/bin:$PATH export PATH=/usr/local/sbin:$PATH # export PATH=/usr/local/bin:$PATH ## Path for LLVM install export PATH=/usr/local/opt/llvm/bin:$PATH ## Path for Golang export GOPATH=$HOME/golang export GOROOT=/usr/local/opt/go/libexec export PATH=$PATH:$GOPATH/bin export PATH=$PATH:$GOROOT/bin  # Setting R variables export R_LIBS_USER=$HOME/Library/R/3.x/library   You can see those files running the following:   1 2 3 4 5 6 7 8 9 10 11 # If you have atom. # zsh $ atom ~/.zshrc # bash $ atom ~/.bash_profile  # If you don't have atom #zsh $ open -a TextEdit ~/.zshrc #bash $ open -a TextEdit ~/.bash_profile   The End   Now you can begin to use your new R.  ","categories": ["Professional","RStats","Technology"],
        "tags": ["homebrew","how to","optimization","RSoft","RStats"],
        "url": "https://luispuerto.net/blog/2018/01/12/install-r-100-homebrew-edition-with-openblas-openmp-my-version/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/R-Homebrew.jpg"
},{
        "title": "Convert your Pi in an AirPlay receiver",
        "excerpt":"I recently needed to convert my Raspberry Pi 3 into a AirPlay receiver because I didn’t have any of my AirPort Express at hand and I wanted to play just music —not the entire collection of sounds my computer produce— on one of my old Hi-Fi Self Stereo System. So I searched on the internet and saw that it was totally possible to do it and I try it. I used as a source this article and I want share with you what steps I specifically followed.   Preliminaries   Let’s download some packages we are going to need as dependencies.   1 $ sudo apt-get update &amp;&amp; upgrade   1 $ sudo apt-get install autoconf libtool libdaemon-dev libasound2-dev libpopt-dev libconfig-dev avahi-daemon libavahi-client-dev libssl-dev   Shairport sync   To convert our Pi in a AirPlay device we need a software that is call Shairport Sync. We are going to need to compile it for our system, so we need to download the source code and then compile. To download it just run on the Pi’s terminal:   1 $ cd ~/Downloads # I like to download my soft in Downloads folder.   1 $ git clone https://github.com/mikebrady/shairport-sync.git   Compile   Now that we have download the shairport-sync repository to our Pi, we can proceeded to compile it. First or all we need to move to the Shairport Sync folder and then we can proceed to configure the compilation process.   1 $ cd ~/Downloads/shairport-sync   1 $ autoreconf -i -f   1 $ ./configure --with-alsa --with-avahi --with-ssl=openssl --with-systemd --with-metadata   Now that we have configured it, we can proceded to compile and install it.   1 $ make   1 $ sudo make install   Final config   When the config and installing process finish we can proceed to configure the software to use it. The main thing we need to do is create a group of users that can access to the hardware and then create a user on that group that will use the software.   1 $ sudo groupadd -r shairport-sync &gt;/dev/null   1 $ sudo useradd -r -M -g shairport-sync -s /usr/bin/nologin -G audio shairport-sync &gt;/dev/null   We also need to set up shairport sync to start on startup.   1 $ sudo systemctl enable shairport-sync   If we want to start right away to using it we can manually start it with the following command.   1 $ sudo service shairport-sync start   You are going to be able to find the Pi among the devices that offer AirPlay from your iOS device or your Mac. The name of the device is going to be the hostname you set up for your Pi using the raspi-config interface, if you haven’t changed it, it’s going to be RaspberryPi.   Tune up                        Tuning-Up Audio. Source: Wikimedia Commons              If you try to use your Pi as a AirPort Express right away you probably going find out that the sound quality leaves a lot to be desired —very low and quite distorted. However, you can do some tune-ups in the settings to improve this, but don’t thing we are going to be able to make it sound like a real AirPort Express.   Update Raspberry Pi Firmware   One of the first things we can do to improve the quality of the sound is to update the sound driver of the Pi. To do it, you need to update the firmware of the Pi. You can read more about this in this forum thread. Please, keep in mind that while you are updating the firmware you must not lose power in the Pi. To update the firmware you run the following command:   1 $ sudo rpi-update   Once the update process finish you need to turn off the Raspberry and take the SD card out of it and insert the card in a computer card reader. We are going to modify the boot config file of the Pi and and to do it we are going to open with a text editor the config file that it’s located on /boot/config.txt. Then you need to add the following variable to the end of the file:   1 audio_pwm_mode=2   Save the file, put the card back on the Pi and turn it on.   Audio jack as main audio   Now, we also need to set up the audio jack as the main audio output. Just run the following command:   1 $ amixer cset numid=3 1   Shairport Sync db range   There is a final setting we need to configure and it’s related to the audio range in which Shairport Sync operates. To do it we need to open the config file of Shariport Sync, therefore run the following command on the Pi’s terminal to open it with Nano.   1 $ sudo nano /usr/local/etc/shairport-sync.conf   Then we need to look for the following variable:   1 //      volume_range_db = 60 ;   and change it to   1 volume_range_db = 30;   Save and close Nano, Ctrl + X then Y and finally press return.   You need to reboot the Pi to make sure that new configuration is properly loaded.   1 $ sudo reboot   Bottom line   As you are going to soon find out the quality of sound isn’t exactly the same as with an AirPort Express, but this tweak allow you to use your Pi as one. It can be handy if you are traveling, you have your Pi with you, and you want to plug your music into some audio device or speakers.   If you wand to use your Raspberry for to play audio there are other options out there. One is RuneAudio which is a dedicated OS to play music on the Pi and the other is to install Kodi since Kodi can play AirPlay.                        Activating Airplay in Kodi             ","categories": ["Personal","Technology"],
        "tags": ["macOS","raspberry pi"],
        "url": "https://luispuerto.net/blog/2018/01/18/convert-you-pi-in-an-airplay-receiver/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/speakers-raspberry.jpg"
},{
        "title": "Sunsets in the land of the magic light",
        "excerpt":"Yesterday I was wandering with my car… warming it up, when I notice one of the most incredible sunsets I’ve ever seen in ages, between the trees in the south side of the road. I tried to think quickly where I could get a better look of the Sun and I finally came out with the perfect place where I can get a really nice view, fast and without eve get out of the car. Eastward of Avaranta beach in Joensuu, in Nuottaniemi, in a place called Siilaisenlahti on lake Pyhäselkä, there is a place where people unload and load their boats in Summer. From there you have a perfect view, with no trees interfering, and without even get out of the car. It’s a pity I couldn’t reach the place earlier, because the view I had when I was on the road was even more stunning.     After I took the shot and share it on my social networks, it started to make me remember a sunset shot that the rover Spirit took in 2005 on Mars.                        Sunset on Mars, source NASA and Astronomy Picture of the Day.              Among the pictures that google images served me there was a little more elaborated sunset on Mars with people on it…                        The Blue Sunset. Source: Wanderers a Short film by Erik Wernquist              It’s incredible how a sunset can make our mind fly away and dream with a close future when we can wander freely on the Solar System. It’s also incredible how a sunset in two different neighbor planets on the Solar System can look quite alike. Perhaps Mars is going to be our next home… and soon we’er going to be able to enjoy those sunsets on first hand.   Wanderers, is a magic short film that show us that future with the incredible voice of Carl Sagan as background. You mind just can’t, but wander into those places.            Wanderers — a short film by Erik Wernquist from Erik Wernquist on Vimeo.      For all its material advantages, the sedentary life has left us edgy, unfulfilled. Even after 400 generations in villages and cities, we haven’t forgotten. The open road still softly calls, like a nearly forgotten song of childhood. We invest far-off places with a certain romance. This appeal, I suspect, has been meticulously crafted by natural selection as an essential element in our survival. Long summers, mild winters, rich harvests, plentiful game—none of them lasts forever. It is beyond our powers to predict the future     Your own life, or your band’s, or even your species’ might be owed to a restless few—drawn, by a craving they can hardly articulate or understand, to undiscovered lands and new worlds.     Herman Melville, in Moby Dick, spoke for wanderers in all epochs and meridians: “I am tormented with an everlasting itch for things remote. I love to sail forbidden seas …”     Maybe it’s a little early. Maybe the time is not quite yet. But those other worlds—promising untold opportunities—beckon.     Silently, they orbit the Sun, waiting.     Source: Pale Blue Dot, Introduction. Pages 18 to 24.    Because wer are natural wanderers and explorers. We always crave for go a little bit far and beyond. I hope we never forgot that appeal.               We used to look up and wonder our place in the stars.           ","categories": ["Personal"],
        "tags": ["exploration","Finland","landscapes","photos"],
        "url": "https://luispuerto.net/blog/2018/01/24/sunsets-in-the-land-of-the-magic-light/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_4707.jpg"
},{
        "title": "Delete all the snapshots from the local time machine",
        "excerpt":"Sometimes you need to delete all the snapshots that the macOS time machine do locally just because you want to get rid of something of because you need to reclaim that space. Before one could use sudo tmutil disablelocal to delete everything and then sudo tmutil enablelocal to enable the local snapshots again —since they are useful. However, that option is no longer available.   So if you run   1 2 3 4 5 6 7 8 $ sudo tmutil listlocalsnapshots com.apple.TimeMachine.2018-03-03-144538 com.apple.TimeMachine.2018-03-03-154712 com.apple.TimeMachine.2018-03-03-170903 com.apple.TimeMachine.2018-03-03-181447 com.apple.TimeMachine.2018-03-04-094245 com.apple.TimeMachine.2018-03-04-110835 com.apple.TimeMachine.2018-03-04-121348   You probably end up with a similar list as mine, or perhaps even longer. If you waned to free some space, you need to use the command tmutil deletelocalsnapshots and introduce the date os each of the snapshot one by one to delete them, which it’s quite a pain in the ***.   Lucky, someone in the MacRumors forum came with a clever idea of using the terminal and the grep command. If you run:   1 2 3 4 5 6 7 8 $ tmutil listlocalsnapshotdates / |grep 20|while read f; do tmutil deletelocalsnapshots $f; done Deleted local snapshot '2018-03-03-144538' Deleted local snapshot '2018-03-03-154712' Deleted local snapshot '2018-03-03-170903' Deleted local snapshot '2018-03-03-181447' Deleted local snapshot '2018-03-04-094245' Deleted local snapshot '2018-03-04-110835' Deleted local snapshot '2018-03-04-121348'   You are going to get rid of all those snapshots in no time.   Learn how to use the terminal commands kids… it’s going to say  ","categories": ["Technology"],
        "tags": ["how to","macOS"],
        "url": "https://luispuerto.net/blog/2018/03/04/delete-all-the-snapshots-from-the-local-time-machine/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/240086966_1f04572a86_z.jpg"
},{
        "title": "Jekyll",
        "excerpt":"Some months ago my wife brought to my attention Jekyll, which is a blog aware static-site __generator, or in other words it’s a piece of software that allows you to beautiful generate static websites. At that moment, I didn’t play too much attention to it, and just look for some comparisons between it and WordPress and I came to the conclusion that it wasn’t worth even a try.                        GitHub Octocat with the Jekyll test tube ready for the transformation              However, after some months and seeing some examples of what you can do with it I think I’m going to give it another shot. Most of the webs you can see there are quite simple, just a couple of static pages and perhaps a simple blog. However, that doesn’t mean that you can’t build complex and beautiful websites with it —as some of the examples can proof— after all it’s just HTML. I’m not an HTML expert, quite the opposite. I have little experience programing on it and it was mostly long time ago when I begun to browse internet. However, since little by little I learning how to program in R and some shell commands, I think I’m going to give a try.   Jekyll has been developed by the GitHub team and for that reason they play quite good together. Some of the advantages of it are the following:      It’s free and open source.   It’s cheap. You can host your web almost anywhere for free. AWS or GitHub Pages offer you free tiers where almost anyone page can be hosted. So you don’t have to deal with expensive hosting.   It’s fast. Everything is really lightweight so you don’t need to a lot of things to optimize how fast it loads   It’s simple. You don’t have to deal with databases.   It’s blog aware. Just following some simple formatting rules Jekyll can create for you a blog.   You can use Git to post. Since everything is a file and nothing is store in a database you can just use git to track your changes and post content.   Uses markdown, so pretty simple to post content.   You can use themes   You can use plugins.   Probably, people that have been using it for longer can give you a more compressive list of its advantages. In my case, I just going give a try and try to generate something similar to my current web with it. My playground is going to be jekyll.luispuerto.net, which is hosted in GitHub Pages.   There are a lot of documentation about it in its official page and I just found a couple of youtube videos [1 &amp; 2] that look quite interesting. There is also an extensive community that can always give you a hand if you run into trouble or get stuck. Just to share with you some of the resources I just found in a minute of searching.   Let’s see what I can achieve.  ","categories": ["Technology"],
        "tags": ["blogging","coding","jekyll","markdown","static web"],
        "url": "https://luispuerto.net/blog/2018/03/04/jekyll/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/jekyll.png"
},{
        "title": "QGIS on macOS with Homebrew",
        "excerpt":"Update Monday, 26th of March: Almost the next day or the following day I published this post KyngChaos just published the QGIS 3 release for macOS. You can download here.     Just a couple days ago I was about to write a post about how difficult, or almost impossible, is to install the last versions of QGIS on macOS right now. Everything started, I believe, when gdal package got update at the beginning of the week and that broke my install of QGIS 2 even thought, and to be honest, I really don’t know if all this mess was caused for that update, or by the update of the python formulae in Homebrew. Anyhow, usually these problems are solved just rebuilding the app from source using the new packages, but in this occasion that solution wasn’t working.   Homebrew decided that on the first of March the behavior of the Python formulae and the name, and I believe the name of the formulae, was going to change to be more consistent. To sum a little bit up, after the change python was going to point to python3 in your shell and python2 is going point to so python2 —or something like that. However, the change in the command in the CLI  broke a truckload of third party software, and seems that also wasn’t compliant with Python’s policy. So… they decided to change some things back —CLI commands, no formulae names— and now seems that python ➜ python2 and python3 ➜ python3. If you are confused, don’t worry, am I also. Besides, since I’m not an expert in the matter and I’ve perhaps missed something in all this chaos. In consequence… I really don’t know!.   Anyhow and right now, with the current state of Homebrew and the OSGeo/homebrew-osgeo4mac tap, it’s quite difficult to install the last versions of QGIS in macOS. A few weeks ago, QGIS released QGIS 3 Girona, however it’s only possible to install it right now in Windows and Linux. Usually on macOS the releases has been a little bit slower than in other platforms basically because QGIS has really few developers able to work on macOS and seems that things aren’t as easy as in other platforms. Traditionally, Homebrew has come to the rescue with OSGeo/homebrew-osgeo4mac tap, which you can build your own version of QGIS, but that repo hasn’t been update almost since the beginning of December. This time the delay not only mean that the macOS users not only can’t install the last small incremental change version, but that for more than 3 weeks we aren’t able to install the last version, which means a big leap from version 2 to 3.   This left the macOS users a little bit hanging although we still can download the official build for macOS in the KyngChaos{.reference.external} download page, where you can find the 2.18.14 version. The problem is that version isn’t even the last version of the long-term repository release, which is 2.18.17.   Nevertheless, this weekend I was able to rebuild the last version of the LTR of QGIS 2 and the developer version of QGIS3. I really don’t know how stable is the QGIS 3 developer version, but at least at first sight everything works and the app even open faster than the QGIS 2.   How to install the last version of QGIS 2?   What I did was just reinstall python, python2, gdal, gdal2, and finally qgis2. More or less as follows:   1 2 3 4 5 $ brew reinstall python2 python bison &amp;&amp; brew reinstall gdal2 \\     --with-armadillo --with-complete --with-libkml \\     --with-opencl --with-postgresql --with-unsupported $ brew reinstall qgis2 --with-grass --with-saga-gis-lts   After that and compiling here and there, I was able to open QGIS.app again.   This left me with QGIS in /usr/local/Cellar/qgis2/2.18.14. But I want to have QGIS.app in /Applications folder so I have two options. First one is run in the following command in the terminal, as advised by Homebrew:   1 $ brew linkapps [--local]   However, I prefer to move the actual app to the /Applications folder and create a symbolic link on the /Cellar folder.   1 2 $ mv -f /usr/local/Cellar/qgis2/2.18.14/QGIS.app /Applications $ ln -s /Applications/QGIS.app /usr/local/Cellar/qgis2/2.18.14/QGIS.app   Yet, weren’t we going to install the last version of QGIS? This isn’t the last version. OK…   QGIS 2.18.17   To install the last version you need to change the the QGIS formulae to make it to download the last version from the LTR. Since Homebrew isn’t anything more than a git with Ruby scripts I recommend you to make a branch in the repository and change the formula. You aren’t going to change anything in Homebrew, but in a branch of tap, in the OSGeo/homebrew-osgeo4mac tap. I did the following.   First, I create a branch and checked out on it.   1 2 3 $ cd /usr/local/Homebrew/Library/Taps/osgeo/homebrew-osgeo4mac $ git checkout -b QGIS2.18.17 $ brew edit qgis2   Now you can edit the formula to be able to install QGIS 2.18.17. ~You can see how I modify mine~~1 below that has highlighted the lines I’ve changed.   1 &lt;pre class=\"nums:true lang:ruby mark:12-13 decode:true\" title=\"qgis2.rb\" data-url=\"https://raw.githubusercontent.com/luisspuerto/homebrew-osgeo4mac/QGIS2.18.17/Formula/qgis2.rb\"&gt;   Now you just commit.   1 2 $ git stage Formula/qgis2.rb $ git commit -m \"qgis update to 2.18.17\"   And now you just reinstall QGIS and move to /Applications:   1 2 3 $ brew reinstall qgis2 --with-grass --with-saga-gis-lts $ mv -f /usr/local/Cellar/qgis2/2.18.17/QGIS.app /Applications $ ln -s /Applications/QGIS.app /usr/local/Cellar/qgis2/2.18.17/QGIS.app                        QGIS 2.18.17 splash screen                                   QGIS 2.18.17 interface              Matplotlib   Since Homebrew/homebrew-science has been deprecated and some of the formulae wasn’t migrated to the Homebrew/homebrew-core this left us with a message at the end of the QGIS’ compilation asking us to install matplotlib via pip.   The following Python modules are needed by QGIS during run-time:   1 2 3 4 5 6 7 The following Python modules are needed by QGIS during run-time:      matplotlib, pyparsing  You can install manually, via installer package or with `pip` (if availble):      pip install &lt;module&gt;  OR  pip-2.7 install &lt;module&gt;   I had a problem and when I tried to install using pip install matplotlib threw me an error, though.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Collecting matplotlib   Using cached matplotlib-2.2.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: numpy&gt;=1.7.1 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: pytz in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python2.7/site-packages (from matplotlib) Requirement already satisfied: setuptools in /usr/local/lib/python2.7/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib) Installing collected packages: matplotlib Exception: Traceback (most recent call last):   File \"/usr/local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main     status = self.run(options, args)   File \"/usr/local/lib/python2.7/site-packages/pip/commands/install.py\", line 342, in run     prefix=options.prefix_path,   File \"/usr/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 784, in install     **kwargs   File \"/usr/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 851, in install     self.move_wheel_files(self.source_dir, root=root, prefix=prefix)   File \"/usr/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files     isolated=self.isolated,   File \"/usr/local/lib/python2.7/site-packages/pip/wheel.py\", line 345, in move_wheel_files     clobber(source, lib_dir, True)   File \"/usr/local/lib/python2.7/site-packages/pip/wheel.py\", line 323, in clobber     shutil.copyfile(srcfile, destfile)   File \"/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 97, in copyfile     with open(dst, 'wb') as fdst: IOError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/site-packages/matplotlib/legend.pyc'   Basically I had a ownership problem, that can be solve using sudo.   1 $ sudo pip install matplotlib   Or fixing the ownership like this:   1 $ sudo chown -R $(echo $USER) ~/Library/Caches/pip   Now, I was able to install.   How to install QGIS 3?   Well, this is a little bit more challenging, but just a little bit. The only version of a Homebrew formula that is available to install QGIS 3 is the developer version one that is in this qgis/homebrew-qgisdev repo-tap. So I just have to tap that repo:   1 $ brew tap qgis/homebrew-qgisdev   However, I needed to make changes in the formula to be able to install because it has a dependency on Matplotlib on Homebrew/homebrew-science and as we learned before that formula doesn’t exist anymore. In this case, it isn’t like in the QGIS 2, that it builds anyway and then ask you to install Matplotlib. It just doesn’t build and throws an error.   I have two options here. I can just delete or comment the line where the dependency is called, or I can replace it for the legacy formula, which is located in the brewsci/homebrew-science tap. Either way I proceeded in the same way as I did previously.   First, I created a branch and edited the formulae.   1 2 3 $ cd /usr/local/Homebrew/Library/Taps/qgis/homebrew-qgisdev $ git checkout -b matplotlib-fix $ brew edit qgis3-dev   You can check how I’ve edited mine. The important part is in the highlighted 86-87 lines.   Then, I just committed my edits   1 2 $ git add Formula/qgis3-dev.rb $ git commit -m \"fix for matplotlib\"   Before I tried to build I have to do two more tweaks to be able to compile without errors. First I have to reinstall Bison.   1 $ brew reinstall bison   And then I have to modify the file /usr/local/bin/pyrcc5 and change pythonw2.7 to python3.   1 2 #!/bin/sh exec python3 -m PyQt5.pyrcc_main ${1+\"$@\"}   Now I could build QGIS 3 developer edition:   1 $ brew install --no-sandbox qgis3-dev --with-grass --with-saga-gis-lts                        QGIS 3 dev splash screen                                   QGIS 3 interface              After I finished building the QGIS 3, I moved the app from the Homebrew Cellar to the /Applications folder in the same way I moved QGIS 2, but since I want to keep QGIS 2 and QGIS 3, in the process I renamed the app to QGIS 3.app.   1 2 $ mv /usr/local/opt/qgis3-dev/QGIS.app /Applications/QGIS\\ 3.app $ ln -s /Applications/QGIS\\ 3.app /usr/local/opt/qgis3-dev/QGIS.app   Final thoughts   I hope that in the near future the situation improve and we can enjoy the last version of QGIS on macOS in a easier way. Perhaps even without needed to compile. However, in this very moment isn’t the case.   You have to take into account that with this method you are installing the QGIS 3 developer version, so probably isn’t going to be really stable, or perhaps you are going to have no problem. I guess that you can install the stable version using the QGIS3-dev formulae, you just have to change the values branch =&gt; to \"release-3_0\" in line 37 and version to \"3.0\" on line 38.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 class Qgis3DevUnlinkedFormulae &lt; Requirement   fatal true   satisfy(:build_env =&gt; false) { !qt4_linked &amp;&amp; !pyqt4_linked &amp;&amp; !txt2tags_linked }    def qt4_linked     (Formula[\"qt\"].linked_keg/\"lib/QtCore.framework/Versions/4\").exist?   rescue     return false   end    def pyqt4_linked     (Formula[\"pyqt\"].linked_keg/\"lib/python2.7/site-packages/PyQt\").exist?   rescue     return false   end    def txt2tags_linked     Formula[\"txt2tags\"].linked_keg.exist?   rescue     return false   end    def message     s = \"Compilation can fail if these formulae are installed and linked:\\n\\n\"      s += \"Unlink with `brew unlink qt` or remove with `brew uninstall qt`\\n\" if qt4_linked     s += \"Unlink with `brew unlink pyqt` or remove with `brew uninstall pyqt`\\n\" if pyqt4_linked     s += \"Unlink with `brew unlink txt2tags` or remove with `brew uninstall txt2tags`\\n\" if txt2tags_linked     s   end end  class Qgis3Dev &lt; Formula   desc \"User friendly open source Geographic Information System\"   homepage \"https://www.qgis.org\"    url \"https://github.com/qgis/QGIS.git\", :branch =&gt; \"release-3_0\"   version \"3.0\"    option \"without-ninja\", \"Disable use of ninja CMake generator\"   option \"without-debug\", \"Disable debug build, which outputs info to system.log or console\"   option \"without-qt5-webkit\", \"Build without webkit based functionality\"   option \"without-pyqt5-webkit\", \"Build without webkit python bindings\"   option \"without-server\", \"Build without QGIS Server (qgis_mapserv.fcgi)\"   option \"without-postgresql\", \"Build without current PostgreSQL client\"   option \"with-globe\", \"Build with Globe plugin, based upon osgEarth\"   option \"with-grass\", \"Build with GRASS 7 integration plugin and Processing plugin support (or install grass-7x first)\"   option \"with-oracle\", \"Build extra Oracle geospatial database and raster support\"   option \"with-orfeo5\", \"Build extra Orfeo Toolbox for Processing plugin\"   option \"with-r\", \"Build extra R for Processing plugin\"   option \"with-saga-gis-lts\", \"Build extra Saga GIS for Processing plugin\"   # option \"with-qt-mysql\", \"Build extra Qt MySQL plugin for eVis plugin\"   option \"with-qspatialite\", \"Build QSpatialite Qt database driver\"   option \"with-api-docs\", \"Build the API documentation with Doxygen and Graphviz\"   option \"with-3d\", \"Build with 3D Map View panel\"    depends_on Qgis3DevUnlinkedFormulae    # core qgis   depends_on \"cmake\" =&gt; :build   depends_on \"ninja\" =&gt; [:build, :recommended]   depends_on \"bison\" =&gt; :build   depends_on \"flex\" =&gt; :build   if build.with? \"api-docs\"     depends_on \"graphviz\"     depends_on \"doxygen\"   end    depends_on :python3    depends_on \"qt\" # keg_only   depends_on \"osgeo/osgeo4mac/qt5-webkit\" =&gt; :recommended # keg_only   depends_on \"sip\"   depends_on \"pyqt\"   depends_on \"osgeo/osgeo4mac/pyqt5-webkit\" =&gt; :recommended   depends_on \"qca\"   depends_on \"qtkeychain\"   depends_on \"qscintilla2\"   depends_on \"qwt\"   depends_on \"qwtpolar\"   depends_on \"gsl\"   depends_on \"sqlite\" # keg_only   depends_on \"expat\" # keg_only   depends_on \"proj\"   depends_on \"spatialindex\"   # depends_on \"homebrew/science/matplotlib\" # deprecated   depends_on \"brewsci/science/matplotlib\"   depends_on \"fcgi\" if build.with? \"server\"   # use newer postgresql client than Apple's, also needed by `psycopg2`   depends_on \"postgresql\" =&gt; :recommended   depends_on \"libzip\"    # needed for PKI authentication methods that require PKCS#8-&gt;PKCS#1 conversion   depends_on \"libtasn1\"    # core providers   depends_on \"osgeo/osgeo4mac/gdal2\" # keg_only   depends_on \"osgeo/osgeo4mac/gdal2-python\" # keg_only   depends_on \"osgeo/osgeo4mac/oracle-client-sdk\" if build.with? \"oracle\"   # TODO: add MSSQL third-party support formula?, :optional    # core plugins (c++ and python)   if build.with?(\"grass\") || (HOMEBREW_PREFIX/\"opt/grass7\").exist?     depends_on \"osgeo/osgeo4mac/grass7\"     depends_on \"gettext\" # keg_only   end    # Not until osgearth is Qt5-ready   # if build.with? \"globe\"   #   # this is pretty borked with OS X &gt;= 10.10+   #   depends on \"open-scene-graph\"   #   depends on \"homebrew/science/osgearth\"   # end    depends_on \"gpsbabel\" =&gt; :optional   # TODO: remove \"pyspatialite\" when PyPi package supports spatialite 4.x   #       or DB Manager supports libspatialite &gt;= 4.2.0 (with mod_spatialite)    # TODO: what to do for Py3 and pyspatialite?   # depends on \"pyspatialite\" # for DB Manager   # depends on \"qt-mysql\" =&gt; :optional # for eVis plugin (non-functional in 2.x?)    # core processing plugin extras   # see `grass` above   depends_on \"osgeo/osgeo4mac/orfeo5\" =&gt; :optional   depends_on \"r\" =&gt; :optional   depends_on \"osgeo/osgeo4mac/saga-gis-lts\" =&gt; :optional   # TODO: LASTools straight build (2 reporting tools), or via `wine` (10 tools)   # TODO: Fusion from USFS (via `wine`?)    # TODO: add one for Py3   #       (only necessary when macOS ships a Python3 or 3rd-party isolation is needed)   # resource \"pyqgis-startup\" do   #   url \"https://gist.githubusercontent.com/dakcarto/11385561/raw/e49f75ecec96ed7d6d3950f45ad3f30fe94d4fb2/pyqgis_startup.py\"   #   sha256 \"385dce925fc2d29f05afd6508bc1f46ec84c0bc607cc0c8dfce78a4bb93b9c4e\"   #   version \"2.14.0\"   # end    needs :cxx11    def install     ENV.cxx11      # when gdal2-python.rb loaded, PYTHONPATH gets set to 2.7 site-packages...     #   clear it before calling any local python3 functions     ENV[\"PYTHONPATH\"] = nil     if ARGV.debug?       puts \"python_exec: #{python_exec}\"       puts \"py_ver: #{py_ver}\"       puts \"brewed_python?: #{brewed_python?}\"       puts \"python_site_packages: #{python_site_packages}\"       puts \"python_prefix: #{python_prefix}\"       puts \"qgis_python_packages: #{qgis_python_packages}\"       puts \"gdal_python_packages: #{gdal_python_packages}\"       puts \"gdal_python_opt_bin: #{gdal_python_opt_bin}\"       puts \"gdal_opt_bin: #{gdal_opt_bin}\"     end      # Vendor required python3 pkgs if they are missing     # TODO: this should really be a requirements.txt in src tree     py_req = %w[       future       psycopg2       python-dateutil       httplib2       pytz       six       nose2       pygments       jinja2       pyyaml       requests       owslib     ].freeze      orig_user_base = ENV[\"PYTHONUSERBASE\"]     ENV[\"PYTHONUSERBASE\"] = libexec/\"python\"     system HOMEBREW_PREFIX/\"bin/pip3\", \"install\", \"--user\", *py_req     ENV[\"PYTHONUSERBASE\"] = orig_user_base      # Set bundling level back to 0 (the default in all versions prior to 1.8.0)     # so that no time and energy is wasted copying the Qt frameworks into QGIS.      # Install custom widgets Designer plugin to local qt plugins prefix     mkdir lib/\"qt/plugins/designer\"     inreplace \"src/customwidgets/CMakeLists.txt\",               \"${QT_PLUGINS_DIR}/designer\", lib/\"qt/plugins/designer\".to_s      # Fix custom widgets Designer module install path     mkdir lib/\"python#{py_ver}/site-packages/PyQt5\"     inreplace \"CMakeLists.txt\",               \"${PYQT5_MOD_DIR}\", lib/\"python#{py_ver}/site-packages/PyQt5\".to_s      # Install db plugins to local qt plugins prefix     if build.with? \"qspatialite\"       mkdir lib/\"qt/plugins/sqldrivers\"       inreplace \"src/providers/spatialite/qspatialite/CMakeLists.txt\",                 \"${QT_PLUGINS_DIR}/sqldrivers\", lib/\"qt/plugins/sqldrivers\".to_s     end     if build.with? \"oracle\"       inreplace \"src/providers/oracle/ocispatial/CMakeLists.txt\",                 \"${QT_PLUGINS_DIR}/sqldrivers\", lib/\"qt/plugins/sqldrivers\".to_s     end      args = std_cmake_args     args &lt;&lt; \"-DCMAKE_BUILD_TYPE=RelWithDebInfo\" if build.with? \"debug\" # override      cmake_prefixes = %w[       qt5       qt5-webkit       qscintilla2       qwt       qwtpolar       qca       gdal2       gsl       geos       proj       libspatialite       spatialindex       expat       sqlite       libzip       flex       bison       fcgi     ].freeze     # Force CMake to search HB/opt paths first, so headers in HB/include are not found instead;     # specifically, ensure any gdal v1 includes are not used     args &lt;&lt; \"-DCMAKE_PREFIX_PATH=#{cmake_prefixes.map { |f| Formula[f.to_s].opt_prefix }.join(\";\")}\"      args += %w[       -DENABLE_TESTS=FALSE       -DENABLE_MODELTEST=FALSE       -DQGIS_MACAPP_BUNDLE=0       -DQGIS_MACAPP_INSTALL_DEV=FALSE       -DWITH_QWTPOLAR=TRUE       -DWITH_INTERNAL_QWTPOLAR=FALSE       -DWITH_ASTYLE=FALSE       -DWITH_QSCIAPI=TRUE       -DWITH_STAGED_PLUGINS=TRUE       -DWITH_GRASS=FALSE       -DWITH_CUSTOM_WIDGETS=TRUE     ]      args &lt;&lt; \"-DWITH_QTWEBKIT=#{build.with?(\"qt5-webkit\") ? \"TRUE\" : \"FALSE\"}\"      # Prefer opt_prefix for CMake modules that find versioned prefix by default     # This keeps non-critical dependency upgrades from breaking QGIS linking     args &lt;&lt; \"-DGDAL_LIBRARY=#{Formula[\"gdal2\"].opt_lib}/libgdal.dylib\"     args &lt;&lt; \"-DGEOS_LIBRARY=#{Formula[\"geos\"].opt_lib}/libgeos_c.dylib\"     args &lt;&lt; \"-DGSL_CONFIG=#{Formula[\"gsl\"].opt_bin}/gsl-config\"     args &lt;&lt; \"-DGSL_INCLUDE_DIR=#{Formula[\"gsl\"].opt_include}\"     args &lt;&lt; \"-DGSL_LIBRARIES='-L#{Formula[\"gsl\"].opt_lib} -lgsl -lgslcblas'\"      args &lt;&lt; \"-DWITH_SERVER=#{build.with?(\"server\") ? \"TRUE\" : \"FALSE\"}\"      args &lt;&lt; \"-DPOSTGRES_CONFIG=#{Formula[\"postgresql\"].opt_bin}/pg_config\" if build.with? \"postgresql\"      args &lt;&lt; \"-DWITH_GRASS7=#{(build.with?(\"grass\") || brewed_grass7?) ? \"TRUE\" : \"FALSE\"}\"     if build.with?(\"grass\") || brewed_grass7?       # this is to build the GRASS Plugin, not for Processing plugin support       grass7 = Formula[\"grass7\"]       args &lt;&lt; \"-DGRASS_PREFIX7='#{grass7.opt_prefix}/grass-base'\"       # Keep superenv from stripping (use Cellar prefix)       ENV.append \"CXXFLAGS\", \"-isystem #{grass7.prefix.resolved_path}/grass-base/include\"       # So that `libintl.h` can be found (use Cellar prefix; should not be needed anymore with QGIS 2.99+)       # ENV.append \"CXXFLAGS\", \"-isystem #{Formula[\"gettext\"].include.resolved_path}\"     end      args &lt;&lt; \"-DWITH_GLOBE=#{build.with?(\"globe\") ? \"TRUE\" : \"FALSE\"}\"     if build.with? \"globe\"       osg = Formula[\"open-scene-graph\"]       opoo \"`open-scene-graph` formula's keg not linked.\" unless osg.linked_keg.exist?       # must be HOMEBREW_PREFIX/lib/osgPlugins-#.#.#, since all osg plugins are symlinked there       args &lt;&lt; \"-DOSG_PLUGINS_PATH=#{HOMEBREW_PREFIX}/lib/osgPlugins-#{osg.version}\"     end      args &lt;&lt; \"-DWITH_ORACLE=#{build.with?(\"oracle\") ? \"TRUE\" : \"FALSE\"}\"     if build.with? \"oracle\"       oracle_opt = Formula[\"oracle-client-sdk\"].opt_prefix       args &lt;&lt; \"-DOCI_INCLUDE_DIR=#{oracle_opt}/include/oci\"       args &lt;&lt; \"-DOCI_LIBRARY=#{oracle_opt}/lib/libclntsh.dylib\"     end      args &lt;&lt; \"-DWITH_QSPATIALITE=#{build.with?(\"qspatialite\") ? \"TRUE\" : \"FALSE\"}\"      args &lt;&lt; \"-DWITH_APIDOC=#{build.with?(\"api-docs\") ? \"TRUE\" : \"FALSE\"}\"      args &lt;&lt; \"-DWITH_3D=#{build.with?(\"3d\") ? \"TRUE\" : \"FALSE\"}\"      # nix clang tidy runs     args &lt;&lt; \"-DCLANG_TIDY_EXE=\"      # if using Homebrew's Python, make sure its components are always found first     # see: https://github.com/Homebrew/homebrew/pull/28597     ENV[\"PYTHONHOME\"] = python_prefix      # handle custom site-packages for keg-only modules and packages     ENV.append_path \"PYTHONPATH\", python_site_packages     ENV.append_path \"PYTHONPATH\", libexec/\"python/lib/python/site-packages\"      # handle some compiler warnings     # ENV[\"CXX_EXTRA_FLAGS\"] = \"-Wno-unused-private-field -Wno-deprecated-register\"     # if ENV.compiler == :clang &amp;&amp; (MacOS::Xcode.version &gt;= \"7.0\" || MacOS::CLT.version &gt;= \"7.0\")     #   ENV.append \"CXX_EXTRA_FLAGS\", \"-Wno-inconsistent-missing-override\"     # end      ENV.prepend_path \"PATH\", libexec/\"python/bin\"      mkdir \"build\" do       # editor = \"/usr/local/bin/bbedit\"       # cmake_config = Pathname(\"#{Dir.pwd}/#{name}_cmake-config.txt\")       # cmake_config.write [\"cmake ..\", *args].join(\" \\\\\\n\")       # system editor, cmake_config.to_s       # raise       system \"cmake\", \"-G\", build.with?(\"ninja\") ? \"Ninja\" : \"Unix Makefiles\", *args, \"..\"       # system editor, \"CMakeCache.txt\"       # raise       system \"cmake\", \"--build\", \".\", \"--target\", \"all\", \"--\", \"-j\", Hardware::CPU.cores       system \"cmake\", \"--build\", \".\", \"--target\", \"install\", \"--\", \"-j\", Hardware::CPU.cores     end      # Fixup some errant lib linking     # TODO: fix upstream in CMake     dy_libs = [lib/\"qt/plugins/designer/libqgis_customwidgets.dylib\"]     dy_libs &lt;&lt; lib/\"qt/plugins/sqldrivers/libqsqlspatialite.dylib\" if build.with? \"qspatialite\"     dy_libs.each do |dy_lib|       MachO::Tools.dylibs(dy_lib.to_s).each do |i_n|         %w[core gui native].each do |f_n|           sufx = i_n[/(qgis_#{f_n}\\.framework.*)/, 1]           next if sufx.nil?           i_n_to = \"#{opt_prefix}/QGIS.app/Contents/Frameworks/#{sufx}\"           puts \"Changing install name #{i_n} to #{i_n_to} in #{dy_lib}\" if ARGV.debug?           dy_lib.ensure_writable do             MachO::Tools.change_install_name(dy_lib.to_s, i_n.to_s, i_n_to, :strict =&gt; false)           end         end       end     end      # Update .app's bundle identifier, so other installers doesn't get confused     inreplace prefix/\"QGIS.app/Contents/Info.plist\",               \"org.qgis.qgis3\", \"org.qgis.qgis3-hb-dev\"      py_lib = lib/\"python#{py_ver}/site-packages\"     py_lib.mkpath     ln_s \"../../../QGIS.app/Contents/Resources/python/qgis\", py_lib/\"qgis\"      ln_s \"QGIS.app/Contents/MacOS/fcgi-bin\", prefix/\"fcgi-bin\" if build.with? \"server\"      doc.mkpath     mv prefix/\"QGIS.app/Contents/Resources/doc/api\", doc/\"api\" if build.with? \"api-docs\"     ln_s \"../../../QGIS.app/Contents/Resources/doc\", doc/\"doc\"      # copy PYQGIS_STARTUP file pyqgis_startup.py, even if not isolating (so tap can be untapped)     # only works with QGIS &gt; 2.0.1     # doesn't need executable bit set, loaded by Python runner in QGIS     # TODO: for Py3     # (libexec/\"python\").install resource(\"pyqgis-startup\")      bin.mkdir     qgis_bin = bin/name.to_s     touch qgis_bin.to_s # so it will be linked into HOMEBREW_PREFIX     qgis_bin.chmod 0755     post_install   end    def post_install     # configure environment variables for .app and launching binary directly.     # having this in `post_intsall` allows it to be individually run *after* installation with:     #    `brew postinstall -v &lt;formula-name&gt;`      app = prefix/\"QGIS.app\"     tab = Tab.for_formula(self)     opts = tab.used_options      # define default isolation env vars     pthsep = File::PATH_SEPARATOR     pypth = python_site_packages.to_s     pths = %w[       /usr/bin       /bin       /usr/sbin       /sbin       /opt/X11/bin       /usr/X11/bin       #{opt_libexec}/python/bin     ]      # unless opts.include?(\"with-isolation\")     #   pths = ORIGINAL_PATHS.dup     #   pyenv = ENV[\"PYTHONPATH\"]     #   if pyenv     #     pypth = pyenv.include?(pypth) ? pyenv : pypth + pthsep + pyenv     #   end     # end      unless pths.include?(HOMEBREW_PREFIX/\"bin\")       pths = pths.insert(0, HOMEBREW_PREFIX/\"bin\")     end      # set install's lib/python#{py_ver}/site-packages first, so app will work if unlinked     pypths = %W[       #{opt_lib}/python#{py_ver}/site-packages       #{opt_libexec}/python/lib/python/site-packages       #{pypth}     ]      pths.insert(0, gdal_opt_bin)     pths.insert(0, gdal_python_opt_bin)     pypths.insert(0, gdal_python_packages)      if opts.include?(\"with-gpsbabel\")       pths.insert(0, Formula[\"gpsbabel\"].opt_bin.to_s)     end      envars = {       :PATH =&gt; pths.join(pthsep),       :PYTHONPATH =&gt; pypths.join(pthsep),       :GDAL_DRIVER_PATH =&gt; \"#{HOMEBREW_PREFIX}/lib/gdalplugins\",       :GDAL_DATA =&gt; \"#{Formula[\"gdal2\"].opt_share}/gdal\",     }      # handle multiple Qt plugins directories     qtplgpths = %W[       #{Formula[\"qt\"].opt_prefix}/plugins       #{HOMEBREW_PREFIX}/lib/qt/plugins     ]     envars[:QT_PLUGIN_PATH] = qtplgpths.join(pthsep)      proc_algs = \"Contents/Resources/python/plugins/processing/algs\"     if opts.include?(\"with-grass\") || brewed_grass7?       grass7 = Formula[\"grass7\"]       # for core integration plugin support       envars[:GRASS_PREFIX] = \"#{grass7.opt_prefix}/grass-base\"       begin         inreplace app/\"#{proc_algs}/grass7/Grass7Utils.py\",                   \"'/Applications/GRASS-7.{}.app/Contents/MacOS'.format(version)\",                   \"'#{grass7.opt_prefix}/grass-base'\"         puts \"GRASS 7 GrassUtils.py has been updated\"       rescue Utils::InreplaceError         puts \"GRASS 7 GrassUtils.py already updated\"       end     end      unless opts.include?(\"without-globe\")       osg = Formula[\"open-scene-graph\"]       envars[:OSG_LIBRARY_PATH] = \"#{HOMEBREW_PREFIX}/lib/osgPlugins-#{osg.version}\"     end      # TODO: add for Py3     # if opts.include?(\"with-isolation\") || File.exist?(\"/Library/Frameworks/GDAL.framework\")     #   envars[:PYQGIS_STARTUP] = opt_libexec/\"python/pyqgis_startup.py\"     # end      # envars.each { |key, value| puts \"#{key.to_s}=#{value}\" }     # exit      # add env vars to QGIS.app's Info.plist, in LSEnvironment section     plst = app/\"Contents/Info.plist\"     # first delete any LSEnvironment setting, ignoring errors     # CAUTION!: may not be what you want, if .app already has LSEnvironment settings     dflt = `defaults read-type \\\"#{plst}\\\" LSEnvironment 2&gt; /dev/null`     `defaults delete \\\"#{plst}\\\" LSEnvironment` if dflt     kv = \"{ \"     envars.each { |key, value| kv += \"'#{key}' = '#{value}'; \" }     kv += \"}\"     `defaults write \\\"#{plst}\\\" LSEnvironment \\\"#{kv}\\\"`     # add ability to toggle high resolution in Get Info dialog for app     hrc = `defaults read-type \\\"#{plst}\\\" NSHighResolutionCapable 2&gt; /dev/null`     `defaults delete \\\"#{plst}\\\" NSHighResolutionCapable` if hrc     `defaults write \\\"#{plst}\\\" NSHighResolutionCapable \\\"True\\\"`     # leave the plist readable; convert from binary to XML format     `plutil -convert xml1 -- \\\"#{plst}\\\"`     # make sure plist is readble by all users     plst.chmod 0644     # update modification date on app bundle, or changes won't take effect     touch app.to_s      # add env vars to launch script for QGIS app's binary     qgis_bin = bin/name.to_s     rm_f qgis_bin if File.exist?(qgis_bin) # install generates empty file     bin_cmds = %W[#!/bin/sh\\n]     # setup shell-prepended env vars (may result in duplication of paths)     unless pths.include? HOMEBREW_PREFIX/\"bin\"       pths.insert(0, HOMEBREW_PREFIX/\"bin\")     end     # even though this should be affected by with-isolation, allow local env override     pths &lt;&lt; \"$PATH\"     pypths &lt;&lt; \"$PYTHONPATH\"     envars[:PATH] = pths.join(pthsep)     envars[:PYTHONPATH] = pypths.join(pthsep)     envars.each { |key, value| bin_cmds &lt;&lt; \"export #{key}=#{value}\" }     bin_cmds &lt;&lt; opt_prefix/\"QGIS.app/Contents/MacOS/QGIS \\\"$@\\\"\"     qgis_bin.write(bin_cmds.join(\"\\n\"))     qgis_bin.chmod 0755   end    def caveats     s = &lt;&lt;-EOS.undent       Bottles support only Homebrew's Python3        QGIS is built as an application bundle. Environment variables for the       Homebrew prefix are embedded in QGIS.app:         #{opt_prefix}/QGIS.app        You may also symlink QGIS.app into /Applications or ~/Applications:         brew linkapps [--local]        To directly run the `QGIS.app/Contents/MacOS/QGIS` binary use the wrapper       script pre-defined with Homebrew prefix environment variables:         #{opt_bin}/#{name}        NOTE: Your current PATH and PYTHONPATH environment variables are honored             when launching via the wrapper script, while launching QGIS.app             bundle they are not.        For standalone Python3 development, set the following environment variable:         export PYTHONPATH=#{qgis_python_packages}:#{gdal_python_packages}:#{python_site_packages}:$PYTHONPATH      EOS      s += &lt;&lt;-EOS.undent       If you have built GRASS 7 for the Processing plugin set the following in QGIS:         Processing-&gt;Options: Providers-&gt;GRASS GIS 7 commands-&gt;GRASS 7 folder to:            #{HOMEBREW_PREFIX}/opt/grass7/grass-base      EOS      s   end    test do     output = `#{bin}/#{name.to_s} --help 2&gt;&amp;1` # why does help go to stderr?     assert_match /^QGIS is a user friendly/, output   end    private    def brewed_grass7?     Formula[\"grass7\"].opt_prefix.exist?   end    def python_exec     if brewed_python?       Formula[\"python3\"].opt_bin/\"python3\"     else       py_exec = `which python3`.strip       raise if py_exec == \"\"       py_exec     end   end    def py_ver     `#{python_exec} -c 'import sys;print(\"{0}.{1}\".format(sys.version_info[0],sys.version_info[1]))'`.strip   end    def brewed_python?     Formula[\"python3\"].linked_keg.exist?   end    def python_site_packages     HOMEBREW_PREFIX/\"lib/python#{py_ver}/site-packages\"   end    def python_prefix     `#{python_exec} -c 'import sys;print(sys.prefix)'`.strip   end    def qgis_python_packages     opt_lib/\"python#{py_ver}/site-packages\".to_s   end    def gdal_python_packages     Formula[\"gdal2-python\"].opt_lib/\"python#{py_ver}/site-packages\".to_s   end    def gdal_python_opt_bin     Formula[\"gdal2-python\"].opt_bin.to_s   end    def gdal_opt_bin     Formula[\"gdal2\"].opt_bin.to_s   end    def module_importable?(mod)     `#{python_exec} -c 'import sys;sys.path.insert(1, \"#{gdal_python_packages}\"); import #{mod}'`.strip   end end   Anyhow, I think I’m going to keep the 3.1 version for a while.                  No, you can see that because I deteletd the branch… Sorry. &#8617;           ","categories": ["GIS","Personal","Technology"],
        "tags": ["homebrew","macOS","QGIS"],
        "url": "https://luispuerto.net/blog/2018/03/12/qgis-on-macos-with-homebrew/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/QGIS_logo_2017.svg.png"
},{
        "title": "Updating to R 3.4.4 \"Someone to Lean On\"",
        "excerpt":"Just today yesterday Last Thursday —took me more that I expected to have a moment to write finish this post— R was updated to 3.4.4 and this new released is called “Someone to Lean On”, which is —as all the rest are— a reference to Peanuts comic. These are the released notes:      NEW FEATURES:          Sys.timezone() tries more heuristics on Unix-alikes and so is more likely to succeed (especially on Linux).  For the slowest method, a warning is given recommending that TZ is set to avoid the search.     The version of LAPACK included in the sources has been updated to 3.8.0 (for the routines used by R, a very minor bug-fix change).     parallel::detectCores(logical = FALSE) is ignored on Linux systems, since the information is not available with virtualized OSes.       INSTALLATION on a UNIX-ALIKE:          Configure will use pkg-config to find the flags to link to jpeg if available (as it should be for the recently-released jpeg-9c and libjpeg-turbo).  (This amends the code added in R 3.3.0 as the module name in jpeg-9c is not what that tested for.)       DEPRECATED AND DEFUNCT:          Sys.timezone(location = FALSE) (which was a stop-gap measure for Windows long ago) is deprecated.  It no longer returns the value of environment variable TZ (usually a location).     Legacy support of make macros such as CXX1X is formally deprecated: use the CXX11 forms instead.     BUG FIXES:     power.prop.test() now warns when it cannot solve the problem, typically because of impossible constraints. (PR#17345)     removeSource() no longer erroneously removes NULL in certain cases, thanks to D’enes T’oth.     nls(`NO [mol/l]` ~ f(t)) and nls(y ~ a) now work.  (Partly from PR#17367)     R CMD build checks for GNU cp rather than assuming Linux has it. (PR#17370 says ‘Alpine Linux’ does not.)     Non-UTF-8 multibyte character handling fixed more permanently  (PR#16732).     sum(, ) is more consistent. (PR#17372)     rf() and rbeta() now also work correctly when ncp is not scalar, notably when (partly) NA.  (PR#17375)     R CMD INSTALL now correctly sets C++ compiler flags when all source files are in sub-directories of src.      If you are on macOS and you want to enjoy the improvements and the bug fixings, you just can download the binary from CRAN or if you are a Homebrew user you just can update with the following command   1 $ brew update &amp;&amp; brew upgrade   This time the binaries for macOS came almost as the same time as the binaries for the rest of the platforms and I really welcome the change and the diligence. Sometimes I feel as a macOS user a second class user in some open source projects, since they release the new versions a little bit later on macOS than in the rest of the platforms. However, I really appreciate the work that all these people put in develop R and to bring to it to the macOS ecosystem. I acknowledge that they are doing a non-payed job and they are doing it for the community of users, which is incredible remarkable.   On the other hand, this time Homebrew was the one falling back a little bit, and on Thursday afternoon the formula to install R hadn’t been updated yet. So I just, just decided to update myself and make my first contribution to the Homebrew project. Update a Homebrew formula isn’t rocket science, and they even have a script to make things easier, so I really encourage you to update Homebrew formula and contribute to the community. In the next post I’ll try to explain the procedure of how to update a Homebrew formula.   Happy data analysis with the new R.  ","categories": ["Professional","RStats","Technology"],
        "tags": ["data science","homebrew","RStats"],
        "url": "https://luispuerto.net/blog/2018/03/19/updating-to-r-3-4-4-someone-to-lean-on/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/someoonetoleanon.jpg"
},{
        "title": "How I updated the R formula in Homebrew",
        "excerpt":"As I just explained in my last post, the last version of R —3.4.4 “Someone to Lean on”— was rolled out las Thursday and this time I was the one that updated the Homebrew’s formula to reflect the new version. I just decided to give it a try and update for first time a Homebrew’s formula since no one had updated it yet on Thursday afternoon and this way I would be able to install the new version with Homebrew.   As I’ve already mentioned, I really encourage anyone to contribute to Homebrew’s community and try to keep the formulae update. Homebrew also encourage this, since as more people keep an eye on formula and update them, the better for the community.   How can you update a Homebrew formula?   The first and foremost important thing is to check if someone already filed a pull request for that same formulae, or in other words, if that formula is in the process of being updated. If no one is updating that formula, it’s opportunity to contribute and update it for the benefit of the community.   Before you begin, you need to update Homebrew to get the last version:   1 $ brew update   When you have the last version of Homebrew, you can edit your formula. If you just going to update the formula because there is a new version of the software that the formula installs, the best way to go is to use the script Homebrew provided to update formulae. In the case of R formula —r.rb— the syntax was as follows.   1 $ brew bump-formula-pr --strict R with --url=https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz and --sha256=b3e97d2fab7256d1c655c4075934725ba1cd7cb9237240a11bb22ccdad960337   In this case, the URL and the sha256 was provided by Peter Dalgaard in the R 3.4.4 announcement. However, you have to take into account that https urls are preferred, so I had to recommit to follow that guidelines.   If you don’t have the sha256, you can calculate it yourself downloading the file and then running the following command that for me was as follows:   1 $ openssl sha -sha256 ~/Downloads/R-3.4.4.tar.gz   When you have those two parameters, you are ready to run the command. What the command does is basically create a fork of Homebrew in your account of Github, and then create a branch where it’s going to upload the modified formula. Then, it just makes a pull request to Homebrew/homebrew-core.  If everything is ok, they’ll accept your changes and the new formula will be accessible to everyone after they run $ brew update. On the other hand, if you have to make modifications in the formula you are submitting in your pull request —as I needed to make— you just do them using Git.   1 2 3 $ cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core # go to your local repo for Homebrew $ git branch # here you select the branch with the name of the update you are creating $ brew edit R # your preferred editor with open   Now, you can make the necessary changes and when you are done you just save the file and   1 2 3 $ git add R.rb # in this specific case $ git commit -m \"fixed the previous error\" $ git push YourForkOfTheRepo TheNameOfTheBranch   If you want to test your version of the formula this is easy, you just stay in the branch where the modified formula is and run:   1 $ brew upgrade R                        Updating R with Homebrew              ## Use someone else’s formula   You can even use this updated formula to update the R of someone computer while they accept your pull request —if you are in a hurry or you are antsy guy. To do so you can use hub.   1 2 3 4 $ brew install hub $ brew update $ cd $(brew --repository) $ hub pull someone_else   Or you can directly install the formula that is their repo in GitHub:   1 $ brew install https://raw.github.com/user/repo/branch/formula.rb   In my case:   1 $ brew install https://github.com/luisspuerto/homebrew-core/blob/r-3.4.4/Formula/r.rb   Or pull the formula from the pull request:   1 $ brew pull https://github.com/Homebrew/homebrew-core/pull/1234   In my case:   1 $ brew pull https://github.com/Homebrew/homebrew-core/pull/25321   However, I don’t recommend you to do something like this and install the someone’s formula unless you really know the person. You never know what it’s in the install script. Usually, pull request are resolved within hours and it’s much better practice to wait until the Homebrew maintainers review the updated formula and approve it.  ","categories": ["RStats","Technology"],
        "tags": ["data science","homebrew","RStats"],
        "url": "https://luispuerto.net/blog/2018/03/19/how-i-updated-the-r-formula-in-homebrew/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/R-Homebrew.jpg"
},{
        "title": "Why don't I like Facebook anymore?",
        "excerpt":"                     #deletefacebook from TechCrunch post.              It’s funny, but I was on my way to write this post about why I don’t like Facebook anymore and suddenly the Cambridge Analytica scandal about Facebook just explode the last weekend. At the moment of writing this post, the front page of Reddit has a good number of posts about the scandal and r/Technology —the screenshot— is full of post about the issue. Not to mention that are also a couple of good posts in r/Politics, r/Worldnews and r/News. And of course all over the Internet too. There is even a hashtag in Twitter —#DeleteFacebook— with an even other hashtags associated —like #cambridgeanalytics— where people look for advice to delete their data from Facebook accounts and are vowing they are going to delete their accounts. All of this scandal started —I think— with this article on The Guardian, where an ex-employee and co-founder of Cambridge Analytica denounced how they harvested information from about 50 million of Facebook accounts. Not so much, if you take into account that Facebook has 2.2. billion of users.   All of this touches me a little bit closer, since I really believe that Data Science —yes this is what Cambridge Analytica was doing, Data Science— is here to help people to improve their lives and to understand better our world and lives, not to try to swing elections. Basically, what they were doing was to understand the psychological profiles of those Facebook profiles and then they crafted messages that appealed to their most inner core values to make them vote or change their vote in the elections. In other words “information warfare” at the service of political ideologies, or perhaps in this case not even that, just at the service of the powerful. Something, that surprisingly —or unsurprisingly— has been already depicted on media.                        I bet these two don’t have a Facebook account              However, this is not a post about how bad Facebook is, about your privacy or conspiracy theories about how they  and other social media platforms and networks want to harvest your data to sell it and to control you. Don’t worry, I’m not going to ask you to delete your Facebook, Twitter, Google+ or [put here whatever social network you are concerned about] accounts, get a tinfoil hat and hide in your basement. That is totally up to you, and how crazy you are about the issue, and I really think it’s a really personal decision. I still believe that social networks, Facebook included, are services that really can help us to connect with other people all over the world, organize events or make us discover what is going on in our city, university or whatever social group or circle you are involved.   I don’t think that hiding is the answer either, but exactly one of the exacts reactions they want us to have. They want us to shut up, get quiet, be scared, follow their commands and continue with our lives. When we don’t share our ideas —political or not— our good moments, our photos, our lives… because we’re scare of what other people may think or what we can lose, then we are losing and they are winning. Do we really want to live in a society where what you shared in a social network, or any other public platform, at some point could be the difference between have some job, or be attacked by others? If we lose our freedom of speak up about what we think isn’t right them we turned our society into a kind of social dictatorship. Honest and respectful debate always helps, no mater the topic. Perhaps you are wondering who are they… well this is going to be a shock perhaps, but they are you and me and the next guy and other guy a little bit away. They is society, and it’s a system, we’re we all interact. Perhaps some people has a longer lever than others, or even have more than one lever, or even they have some other people working for them with their levers, but we all are they and we can interact in the system. So, we all part and solution of the problem.   Nevertheless, Facebook has lost its appeal for me and I no longer interested in spend a lot of time there. I would even say that my life has improved since I’m there just a couple of times a week and just for a couple of minutes. I just find it boring and no longer fits my communication and social needs. Why? Well there are several reasons and I’ll try to explain them next.   Facebook isn’t the correct format for me anymore   I don’t feel interested to share content on Facebook anymore because I find if too simple. Although it’s more painful, it takes me more effort, I prefer blogging. Blogging allows me to create more complex post and develop a much great deal of ideas in comparison with Facebook.   Facebook is designed to share small pieces of text without almost any formatting. This makes really difficult to create and explain complex ideas. Also usually when you write too much, Facebook just hide it and put a button to expand the text.   It’s true that you can share other content and not only photos in Facebook, but I find other networks more suitable to do it. If I just want to share a couple of photos, usually Instagram fits the bill much better and if want to share a whole album Flickr looks better suited for that.                        Facebook update box              Before Facebook came out that was the way we shared content, using multiple open services. After ponder how Facebook has contributed to share that content I think that with nothing positive or worthy. More the contrary, our content now is trapped in a platform that unless you have an account you see or share with others. It’s stuck there forever.   I really like the kind of interaction a blog provides. More meaningful, real and open. It allows me share content in more rich and diverse ways.   Facebook is an attention whore   This’s something that I really detest from Facebook with all my soul and I think it has become more acute lately. I really don’t know if it’s because this has been always the case, but since now I spend much much less time there —so they are trying to lure me back— or because they’ve deployed that policy lately to try to keep people engaged to the platform. I abhor it so much that I finally deleted the app from my iPhone and iPad because to get rid of the notifications and red bubbles.   Facebook want you to be inside of its interface as much as you can, so if you don’t hang out there for a bit, it’s going to create for you fake notifications to lure you into the application. At least in my case they where in the form of “friend has publish a photo” or “random friend has updated their state for a while”, when I have disable those kind of notifications in my profile. I’ve read somewhere that this is supposedly a bug in Facebook, but I really doubt it. A big company like Facebook seldom make that kind of errors, and even less they keep for so long.  This is clearly a designed feature to lure you again inside. Other typical notifications were about some event that this or the other friend were about to attend and they were totally not interesting for me. Why? Whyyyyy?   Almost, 99% of those notifications had zero interest for me and were really annoying in the end.   Ads everywhere and in hiding in multiple forms   Facebook is full of ads. They are the source of their revenue. They have ads in the sidebar, featured groups, pages, someone you perhaps know has liked some page, a featured post here and there… I despise that. I understand that as a company needs revenue to run their payroll and their servers, but at least in my opinion this is too much for me. Google has a much better approach, trying to be as less intrusive as possible. Normally, I use a script on my browser —F.B. Purity— that, on top of the ad blocker, remove all those annoyances. However I can’t have it on my mobile and my tablet. It would be fine if they had some ad here and there, but not every three post or something like that.   For me the most annoying thing are when Facebook pretend that an advertisement could be disguised as an interaction from one of your friends. Please don’t do that, don’t build fake stories to fill the timeline.   Toxic or silly conversation                 Facebook has become a political tool, proof of this is just the last weekend scandal about Cambridge Analytical, and that is the less obvious way. And I say less obvious because they were doing it on the hiding.  However, there’re much most obvious ways to use Facebook —or Twitter— as a political tool. There is an incredible amount of pages and groups which aim is to discuss politics promote political ideas, or rather, be a battering ram against the opposite political group. There is a lot of toxic people and conversation there and a lot of disrespect for other. Besides, conversations can get pretty hairy easily, and you can easily be called names quick just to express your opinion with respect. Suddenly, people get angry or offended really fast and they don’t behave like they were talking to other human being but just to their computers. Computers and technology facilitate communication, but it also make it easy to forget that in the other side there is someone with feelings.   On the other hand, Facebook is full of content that doesn’t have any interested for me and due the way Facebook is design it ended in my timeline. I really care about my friends and what is going on with their lives,  but I even care more when we are able to be together in the same spacetime, have a beer and talk. In other words, meaningful and real social interaction.   Time consuming   Facebook is a time-consuming machine and it’s been designed for that, to trap us inside and keeps us browsing and browsing the life of others. That doesn’t have any sense for me, doesn’t produce anything or have any outcome and in the end keep me separate from others. I like much more to spend time writing in English this blog about what I care and like, and using it to improve my English skills. When the web 2.0 came out everyone talked about being a prosumer —producer and consumer. I think that facebook has make us to be just consumers. Consumers of the life of others instead of producing out own life.   I prefer Twitter or Reddit   Although Twitter has some of the same problems that Facebook has, I prefer it because it’s much simpler. You have a small set of characters —240— so you have to be brief and direct and it’s mean to be use in real time. You don’t browse your the timeline so much back in time, perhaps just one hour or so, so you don’t spend too much time there. What it’s important is now &amp; here.  You can have a richer timeline in Twitter since it’s totally open, or most people use it that way. You don’t need to be friend of someone, you just follow them. I can interact there not only with friends and family, but almost with anyone, which is great. Personal and professional interactions also mix up, which make it more real. Everyone is at the same level.   Twitter has also ads and fake notifications —someone and some other have liked some third person post. However, they are easily avoidable if you use a third-party app, as I do —TweetBot.   I also like Reddit a lot. Reddit is really organic and mostly anonymous. You don’t even need to have an email account to register. I almost find gems in Reddit everyday. However, I don’t know if I would call reddit a social network, or just a place where people share stuff and other vote how worthy it’s and make comments about it.   Reddit is composed of subreddits, that are like thematic sub-forums or something like that. The mechanic of Reddit is really simple. Someone post something, usually a piece of text, which could be more complex than anything you can post in Facebook, or a link to something —image, gif, video, or other webpage. From there on things begin to become pretty wild and organic. Every post could be voted —positively or negatively— there are comments, people commenting in those comments and so on… Comments can be voted too. This creates a pretty interesting democratic structure where the most interesting content —for the community— get up in the page and the less interesting is buried to the bottom. It isn’t perfect, of course, but since it’s pretty democratic and free, the outcome is quite interesting and sometimes unpredictable. It isn’t free of toxicity and a week ago or so, the New Yorker, whose parent company owns Reddit, published an article about their struggle to keep things free of trolls and toxic conversation. There has been also pretty mess up things, like when they thought that they have capture the Boston bomber and other pretty similar things. Mobs and circle-jerking are dangerous everywhere.   Bottom line   As a result, I really think that I’m going to use less and less Facebook. I don’t think that I’m going to delete my account or just stop to use it. Facebook is still a valuable tool for me, it allows me to connect with my friends around the globe, specially with the Spanish ones. However, I encourage you to use other way to contact me, like email, messaging or just phone me if you know my number.   What probably is going to change is the way I interact there and the time I’m going to spend around, and I’m probably to spend more and more time creating post in this blog and less just browsing Facebook.  ","categories": ["Personal","Technology"],
        "tags": ["blogs","data science","facebook","privacy","social media"],
        "url": "https://luispuerto.net/blog/2018/03/21/why-dont-i-like-facebook-anymore/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Facebook-dislike.jpg"
},{
        "title": "Updating Homebrew, R and Python pip packages + Ruby Gems + macOS",
        "excerpt":"Update Monday 2nd of April 2018: I decided to add to this post how to update the RubyGems since they are a key feature of Jekyll.   Update Thursday, 19th of April 2018 11.00 UTC+3: I added info about how to update on the CLI the App Store Apps and macOS software updates.     In the post about Homebrew I think I’ve already posted how I update Homebrew packages with a single comment in the terminal. However these days I update more software via terminal, mainly packages, because it’s really handy run just a couple of commands and then see how the software it’s updated automatically and easily.   Homebrew packages   To update Homebrew packages I run:   1 $ brew update &amp;&amp; brew upgrade &amp;&amp; brew cleanup &amp;&amp; brew prune &amp;&amp; brew cu -ay &amp;&amp; brew cask cleanup      brew update updates Homebrew itself and download the last version of the formulae   brew upgrade updates the packages you have installed that have new formulae.   brew cleanup cleans the cache of old versions of packages.   brew prune clean the old symbolic links form `/usr/bin/`.   brew cu -ay uses buo/homebrew-cask-upgrade to update casks. -ay flag is all and yes update all outdated apps.   brew cask cleanup clean the old caches of the updated apps.   If for some reason you don’t want to update an specific package in Homebrew, you can _pin _in to an specific version or the current version.   1 $ brew pin &lt;formulae&gt;   R packages   To update R packages I run:   1 2 3 4 # to see what are the old packages $ Rscript --vanilla -e \"old.packages(repos = 'cloud.r-project.org')\" # to directly update $ Rscript --vanilla -e \"update.packages(ask = F, repos = 'cloud.r-project.org')\"   Update R packages on terminal is done using the command Rscript that allows us to send commands to R using the shell. I use the flag --vanilla that combine --no-save, --no-restore, --no-site-file--no-init-file and --no-environ. In other words a way to load R faster and with a standard configuration.   I like to run old.packages first because I like to see a list first of the packages I’m going to update and then update. I do this because some packages —data.table— need a different makevars than the rest of the packages so just in case I needed to change the makevars and rebuild that package.   I really think it would be cool to be able also _pin _packages in R, but I haven’t found any way to do so at system wide level. You can do easily at project level with the package Packrat. However, I really think it would be really nice to have email notifications when new versions of packages hit CRAN repository and some function to pin packages to the current version.   Python pip   To update all the packages from pip and pip itself I run:   1 2 $ pip install --upgrade pip &amp;&amp; pip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U $ pip3 install --upgrade pip &amp;&amp; pip3 freeze --local | grep -v '^-e' | cut -d = -f 1  | xargs -n1 pip3 install -U   I took the idea from here.   pip is the package manager for packages written in Python. You can read a little bit more on the Wikipedia.   RubyGems   RubyGems is a package manager for Gems that are packages written in Ruby language. Homebrew is written in Ruby and Jekyll too, the latter make use of several RubyGems for functionalities and extensions.   To update the RubyGems you run the following command:   1 $ gem update   App Store and macOS software updates   Although these ones are really easy and you can update them just using the Mac App Store app in your Mac, it’s possible to trigger the update checking and the update itself through CLI. macOS has a command that allow you to make this happen softwareupdate. You can run it like this:   1 2 3 4 5 6 $ softwareupdate -l ## to list all the updates $ softwareupdate -i ## to install updates $ softwareupdate -ia ## to install all updates $ softwareupdate -iR ## to automatically restart if necessary by the update $ softwareupdate -ir ## install only the recommended updates $ softwareupdate -d ## only download the updates   As you see it’s quite thorough and you can see more options with running man softwareupdate.   If you want something more complex you can install mas-cli, which is a Mac App Store command line interface. Install you just run:   1 $ brew install mas   Then, you can run in your command line the following to update:   1 2 3 4 5 6 $ mas list ## List your Mac App Store apps $ mas search &lt;app&gt; ## Search for an app $ mas install &lt;app-number&gt; ## Install an specific app $ mas outdated ## shows the outdated apps $ mas upgrade ## Upgrade all your apps $ mas upgrade &lt;app-number&gt; ## Upgrade an specific app  ","categories": ["RStats","Technology"],
        "tags": ["app store","homebrew","packages","python","RStats","rubygems"],
        "url": "https://luispuerto.net/blog/2018/03/22/updating-homebrew-r-and-python-pip-packages-ruby-gems-macos/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Homebrew-R-Python-Ruby-macOS.png"
},{
        "title": "R and Java 10",
        "excerpt":"Update 27th of March 2018 at 17.28 UTC+3: After talk a little bit on the R mail lists [1][2], it’s seems that it’s a know issue and they are on the way to fix this. Till them we have to stick to Java 9.0.4 to work with R.   Update 4th of April 2018 at 17.55 UTC+3: The rJava author just fix this in an update that haven’t been fed to CRAN yet, bu you can install from source from his repo on rforge.net using the following command:   1 install.packages('rJava', repos = 'http://rforge.net')     Java 10 was just released a week ago on 20 March 2018 and I just updated to it using Homebrew Cask. However, I haven’t been able to make it work with R. As you know, I have a complete install of R with Homebrew, which I’m very happy with since it’s it easier to manage —in my opinion— and it’s faster. I would say also that R and Java are infamous for their bad relations and it’s not the first time that me and a whole bunch of people are driven nuts for not be able to configure it appropriately, even more in macOS.   In the moment of writing this post I’m on R 3.4.4 on macOS 10.13.3 with Java 10 and 9.0.4 installed.   First, I have to tell that I’m puzzle by something that it’s really intrigued, R CMD javarenconf and sudo R CMD javarendonf doesn’t yield the same result:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ R CMD javareconf Java interpreter : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/java Java version     : 10 Java home path   : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/javac Java headers gen.: /usr/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/jar Non-system Java on macOS  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang  -I/usr/local/Cellar/r/3.4.4/lib/R/include -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang++ -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -L/usr/local/Cellar/r/3.4.4/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.4.4/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.4.4/lib/R override rw-r--r--  root/admin for /usr/local/Cellar/r/3.4.4/lib/R/etc/Makeconf? (y/n [n]) y override rw-r--r--  root/admin for /usr/local/Cellar/r/3.4.4/lib/R/etc/ldpaths? (y/n [n]) y Done.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ sudo R CMD javareconf Java interpreter : /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/bin/java Java version     : 9.0.4 Java home path   : /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/bin/javac Java headers gen.: /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/bin/jar Non-system Java on macOS  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang  -I/usr/local/Cellar/r/3.4.4/lib/R/include -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang++ -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -L/usr/local/Cellar/r/3.4.4/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.4.4/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.4.4/lib/R Done.   I really don’t understand why… with root privileges it uses Java 9.0.4 and without it uses Java 10 and it’s not only my computer but all the rest of the Mac computers I have around, one of it with El Capitan instead of High Sierra. If you know why is like this please tell me.   This problem has the consequence that if I choose Java 10 —or configure R with Java without root privileges— R and Java doesn’t behave properly. In other words, I’m not able to build rJava properly.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 warning: [options] bootstrap class path not set in conjunction with -source 6 warning: [options] source value 6 is obsolete and will be removed in a future release warning: [options] target value 1.6 is obsolete and will be removed in a future release warning: [options] To suppress warnings about obsolete options, use -Xlint:-options. Note: Some input files use or override a deprecated API. Note: Recompile with -Xlint:deprecation for details. Note: Some input files use unchecked or unsafe operations. Note: Recompile with -Xlint:unchecked for details. 4 warnings /usr/bin/javah -d . -classpath . org.rosuda.JRI.Rengine Unable to locate an executable at \"/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/javah\" (-1) make[2]: *** [org_rosuda_JRI_Rengine.h] Error 2 make[1]: *** [src/JRI.jar] Error 2 make: *** [jri] Error 2 ERROR: compilation failed for package ‘rJava’ * removing ‘/Users/lpuerto/Library/R/3.x/library/rJava’ * restoring previous ‘/Users/lpuerto/Library/R/3.x/library/rJava’  The downloaded source packages are in     ‘/private/var/folders/wf/41gjf2mx7m7fmvfd8dr22_5h0000gn/T/RtmpT2kJMY/downloaded_packages’ Warning message: In install.packages(\"rJava\", repos = \"cloud.r-project.org\") :   installation of package ‘rJava’ had non-zero exit status   If we look closer to the error we can see that the source of the problem are, most probably, the highlighted lines —10 and 11— which mainly say javah can’t be found. I checked in /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/ and it’s true that there isn’t a javah there. However, if I go the the Java 10 release notes they mention:      tools/javah  ➜ JEP 313 Remove the Native-Header Generation Tool (javah)  As previously announced, the native-header tool, javah, has been removed.Native headers can now be generated by using the Java compiler, javac, with the -h option.       See JDK-8182758    In other words, javah isn’t missing, they removed it on purpose. They stated there a way to create headers with Java, using the Java compiler, javac, with the -h option.   I’ve tried to to parse the this when running R CMD javareconf as:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ R CMD javareconf JAVAH=\"/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/javac -h\" /usr/local/Cellar/r/3.4.4/lib/R/bin/javareconf: line 66: -h: command not found Java interpreter : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/java Java version     : 10 Java home path   : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/javac Java headers gen.: /usr/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/bin/jar Non-system Java on macOS  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang  -I/usr/local/Cellar/r/3.4.4/lib/R/include -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang++ -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -L/usr/local/Cellar/r/3.4.4/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.4.4/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.4.4/lib/R Done.   But I haven’t succeeded since it’s using the /usr/bin/javah as the header generator, resulting again as an error when building rJava. Perhaps I’m not doing it right or it’s something more I should do.   If you have any idea, you are more than welcomed. I’ve opened a question in stackoverflow and a issue in rJava.  ","categories": ["RStats","Technology"],
        "tags": ["config","error","homebrew","java","RStats"],
        "url": "https://luispuerto.net/blog/2018/03/28/r-and-java-10/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Java10RError.jpg"
},{
        "title": "MBP wakes up from sleep to back screen",
        "excerpt":"As you know, I have had problems with my old MacBook Pro Late 2011 and its dGPU which I’ve been able to patch to make the computer work again. In the beginning, the side effect of the fix was not to be able to wake up properly the Mac from sleep mode. The computer woke up to a black or grey screen and the fans started to work at full speed to finally turn off itself. This was cause because on the wake up process the computer decided to stuck to the faulty dGPU instead to the iGPU. Luckily, this problem was solve as you can see in the final fix.   However, my Mac still wakes up to a black screen from time to time. This isn’t a new behavior at all, just happen to come up more often. Perhaps, in the past, this problem happened once per month or two months or even once every six months.   Basically, the computer wakes up from sleep mode to a black screen that remains black no matter you do or what key you punch, while the computer seems to perfectly work in the dark. In this case, the fans don’t run at full speed and seems that the computer can stay like that for long. I haven’t tried, but I guess you can even log in remotely. If your Mac happen to have the iconic glowing apple in the lid you can put there a source of light —the flashlight of your smartphone for example— and you’ll probably be able to see the login screen —just the part around the Apple logo. Basically, what is going on in most of the cases, is the computer wakes up but it doesn’t turn on the blacklight of the screen for some reason.   This is a quite a common problem on MacBooks and if you search on internet about it you are going to find multiple solutions. Till now, none of them has really worked for me, but if you have this problem perhaps some of them are going to work for your, so why not to give it a try.   Reset SMC and PRAM/NVRAM   This is the most common solution you are going to find our there since perhaps the problem is related to those basic configurations. Yet, in my case I can’t do that —or I rather not— because the solution for disable the dGPU of my MPB is partly a setting on the NVRAM, so if I reset it I need to reapply the whole thing again, or at least that.   If you want to give it a try to can reset those settings with this directions:      SMC: shutdown, unplug everything except power, now hold leftShift + Ctrl + Opt/Alt + Power for about 10” and release at the same time.   PRAM/NVRAM: with the power cord on, power on and immediately later and before the chime hold cmd + Opt/Alt + P + R at the same time until you hear the chime for the second time.   Making it sleep again   Some people argue that the problem is related to the the lid itself that becomes a little bit faulty, either in the hinge or in the magnet, so it doesn’t send the proper signal to the computer to wake up when you open the lid. In in this camp I’ve seen like a couple of solutions.   Try to log in and make it sleep again   Since you computer seems to work properly but the screen if not receiving backlight, so it remains black, you can log in and command your computer to sleep again to reattempt to wake up properly this time. You can check if this is your case, if your lid’s Apple logo is not glowing chances are that the problem is just the backlight. You can put a source of light there —the flashlight of your smartphone, as suggested before— and see in you can spot the login screen. That means your computer is up and running perfectly.   Then, you can type your password, log in and after that push the power button for just a couple of seconds —no more or you are going to turn off the computer and we are trying to avoid that. That we’re trying to accomplish here is to make to show up the power / shut down menu in macOS. If we’ve succeeded we can hit the S key next and your computer should sleep again.                        macOS power / shut down menu              If your computer doesn’t go to sleep, don’t worry, mine either. I haven’t been able to make this solution yet a single time.   Push the power button and close the lid   When you see that your Mac doesn’t wake up in the correct way, one of the most obvious behaviors is close the lid again —like trying to make this a bad dream— and open again, to give another try. Also pushing buttons here and there, scape key —let me get out of this nightmare, I don’t want to lose my 150 pages document that I didn’t save before and I’ve been working the whole week when I sent the Mac to sleep— and the power button and go on and so for. That has worked for me in the past sometimes, not always. Sometimes the computer turned the screen on again and some others it just reboots.   Some people say that the correct way to do this just push the power button and close the lid, to open it again after a couple of seconds. But I haven’t try this yet. I don’t remember if the times I’ve succeeded to be back to life the computer I’ve followed that sequence by any chance. Who knows!!   Changing energy parameters   Other solution I’ve read about is just change the way your Mac sleeps. You Mac can be sent to sleep in two ways, simple sleep or Safe Sleep —the last one is/was called hibernation in windows— so macOS has three different setups for sleeping —only simple sleep, only safe sleep or both. You can see your config using the command:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $ sudo pmset -g custom   Battery Power:    lidwake              0    standbydelay         4200    standby              0    ttyskeepawake        1    hibernatemode        0    gpuswitch            1    hibernatefile        /var/vm/sleepimage    displaysleep         2    sleep                10    acwake               0    halfdim              1    sms                  1    lessbright           1    disksleep            10   AC Power:    lidwake              0    standbydelay         4200    standby              0    ttyskeepawake        1    hibernatemode        0    gpuswitch            1    hibernatefile        /var/vm/sleepimage    womp                 0    displaysleep         10    networkoversleep     0    sleep                0    acwake               0    halfdim              1    sms                  1    disksleep            10   The variable that holds this setting is hibernatemode and  0 is simple sleep, 3 is both and 25 is just safe sleep. In essence what is going on here is:      Simple Sleep — `hibernatemode 0`: Your computer keeps all the info in the RAM and stops all the rest of the computer. The RAM still has power and if you happen to run our of battery —after quite long time— you lose everything —not really, but let’s be catastrophical and think in the worse picture. However, you usually you don’t let your Mac sleeping and unplugged for more than a week, do you? I think the battery in this state can last more than a week, let be conservative here. This mode is the quickest, and usually your Mac is sleeping just after a couple of seconds you close the lid.   Both — hibenatemode 3: This is the standard mode and how most of the MacBooks behave. When you close the lid the Mac just normally sleep, but before it’s going to save its state in the sleepimage. If it doesn’t run out of battery, it usually wakes up normally, from the info in the RAM and you continue working as usually. If it runs out of battery it’s going to switch to safe sleep before it loses power. In this case when you open the lid you have to push the power button and your Mac is going to recover from the sleepimage. This is the safest… since it’s redundant, but it’s is slower than just sleep. This in modern MacBooks with SSD is call Standby Mode, and instead of waiting till the total lost of power between 1 h or 3h —depending on the year model— it goes to safe sleep.   Safe Sleep — hibernatemode 25: This is the hibernation mode in windows, in other words, the computer saves everything to the sleepimage and them disconnect power from everything. This mode is also really safe, perhaps safer than the previous one for some people and cases, but it’s the slowest. Usually the sleep process is the same than in the previous one, but it’s always going to wake up from the sleepimage, regardless of it has run out of battery or not. So it’s going to take more or less time depending of the capacity of your machine to read the sleepimage.   There is more info about the sleep modes here.                        Waking up from safe sleep              Some people argue that the back screen problem is related to the sleep image or the hibernatemode 3, so you have two options, change to hibernatemode 0 —the one I have right now— or to hibernatemode 25. If the problem is in the sleepimage the obvious candidate is the zero mode, but I’ve tried it and I’ve just woken up to a back screen. On the other hand,  while I was trying to fix the dGPU problem I had set the 25 mode for long, but sometimes it also woke up to the back screen. So this isn’t working for me.   To change the sleep behavior you can run the following commands   1 2 3 4 5 6 # Just sleep mode $ sudo pmset -a hibernationmode 0 # Sleep mode + safe sleep $ sudo pmset -a hibernationmode 3 # Just safe sleep $ sudo pmset -a hibernationmode 25   -a is for all —when you are on power ac and battery— but you can specify different settings. -b for battery and -c wall power.   If you decide to go for hibernatemode 0 you can also delete de sleepimage and sabe some space in your hard drive.   1 $ sudo rm -f /var/vm/sleepimage   The lidwake (on test)   Since some people say that the problem is related to the lid I’ve decided to change the way the computer wakes up. Instead of waking it up when I open the lid I going to wake up it pushing a key after opening the lid.   You can achieve this changing the lidwake parameter to 0:   1 $ sudo pmset -a lidwake 0   The normal value is 1.   Let’s see how this pan out and I’m able to stop the back screen wake up.   Final thoughts   The issue isn’t incredible problematic, even more right now that in modern system is really difficult to lose data because a force reboot. However, isn’t a normal behavior and is something that Apple should take a look to it. Seems that the problem extents across several generation of Macs and for some people has become more acute with High Sierra. Perhaps this is my case, but I don’t really know. I’ve installed High Sierra around beginning of November and in the beginning of December my dGPU crashed. So it’s difficult for me if this behavior has increased due to the crash, due to High Sierra or just because my Mac is old.  ","categories": ["Personal","Technology"],
        "tags": ["dGPU","high sierra","macOS"],
        "url": "https://luispuerto.net/blog/2018/04/13/mbp-wakes-up-from-sleep-to-back-screen/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/apple-wake-up.jpg"
},{
        "title": "R 3.5 \"Joy in Playing\"",
        "excerpt":"                     The joy is (in the) playing              Last Monday, 23rd of April, R was updated to its version 3.5, codenamed The Joy in Playing, which as the rest of the releases, make reference to a Peanuts’ cartoon.   Since it’s a minor release (3.x) and not just a patch, it’s advisable to reinstall all your packages, in some cases, to make then work properly. For example, and in my case, since I’ve built all of them as a consequence of my Homebrew install, some of them where throwing me the following message   1 Error: package ‘XXXXXXXXXXX’ was installed by an R version with different internals; it needs to be reinstalled for use with this R version   So, to reinstall all the packages that haven’t been build for your current R build, you can run the following command. (Be careful, rebuild all your packages could take time and resources, so it’s recommendable to do it at some moment that you aren’t using your machine)   1 update.packages(ask = F, repos = 'cloud.r-project.org', checkBuild = T)   I would recommend to run it twice, since some packages have dependencies and they need to be installed first and I don’t know if the command follows a specific installation order to avoid this errors like this —in other words, if the dependencies aren’t installed first to this specific release, the installation is going to fail. It doesn’t hurt to run the command again since if all the packages were rebuilt with the current R version they aren’t going to be reinstalled.   Java and rJava configuration   In some of my machines I hadn’t configured the new Java 10 with the prerelease rJava so Java 10 can be run properly in R. If this is your case remember to run:   1 $ R CMD javareconf   so you yield something similar to:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Java interpreter : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/bin/java Java version     : 10.0.1 Java home path   : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/bin/javac Java headers gen.: /usr/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/bin/jar  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang  -I\"/usr/local/Cellar/r/3.5.0/lib/R/include\" -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/Cellar/r/3.5.0/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.5.0/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.5.0/lib/R Done.   So you can install / build rJava prerelease with the following command   1 install.packages('rJava', repos = 'http://rforge.net')   devEMF   In another machine I wasn’t being able to install devEMF package. You can see the specific error I was getting in this Stack Overflow question.   The problem was the makevars file, which is crafted to use the LLVM. I commented all of the lines to build the package and all set. Seems that for some reason LLVM isn’t supported to build this package in this version of R (or it isn’t supported at all).   1 2 3 4 5 6 7 8 # Remove the comment on -fopenmp for compiling data.table packag`akevars\"&gt;# Remove the comment on -fopenmp for compiling data.table packagakevars\"&gt;# Remove the comment on -fopenmp for compiling data.table packag`e # CC=/usr/local/opt/llvm/bin/clang #-fopenmp # CXX=/usr/local/opt/llvm/bin/clang++ #-fopenmp # -O3 should be faster than -O2 (default) level optimisation .. # CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe # CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe # LDFLAGS=-L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib # CPPFLAGS=-I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   Remember to uncomment them after you finish the build.   1 2 3 4 5 6 7 8 # Remove the comment on -fopenmp for compiling data.table package CC=/usr/local/opt/llvm/bin/clang #-fopenmp CXX=/usr/local/opt/llvm/bin/clang++ #-fopenmp # -O3 should be faster than -O2 (default) level optimisation .. CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe LDFLAGS=-L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib CPPFLAGS=-I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   ## data.table   Don’t forget that data.table package has also specific makevars necessities if you are building it, as you should, with LLVM. Remember that the flag -fopenmp has to be present / uncommented in the lines related to C and C++ compilers.   1 2 CC=/usr/local/opt/llvm/bin/clang -fopenmp CXX=/usr/local/opt/llvm/bin/clang++ -fopenmp   Remember to re-comment or delete the -fopenmp after you build data.table.  ","categories": ["Professional","RStats","Technology"],
        "tags": ["homebrew","how to","RSoft","RStats"],
        "url": "https://luispuerto.net/blog/2018/04/28/r-3-5-joy-in-playing/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/4421de17ce2dc4cd3843ba00b224fbe0-header.jpeg"
},{
        "title": "The ciabatta recipe",
        "excerpt":"Lately I’ve been extending my baking skills and I’ve been practicing the ciabatta recipe from Iban Yarza’s book, Pan Casero, which I think is delicious. I say extending my baking skills because it’s the first time I’ve tried a preferment recipe and more specifically poolish. Poolish have zero complications in comparison with the straight doughs, but it’s true that in general you need extra time and the final dough is more sticky since it’s more moisturized. So you are going to need to have some experience working with dough if you don’t want to end up full of dough and even, depending how brave you are, your kitchen.   What you are going to need for this recipe is the same as with the previous ones, plus a tray or container to handle the dough after we finish the kneading. I recommend to review the preliminaries for baking post for a full description of the methods. Take into account that the amount of yeast is specify for 12, 8 and 3 hours for the bulk fermentation of the poolish. On top of that you are going to need around 2 hours of mixing — kneading — proofing ** after the polish is ready, plus the usually **40-50’ of baking.   Ingredients   For the poolish      Flour: 450 gr (60%).   Water: 450 gr (60%).   Yeast: 1 to 7 gr of fresh yeast (see below) depending on how much we want to leave the poolish resting — fermenting.            0.9 gr for 12 hour fermentation (0.12%).       3 gr for 8 hour fermentation (0.4%).       7 gr for 3 hours fermentation (0.93%).           For the final dough      Poolish: 900 gr (60%).   Flour: 300 gr of flour (100% = 60% from poolish + 40% from the final dough)   Water: 120 gr of water (76% = 60% from poolish + 16% from the final dough)   Salt: 15 gr of salt (2%)   As you probably have noted the proportions are a little bit weir, this is because, as always, they are the percentage of the full amount of flour. So, if we want to end up with a 750 gr. of bread —in this case two loaves of ~375 gr.— you need the 60% of flour for the poolish and the same amount of water. From there on, you have to complete the quantities for the final dough.   Directions      You measure 450 gr. of flour and put into a bowl at least the double and preferably the triple of that volume.   Measure 450 gr. of warm water (37 ºC) and mix it with the yeast. When dissolved, pour into the flour and mix it, with a spoon if you want, until homogenous.   When the mixture has double the volume (in 3, 8 or 12 hours) pour the 300 gr. flour, 120 gr. of water and the 15 gr. of salt in the poolish and mix everything with your hands using the pincer method. Be aware that the dough is really sticky, so have at hand a spoon to clean your hand when you are done.   Now you have two options, you can take the dough out of the bowl and use the French kneading method for 10’ or you can take the easy way and practice the folding like 5-6 times every 10’ for the next 30’. The aim here is to develop the gluten so the dough doesn’t just flat out in the oven tray and it’s able to rise. If you don’t have experience with the French kneading I just recommend you to leave the dough in the bowl and practice the folding. A great way to avoid the dough to sticks to your hand is set it like a paddle —put your fingers tight together— and dip it in cold water before you begin to fold every 10’. I usually go a little bit the extra mile and I do the first folding repetition extra long and fold the dough for about 5’ minutes.   After you’ve developed the gluten you take out the dough from the bowl and set in on a tray where you’ve spread olive oil so the dough doesn’t stick to it. There you fold it like a small package using the 4 sides and turn it over with the seam downwards for 30’.   After 30’ you repeat the operation, fold again like a small package using the 4 sides and turn it over, but this time let it rest for an hour.                        Poolish fermenting and dough resting              At this point you could think to preheat the oven to 250 ºC so it’s ready after an hour. Remember to put a container with water inside so there is humidity when you introduce the bread. Preheat the oven tray too.      After an hour you take the dough out of the tray / container —genteelly so you don’t ruin the air pockets— and put in the counter where you have sprinkle some flour. You can cut the dough in half longwise so you end up with a couple of strips of dough that are like a small mattresses.   Now, you take the tray out of the oven and put parchment paper over it —without burn yourself— and after that you put the two stripes of dough.   Bake them in the oven for about 20’ and remove the water.   You can reduce a little bit the temperature now to around 230 ºC and bake for another 10’.                        Loaves in the oven                 After 10’ I usually turn around the loaves so they get well baked on the bottom and bake for another 10’.   Finally, turn off the oven and open the door a little bit leaving the bread there for an additional 10’.   Take the loaves out and let them rest over the rack for at least half an hour.   Enjoy your ciabatta.  ","categories": ["Personal","Recipes"],
        "tags": ["bread","how to"],
        "url": "https://luispuerto.net/blog/2018/04/29/the-ciabatta-recipe/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/IMG_5302.jpg"
},{
        "title": "MBP wakes up from sleep to back screen —  Solution",
        "excerpt":"A couple of weeks ago I wrote about the problem I have with my old MacBook Pro wakening from sleeping to a black screen. I also pointed out that I was testing a solution related to the lid. I can tell that since I’ve testing this solution I haven’t suffered any wake up to a black screen, so I guess that the problem is related to the lid, either to the hinge or to the magnet that trigger the wake up and the sleep processes.   I’m doing two things to prevent the black screen behavior. First I’m sleeping the computer using the command line:   1 $ sudo shutdown -s now   I’ve also change the hibernation mode of the machine to zero, in other words, it doesn’t save the state in a file in the hard drive. I’ve also deleted that file from my hard drive.   1 $ sudo rm -f /var/vm/sleepimage   Finally I’ve disable the options to wake up the computer when you connect to power source and when you open the lid.   1 $ sudo pmset -a lidwake 0   So, now to wake up the computer I have to hit any key on the computer keyboard or the trackpad after I’ve open the lid.   So far it has been working great.  ","categories": ["Technology"],
        "tags": ["dGPU","high sierra","macOS"],
        "url": "https://luispuerto.net/blog/2018/05/02/mbp-wakes-up-from-sleep-to-back-screen-solution/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/apple-wake-up.jpg"
},{
        "title": "Homebrew's R doesn't have all the capabilities",
        "excerpt":"A couple of days ago I just found out that when you install R with Homebrew you don’t get all the capabilities that the binary from CRAN have. In other words, you have a kind of second class install, in some regards, and depending on how you do install and for what you are going to use R.   1 2 3 4 5 6 7 8  &gt;capabilities()         jpeg         png        tiff       tcltk         X11        aqua       FALSE       FALSE       FALSE       FALSE       FALSE        TRUE    http/ftp     sockets      libxml        fifo      cledit       iconv        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE         NLS     profmem       cairo         ICU long.double     libcurl        TRUE        TRUE       FALSE        TRUE        TRUE        TRUE  1 2 3 4 5 6 7 &gt;capabilities()        jpeg         png        tiff       tcltk         X11        aqua        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE    http/ftp     sockets      libxml        fifo      cledit       iconv        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE         NLS     profmem       cairo         ICU long.double     libcurl        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE   How did I notice? Well basically I was about to update the sm package in one of my computers and R yield an error similar to this one (sorry I didn’t copy the original one):    1 2 3 4 5 6 Error: package or namespace load failed for ‘tcltk’:  .onLoad failed in loadNamespace() for 'tcltk', details:   call: fun(libname, pkgname)   error: Tcl/Tk support is not available on this system In addition: Warning message: S3 methods ‘as.character.tclObj’, ‘as.character.tclVar’, ‘as.double.tclObj’, ‘as.integer.tclObj’, ‘as.logical.tclObj’, ‘as.raw.tclObj’, ‘print.tclObj’, ‘[[.tclArray’, ‘[[&lt;-.tclArray’, ‘$.tclArray’, ‘$&lt;-.tclArray’, ‘names.tclArray’, ‘names&lt;-.tclArray’, ‘length.tclArray’, ‘length&lt;-.tclArray’, ‘tclObj.tclVar’, ‘tclObj&lt;-.tclVar’, ‘tclvalue.default’, ‘tclvalue.tclObj’, ‘tclvalue.tclVar’, ‘tclvalue&lt;-.default’, ‘tclvalue&lt;-.tclVar’, ‘close.tkProgressBar’ were declared in NAMESPACE but not found   As you can see I’ve highlighted the key line, R doesn’t have tcltk available and running in that system and if you run capabilities() in the R console you’ll probably get something similar to the first code-block. After researching a little bit about the problem [1 &amp; 2], I found out what I’ve already said, Homebrew R isn’t build with those capabilities. Why? Well…      It seems that those capabilities are optional when you build R from source and build R with those capabilities / options on Homebrew’s bottle server is “error prone”.   To have some of the missing capabilities you must have in your system X11/XQuartz, which isn’t installed in every Mac, because it’s not longer provided as macOS basic installation.   You can install X11/XQuartz, but you have to do it with Homebrew Cask, since it can’t be build from source. As a consequence is a off project dependency and they don’t want to rely on that.   Homebrew seems to be heading to a option-less direction where the formulas aren’t going to have any option at all. So if you want to have options or different kind of build they encourage you to have your own tap.   Whether you agree or not, you have to understand that the maintainers of Homebrew-core took this direction for a reason, which is probably they have already too much work to do. Don’t forget they maintain the package manager for free. If you disagree, as I do, at least partly, please be polite. A lot of users have been expressing their disagreement in really bad manners lately and that is not acceptable.   Lucky for us users, there is a solution to the problem. You can always create your own tap of Homebrew and tweak the formula to fit your needs, as maintainers suggest. As a result someone already did that and there is already a tap which you can install R with the same formula we installed R when it was in Homebrew Science, but up to day.   I’ll explain in my next post how to install R using that tap.  ","categories": ["Professional","RStats","Technology"],
        "tags": ["homebrew","RSoft","RStats"],
        "url": "https://luispuerto.net/blog/2018/05/11/homebrews-r-doesnt-have-all-the-capabilities/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/R-Homebrew.jpg"
},{
        "title": "Installing R with Homebrew with all the capabilities",
        "excerpt":"As I explained in my previous post if you installed R with Homebrew you have less capabilities than with a R installed from CRAN’s binary. But, can you have all the capabilities while you still use Homebrew to install R? Yes! However… why you bother to install it with Homebrew at all instead of installing it from CRAN. Well, it’s true that CRAN install it’s easier. You just have to download the binary and that’s it. You can even use Homebrew Cask to install R that way   1 $ brew cask install r-app   However, if you install that way you can’t take advantage of OpenBlas and OpenMP which really enhance the speed of R processing. Well, you can take advantage of OpenMP with CRAN install if you use the coatless professor method.   Getting all the capabilities   To make this possible we are going to install/reinstall R, and Cairo, with the formulae Seth Fore has in his tap, sethrfore/homebrew-r-srf. These formulae are basically the same on brewsci/homebrew-science, which is a legacy tap (no longer updated), which we all were using to install R with Homebrew before they merge everything to Core. Seth updated both formulae for us, so we can enjoy the last version of R and Cairo while not loosing any capability.   You also have to remember that this instructions are just aimed to reinstall R if you have follow my previous instructions to have a 100% R install with Homebrew. And they would replace this section.   The first thing you have to do is uninstall R and Cairo if you have them installed:   1 2 $ brew uninstall R $ brew uninstall --ignore-dependencies cairo   If you have Cairo installed, it’s going to protest about dependencies, but don’t worry and we are just going to reinstall it in a minute.   When you don’t have R and Cairo in your system you can go ahead.   You begin tapping Seth’s tap.   1 $ brew tap sethrfore/r-srf   Actually, you can avoid this step since Seth’s formulae have the same name as Homebrew Core’s one we are forced to install them using the full name of the tap in combination with the formula name.   If you don’t have Xquartz already installed in your system you can install with:   1 $ brew cask install xquartz   Now, you can install Cairo.   1 $ brew install sethrfore/r-srf/cairo   When Cairo finish to build you can proceed with R.   1 $ brew install sethrfore/r-srf/r --with-openblas --with-java --with-cairo --with-libtiff --with-pango   To use the Pango flag --with-pango you must have installed in your system Pango brew install pango.   When it end to build, you can use use capabilities() in R console and you have to get something like this:   1 2 3 4 5 6 7 &gt;capabilities()        jpeg         png        tiff       tcltk         X11        aqua        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE    http/ftp     sockets      libxml        fifo      cledit       iconv        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE         NLS     profmem       cairo         ICU long.double     libcurl        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE   To finish I would run:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ R CMD javareconf Java interpreter : /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/bin/java Java version     : 10.0.2 Java home path   : /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home Java compiler    : /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/bin/javac Java headers gen.: /usr/bin/javah Java archive tool: /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/bin/jar  trying to compile and link a JNI program detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm /usr/local/opt/llvm/bin/clang  -I\"/usr/local/Cellar/r/3.5.1/lib/R/include\" -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/include/darwin  -I/usr/local/opt/gettext/include -I/usr/local/opt/llvm/include   -fPIC  -g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe -c conftest.c -o conftest.o /usr/local/opt/llvm/bin/clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/Cellar/r/3.5.1/lib/R/lib -L/usr/local/opt/gettext/lib -L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/lib/server -ljvm -L/usr/local/Cellar/r/3.5.1/lib/R/lib -lR -lintl -Wl,-framework -Wl,CoreFoundation ld: warning: text-based stub file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation.tbd and library file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation are out of sync. Falling back to library file for linking.   JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home Java library path: $(JAVA_HOME)/lib/server JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm Updating Java configuration in /usr/local/Cellar/r/3.5.1/lib/R Done.   To reconfigure Java on R, just in case.  ","categories": ["Professional","RStats","Technology"],
        "tags": ["homebrew","how to","RSoft","RStats"],
        "url": "https://luispuerto.net/blog/2018/05/11/installing-r-with-homebrew-with-all-the-capabilities/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/R-Homebrew.jpg"
},{
        "title": "16 Personalities: Apparently I'm an Entrepreneur",
        "excerpt":"Not long ago, while I was taking a course in Finland, they asked us to take a personality test online called 16 personalities. It wasn’t nothing really official or for much purpose of scrutinize us, but just to know ourselves a little bit more and we can work our strengths and weakness. To my surprise, the test told me that I have an entrepreneurial personality. I never thought about myself in that regard, and in the beginning I thought that it was a lot of bulls**t, like most the so-call psychological test. However, after reading more or less thoroughly over the description I found that I have a lot in common with an entrepreneurial personality. Or at least, my subconscious persona wants to be an entrepreneur.   After all, who don’t want to be an entrepreneur these days and start dozen and dozen of companies? be in command, executive and decided? produce something new and exciting everyday, fill with technology with all the bells and whistles? Be part of that breed of people always in the verge of society… When we think in entrepreneurs it comes to our mind people like, Steve Jobs, Sergey Bin and Larry Page, Henry Ford, and dozen of other people that begin to build something in their house’s —or parent’s— garage.   It’s funny, because I just finish reading Rework and Remote, where the authors talk about the term entrepreneur as “an outdated and loaded with baggage” one. Some kind of a “members-only club”. They argue that there are a lot of people out there starting new business, who aren’t calling themselves entrepreneurs because they are just doing what they love. So they advocate for a new term… let’s just call them starters. And I kind of agree with them that the term entrepreneur is a term stale and overused, which a lot of people use with pride trying distance from others. Like if they had some kind of pedigree. Something I don’t like at all.   But let’s return to the personality test, as they explain in the Our Theory / Our Framework section of the web, they are using the Myers—Briggs Type Indicator —which is an evolution of Carl Jung’s Phycological Types— with the addition of the Big Five personality traits model as an analytical base. They also, of course, add algorithms, models and theories from their own vintage. I’m not a psychologist, a sociologist, or any of the sort, so I can’t, by any means, build an educated critic or something similar. I just can tell you that while reading the profile I was indeed feeling that some parts were related to me, or described myself in some way, but in others it was totally not. Also, I noticed, that while not totally true or untrue, some of the descriptions were appealing, in the sense that if I’m not like that, I would like to be.                        General idea of what it seems I’m              If you check the introduction of my profile, they say that I’m an entrepreneur with a role of an explorer and with a strategy of people mastery. It’s funny because I’ve always considered myself an explorer, I always enjoy wondering, and wandering, discovering new places and new people and trying new things. In fact, lately I’m consider the word try by favorite word. Perhaps for that reason I’ve always loved maps —they help you reach new places and imagine. However, I never considered myself a social person or people master. When I was a child I was always really shy, buy my mother always encourage talk to people and be myself. She, on the other hand, has been always a really sociable person and really outspoken, even when she was a child. Perhaps, that is something I have inside of me.   They also consider that I’m have a 78% extraverted, 58% observant, 58% thinking, 51% prospecting and 76% assertive personality. As you see, I only have two trails really clearly sided, while in the other three I’m quite balance. These should be translated as that I’m clearly extraverted, so “I usually prefer to be group activities”, and clearly assertive, in other words, I’m self-assured, even-tempered and resistant to stress, and I usually don’t worry too much, but I don’t puss myself too much when I have to archive goals. Both of those trails are true or at least I feel they are true.   In relation to the other three. I’m mostly observant (58%), which means I’m really practical, pragmatical and down-to-earch person, focussing in what is happening or has happened. Yet, since I’m not clearly observant I’m also somehow an intuitive (42%) person really interested in ideas and novelty. I’m mainly a thinking individual (53%), in other words, I focus on objectivity and rationality, prioritizing logic over emotions. However, I’m also a feeling being (47%) and I listen to my feelings and like to share with others, preferring cooperation over competition. Finally, I’m a prospecting person (52%) so I’m good at improvising and spotting opportunities, tending to be a flexible, relaxed nonconformists person who prefer keep my options open. Nevertheless, it also means I have a judging (49%) personality and in consequence I’m good planner, like clarity and I have a strong work ethic.   Of course things are a little bit more complicated than that and I plan to explain then a little bit further in the following days in a couple of posts. Why? Because I’ve always believed that openness, sharing who you are with others and be ready for assessment, make you always a better person. I’m always ready to upgrade to a better version of myself and I believe that I’m always doing so. I think it could feel into the definition of Peter Senge, Personal Mastery1. Nonetheless, I don’t want to take this kind of test too seriously since all of them are usually fundamentally flawed. You are the one judging yourself —which is clearly biased and has it drawbacks— and sometimes the language is something like the horoscope, always fit you no matter who reads it —Oh! I’m this, and this too, and of course this one.                  Personal Mastery is define by Senge as: the discipline of continually clarifying and deepening our personal vision, of focusing our energies, of developing patience, and of seeing reality objectively. From The Fifth Discipline. &#8617;           ","categories": ["Personal","Professional"],
        "tags": ["my personality","science","work"],
        "url": "https://luispuerto.net/blog/2018/05/21/16-personalities-apparently-im-an-entrepreneur/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/Luis-Puerto-The-Entreprenour2.jpg"
},{
        "title": "New Web in Jekyll",
        "excerpt":"As you probably have noticed, some days ago I rolled out a new version of my website, which you probably think it’s pretty similar to the previous one, at least talking about its external appearance. However, it’s really different in the inside, since it’s a static web build with Jekyll, while the previous one was build on Wordpress. This isn’t a change that occurred to me just out of the blue and some moths ago I made a post about Jekyll where I stated my desire to change to Jekyll, or at least to try,  for several important reasons, being the main ones cost and simplicity.   Although building a simple site in Jekyll it’s really straightforward and simple, make a transition from Wordpress to Jekyll has more “curves” on the way than you would expect. Even more when I was trying to mimic my original website as much as possible. I used a Wordpress to Jekyll exporter and although the export was smooth, some of the post had the format messed up and I had to edit them manually. It took me a solid month to have everything more or less ready in a decent way.                        luispuerto.net at 23rd July 2018              Right now, the site is up and running —as you can see— but there are some missing features. I have a TODO list where I’m trying to track down all those features I want to implement and the pending changes I want to make. Some are improvements from the previous version but most of them are missing features that are not implemented in Jekyll right away or they are really basic as you take your Jekyll site out of the box. These shortcomings are usually related to the fact I’m hosting the web in GitHub Pages, which is free, but you have some missing features since they build your page in --safe mode. So, you can’t use custom plugins1.   Yet Jekyll provides an archive, categories and tags out of the box, those features aren’t as nice as those provided by a dynamic site. For instance, you need to create yourself the those pages —which isn’t really a problem— but then you need to create manually the pages for each category and tag a page you want to show. However, if you use a plugin like Jekyll Archives all the process is much nicer and the plugin build the all the tag and category pages for you. As you can see right now, I don’t even have an archive page and I’m relying in a paginated chronological archive, what, at this moment, is more than enough.   There are a couple of ways to overcome the shortcomings of GitHub pages if you really want to use custom plugins. The first and most obvious one is to build your site locally every time you make a change and then you push it _site folder to GitHub. From there on, you can go the extra mile and make everything automatically in the server side, whether you use a Continuous Integration service or you use somethings more complex like Netlify or others2. I’ll probably try to implement the Travis CI method and then research other more sophisticated ways to deploy.   I’m taking this as a way of learning about Jekyll platform —liquid— html, css and a little bit of other things like javascript and ruby —ruby looks like a really useful language that I would love to learn in the future. Even I improve my Git skills since to publish in Jekyll you need to push. I wanted to roll out the site before it was fully finished because I find interesting to share how I finish to build the site. I really think we live in a society where we are too fixated on the final product but usually we don’t share how we arrive to it, something that sometimes it’s even more important. This particular idea is one of the characteristics I like the most from Git and GitHub, and all other repo hostings. You can share all the changes with others and understand how and why to reached the final “product”. I hope you enjoy also “the way”, most of the times as satisfying as “the end”.                  You can see a list of the allowed plugins here. &#8617;                  There are several other services besides the ones explained in the Jekyll documentation. If you research a little bit you are going to find several more options. Jenkins seems also to be able to build Jekyll. &#8617;           ","categories": ["Personal","Technology"],
        "tags": ["jekyll","blogging","coding","markdown","static web"],
        "url": "https://luispuerto.net/blog/2018/07/23/new-web-in-jekyll/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/jekyll.png"
},{
        "title": "De-Facebooking",
        "excerpt":"Today I was finally able to empty my Facebook :toilet: account. I totally deleted all my content, or at least the one I was able to delete, since not all the content is deletable. For example, you can’t delete the events you’ve been interested in, invited, or you have attended, or you can’t delete some of the pages you liked, because they aren’t available anymore. It’s quite interesting the amount of data some of us have poured into that social network. If you have an account in Facebook :toilet: I invite you to go into Download Your Data section, download it, and peek into what Facebook :toilet: has about you. And I didn’t say knows because it probably knows more that all that data you are able to download and they probably have a more detailed profile of you in their end.   Why?   Well, as I’ve already explained —more or less— in a previous post, Facebook :toilet: has stopped to feel right as a place to share content. The problem isn’t really privacy or anything like that —although is an importan issue. It’s just that doesn’t fit in what I want to share with the World :earth_africa: right now. It just feels that falls short when I want to share the content I want to share now —rich, complex and more elaborated.   There is also another reason… I don’t want a third party to own my content. Or at leas I don’t want a third party owning my content that makes really bloody difficult to delete or manage my content. I took me a couple of days to delete all my content from Facebook :toilet: —and the help of a couple1 of scripts/Chrome-extentions—  because they don’t have any option or section in their site to proper manage your content. You just either can download your content or delete your account completely —or they say they do— and of course, delete post by post, like by like and comment by comment. Which is incredible painful.   I guess they don’t want to make things easy for users because our content is their most precious treasure. Facebook :toilet: has its value because users, like you and me, have been pouring content about us day after day for the last years —a decade in some cases. I signed up in Facebook :toilet: 10 years ago (yeah, I’m really that old). All of that content is really gold :moneybag: because it’s useful to make profiles of people —anonymous or not— so they can sell products, services and maybe political ideas to you. Perhaps the latter isn’t totally true, but I assure you that it’s the the wet dream of more than one politician.      So… why you don’t just delete your account and that’s it?    The sort answer is, just because I can I want to mess around with Facebook. The real and long one is, Facebook :toilet: still has some value for me —a residual one, but value. Through Facebook :toilet: I can still be in contact with you, or at least you can contact me, and I’m not talking about Facebook :toilet: Messenger only. There are still a lot of people that use Facebook :toilet: to be in contact with their love ones —family, friends and that weir guy they met somewhere else. That’s —more or less— my case —specially the latter one :stuck_out_tongue_closed_eyes:. Most of my friends still use Facebook :toilet: in their daily lives, they share part of their lives there and know about others lives there. This is usually specially the case when you have more or less an international life and you travel and have been living in different places at different parts of your life. Facebook :toilet: make it easy to be in contact with people that is really far far away from you, even when you just log-ins once per week.   How?   If you want to know how I delete the content of my Facebook :toilet: account, it’s really easy. I just installed on my Chrome browser the Delete All Facebook Post extension and then I begin to wipe posts and likes year by year. I have to use it more than a couple of times each year —sometimes when things were really stuck, i.e. I had too much activity during a period of time, I had to pass it like 5 times and sometimes I used it on specific months with a lot of content at lower speed.                        Facebook :toilet: Activity Log              Then, I reviewed manually the activity log and untag me from photos and posts, and delete posts and unlike post that the extension missed. One tool that can be a little bit helpful from Facebook :toilet: side is this “Manage post” button that you can find in your profile.                        Manage posts interface              There, you select post in bulk/batch and delete them that way. But, be careful, because not all the post can be delete that way and if you make a batch selection and one of the posts can’t be delete through this tool —and you have to go to the post— it isn’t going to allow you to delete all the others. So, try to select one by one and if the options at the bottom of the page change deselect the last one.   What now?   My intention is to backpedal a little bit my social media and specially Facebook :toilet:. I’ll stop to share stuff in my Facebook​ :toilet: profile but I’ve created a page in Facebook :toilet: were I’m going to share the posts I write here. The page interface it’s much much more friendly and easier to manage the content. You just have to select all and click on delete to have again a clean slate. In fact, I just did it before I wrote this post because since I changed the url of this website and the url of the posts, the entries there didn’t have any sense. So I have to repost all of them with a backdate —the one they where publish.   I’m going to continue to interact on Twitter :bird: —although I know Twitter is also far from being perfect and I don’t really like the direction it’s heading— and probably in Instagram —and storing those photos in my Tumblr so they are really mine. I also want to return to the more quiet and static Flickr where I want to post, little by little the photos I wan to share with you.   Botom Line   The latest Facebook :toilet: events, not only the Cambridge Analytica ones, but the recent data breach from Facebook :toilet:2, in addition to John Oliver report about Facebook :toilet: , which I really encourage you to watch, made me to finally delete my data —and I asure you that’s something I really wanted to do for long time. Perhaps, you’ve noticed that I haven’t post in Facebook :toilet: for a while —sorry if you can’t check my post history now :joy:— or perhaps you don’t, but my dissatisfaction with Facebook :toilet: isn’t something new. It has to do more with who and how owns the my data than with privacy issues and with the fact that I consider Facebook :toilet: not the right place to share, learn and debate about ideas because it’s a toilet.                  Social Book Post Manager &amp; Delete All Facebook Post. Specially the latter one. Anyhow, I have to pass the scripts several times and usually the best results are yield when you go year by year. In the end, you have to delete by hand of the posts because they need more than hot water to get rid of them :fire:. &#8617;                  You can check if your account has been affected here. &#8617;           ","categories": ["Personal"],
        "tags": ["facebook","blogs","social media","privacy"],
        "url": "https://luispuerto.net/blog/2018/10/18/de-facebooking/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/facebooktoilet.jpg"
},{
        "title": "Testing the Dosdude1 patch to disable the dGPU on macOS",
        "excerpt":"As you know, I have a problem on my vintage MacBook Pro late 2011 related to my dGPU. Basically it died :skull: at the end of 2017. So I have to find a patch to be able to continue use it, and I finally did in this way.   Yesterday, Apple released a security update that seems that in some parts is related to the graphic interface of the system, so I definitely was going to reapply the patch after the update —as after every update I usually do. However, recently I discovered dosdude1 and his page where I found out that he has a patch to disable the dGPU on macs like mine. I thought it was a great opportunity to test the patch, since it seems more convenient.   What does the patch do?   I think the best thing is to copy verbatim from the patch itself:      This program will disable the dedicated GPU on 15” or 17” MacBook Pro System that have dual video cards installed. After this program has finished running, the following things will be done:          An NVRAM variable will be set that prevents the machine from using its dedicated video card.     The video acceleration drivers (kernel extensions) for the respective installed dedicated video card will be removed from the /System/Library/Extensions directory, and backed to up to the root of the hard disk     A LaunchDaemon will be installed that prevents the video cards drivers from being re-installed during Software Updates, and endures the necessary NVRAM variables is set correctly if PRAM is reset.      Installing the update   The fist thing I did was install the update. I didn’t even follow any of the steps I described here —I should have— but I just installed the update.   I encounter problems during the update installation. And at some point I have to force quit the computer —pressing the turn-on/off button till the computer shut off— and upon startup press cmd+s and enter the following in the prompt:   1 2 $ nvram fa4ce28d-b62f-4c99-9cc3-6815686e30f9:gpu-power-prefs=%01%00%00%00 $ reboot   After that the update install continue and I was finally able to access to the desktop.   Undoing previous changes   Since I wanted to apply to Dosdude1 patch, I thought that it would be wise to undo some of the changes I’ve done in the system go disable the dGPU. There are two main changes I though I should reverted:           Disable the Loginghook we created during the fix —steps 6 to 8:       1 $ sudo defaults delete com.apple.loginwindow LoginHook                Restore the AMDRadeonX3000.kext that we moved during the fix — step 4:       1 $ sudo cp -p /System/Library/Extensions-off/AMDRadeonX3000.kext /System/Library/Extensions/           Applying the patch   Well, this is pretty straightforward, isn’t it? You just click on the app and follow the instructions. Take into account that you have to reboot at the end.   Outcome   I have to say that the outcome was positive and the computer worked well. However, there was a minor problem that for me was a deal breaker, the computer didn’t woke up from sleep. I tried several times and never ever was able to wake up. I always got a black screen.   I think the reason for this is pretty simple, what the patch is doing is just wiping all the kext from /System/Library/Extensions/. Nevertheless, you still need some of them to operate all the functions of the computer and to —I think— really disable the dGPU. I asked about these two problems to @dosdude1, as you can see bellow. You can see there too that the temperature of the “GPU Diode” is still high, which mean the dGPU is still active. I haven’t receive any reply at the moment of writing this post.   @dosdude1 Hey... I have a question. I&#39;ve just apply your patch to disable dGPU https://t.co/xUZakry4d7 After it&#39;s applied, shouldn&#39;t the GPU Diode plumb down like in the image at the bottom of this post? https://t.co/EM2SBZKSLG  Thanks for the tool! pic.twitter.com/BKzOJkbliJ &mdash; Luis Puerto (@lpuerto) October 31, 2018   I can&#39;t wake up from sleep... and I can&#39;t find the backup files on the root of the hard drive after using your tool &mdash; Luis Puerto (@lpuerto) October 31, 2018   As you can see in the thread, at that moment I already wanted to revert changes. Dosdude1 doesn’t provide with a tool to revert changes, or with instructions to do so. The major problem I had at that moment was, I couldn’t find the backup kext files in the root of my hard drive. I later found out that they are in a hidden folder names .AMD_Backup. So, I had to pull the backup files from my Time Capsule and apply the security update again —just in case— to reapply my patch to disable the dGPU.   How to roll back changes   If you want to roll back changes you have to   Delete the LaunchDaemon      A LaunchDaemon will be installed that prevents the video cards drivers from being re-installed during Software Updates, and endures the necessary NVRAM variables is set correctly if PRAM is reset.    If you don’t delete this LaunchDaemon your kext are going to be delete after you restore them next time you re/boot. So to delete it you have to run the following command:   1 $ sudo rm -r /Library/LaunchDaemons/com.dosdude1.GPUDisableHelper.plist   Restore the kext      The video acceleration drivers (kernel extensions) for the respective installed dedicated video card will be removed from the /System/Library/Extensions directory, and backed to up to the root of the hard disk    Now, you can restore your kext   1 $ sudo cp -p /.AMD_Backup/*.* /System/Library/Extensions/   I would also check the ownership —just in case:   1 $ sudo chown -R root:wheel /System/Library/Extensions/AMD*.*   Reapply the previous patch   After that, you can reapply the previous patch like it’s explained here.   Bottom Line   I wouldn’t say the Dosdude1 patch is bad, but in my case didn’t work as it should, since the wake up didn’t work. I really don’t know if this was due to my previous setup or configuration or if I did miss something on the way, but for me not be able to wake up from sleep isn’t operate normally.   I also think that the patch should be a little bit more documented. There isn’t specific instructions to roll back changes or a tool to uninstall automatically. There isn’t available, or at least I don’t know where it is, the source code of the tool, so you can’t really know what is really going on. It isn’t I don’t trust the dude, but transparency is always appreciated.   Finally, I think that the reason the computer doesn’t wake up is you can’t delete all the kext. The purpose of the patch I’ve shared —I didn’t created it— if to disable just de dGPU, but still use some of the functions the  kext provide, and for that reason the problematic kext isn’t deleted but moved to another folder, so it isn’t loaded on boot, to later load it manually1. The problem isn’t just the AMDRadeonX3000.kext, but you need to have all the others to really make it work.   1 2 3 4 5 $ kextstat | grep AMD   114    2 0xffffff7f82d1b000 0x122000   0x122000   com.apple.kext.AMDLegacySupport (1.6.8) 69C5152C-0305-3914-AD56-6601DD449AF4 &lt;103 12 11 7 5 4 3 1&gt;   133    0 0xffffff7f835f0000 0x12e000   0x12e000   com.apple.kext.AMD6000Controller (1.6.8) F08FE763-26A1-312E-B690-CB8FDBF8EC31 &lt;114 103 12 11 5 4 3 1&gt;   144    0 0xffffff7f830c3000 0x22000    0x22000    com.apple.kext.AMDLegacyFramebuffer (1.6.8) 13E3BF67-6700-37F0-82EE-E87F8B71A033 &lt;114 103 12 11 7 5 4 3 1&gt;   169    0 0xffffff7f83a6b000 0x568000   0x568000   com.apple.kext.AMDRadeonX3000 (1.6.8) 8559CEFE-85B8-3FB7-B20A-9346E5A7986A &lt;168 145 103 12 7 5 4 3 1&gt;   For these reasons I still stick to my solution.                  This, as far as I know, also make it possible to total disabling of the dGPU. &#8617;           ","categories": ["Personal","Technology"],
        "tags": ["dGPU","graphic card","macOS","high sierra","how to"],
        "url": "https://luispuerto.net/blog/2018/11/01/testing-the-dosdude1-patch-to-disable-the-dGPU-on-macOS/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/grey_screen.jpg"
},{
        "title": "Cutting ✂️ down the size of your repo",
        "excerpt":"Lately, I’ve been playing with Git and trying to fix a couple of size problem with more or less luck. Git, is a wonderful tool to track and follow changes in almost any kind of document/s or project, specially is they are plain text documents —or sort of— like any kind of source code is. Problem is, code isn’t always alone and oftentimes it goes in company with other files. Mostly binary files that the code uses or produces. Those files can still be tracked with Git, but Git usually can’t see through them since they aren’t text. So, you only know that the file has changed and you record the whole size changing1. If you have big files and / or multiple small files that change could be a problem since every time you change —or move— the file, you duplicate —more or less— the size of the file in the repo since Git is keeping a copy of the old file plus the new file.   In other cases, you are a little bit clumsy or inexperience with Git —like me— and you commit files that are too big for GitHub —more that 100mb— so when you want to push the changes to GitHub you can’t.   Measuring your repo   I think the first thing before we start reducing the size of our repo is measuring the real size of it and try to find out where is the problem. For that we can use a tool called git-sizer and it’s going to give us a report of why our repo is so big. To install it:   1 $ brew install git-sizer   Then, you go got the root of your repo and you can type:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $ git-sizer --verbose Processing blobs: 7865 Processing trees: 7315 Processing commits: 2974 Matching commits to trees: 2974 Processing annotated tags: 96 Processing references: 115 | Name                         | Value     | Level of concern               | | ---------------------------- | --------- | ------------------------------ | | Overall repository size      |           |                                | | * Commits                    |           |                                | |   * Count                    |  2.97 k   |                                | |   * Total size               |  1.16 MiB |                                | | * Trees                      |           |                                | |   * Count                    |  7.32 k   |                                | |   * Total size               |  5.17 MiB |                                | |   * Total tree entries       |   131 k   |                                | | * Blobs                      |           |                                | |   * Count                    |  7.87 k   |                                | |   * Total size               |   281 MiB |                                | | * Annotated tags             |           |                                | |   * Count                    |    96     |                                | | * References                 |           |                                | |   * Count                    |   115     |                                | |                              |           |                                | | Biggest objects              |           |                                | | * Commits                    |           |                                | |   * Maximum size         [1] |  11.2 KiB |                                | |   * Maximum parents      [2] |     2     |                                | | * Trees                      |           |                                | |   * Maximum entries      [3] |   573     |                                | | * Blobs                      |           |                                | |   * Maximum size         [4] |  10.4 MiB | *                              | |                              |           |                                | | History structure            |           |                                | | * Maximum history depth      |  1.99 k   |                                | | * Maximum tag depth      [5] |     1     |                                | |                              |           |                                | | Biggest checkouts            |           |                                | | * Number of directories  [6] |   104     |                                | | * Maximum path depth     [6] |     9     |                                | | * Maximum path length    [7] |   105 B   | *                              | | * Number of files        [8] |  1.90 k   |                                | | * Total size of files    [8] |   177 MiB |                                | | * Number of symlinks         |     0     |                                | | * Number of submodules       |     0     |                                |   and you will get something similar to that one…   Now, you know that the problem is your blobs most of the weight of your repo.   When you commit files too big for GitHub   I think this is the most common case, when you are beginning with Git and even more with GitHub. You are happily committing your changes with Git and when you want to push them to GitHub to share your repo or to have a backup of it, you come across a message saying that some files are too fat for GitHub and you can’t upload.   Well, this is more or less easy to fix and you can use two tools to do so. One is an “in house” tool that’s called git-filter-branch. The other one is an external tool that everybody says it’s easier to use and more effective, BFG Repo-Cleaner. I decided to use the latter one since it seems simpler, faster and easy to use.   You can find more detailed instruction in the BFG Repo-Cleaner website, but this is more or less what I’ve done. First, I installed the tool in my machine.   1 $ brew install bgf   Then… I cloned my repo, so I have a backup copy in case something go South.   Be careful: do a copy of your repo before you work with and keep that copy until you are really sure everything work as intended.   Now, I went to root of my repo — you can also perform the command form outside— and run the following command that deleted all the blobs from the commits of my repo bigger than 100 Megabytes.   1 $ bfg -b 100M [some-big-repo.git]   * You need to add the last part if you are outside the repo   BFG doens’t really delete the blobs from the repo, just from the commits. So after I ran BFG I need to do housekeeping in the repo with the following commands.   1 $ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive   reflog command manage the references log and gc clean your repo of unnecessary stuff —garbage collector.   Now, is when I noticed the decrease in size in the repo and I can push changes. However, we have to take into account a couple of things before pushing.      If you have follow the BFG website steps you probably have cloned you repo from some online repository, and now you can push.  :leftwards_arrow_with_hook:   If you don’t, you are going to get message something like you first have to pull and then you are going to push. You can use -f to force the push if you want. :leftwards_arrow_with_hook:   This happens basically because when you are using BFG you are rewriting your history and in consequence you’re changing the hash of your commits, so they don’t match anymore with the commits in your online repo. In other words, you need to ditch everything you have upstream and upload your repo as it was new. For these reason you need to use —force to upload. :leftwards_arrow_with_hook:   This will have consequences if you are sharing your repo with other users or if you have merge your repo with other repos. :warning:   Again:exclamation: be careful:heavy_exclamation_mark:: save a copy of your previous work till you know everything work properly. If you are working with other people in the same repo, notify them of the drastic changes before you begin to work with BGF.   BFG when you’ve merged with an upstream repo that you don’t control or own   I’m going to put as example what I’ve done with the repo of this blog. I didn’t have a lot of experience with Git —and I still don’t, living and learning— and I was sometimes just trying things here and here —the stuff of science, test things. The result has been a a little bit clutter Git history to what you have to add that I’ve moved my images a couple of times till I settle with a location I like. I also optimized them with an app, so the end result is, I have in my tree those images committed perhaps a minimum of couple of times, and sometimes three or four times.   The original repo of the template has around ~70mb is size and my repo reached around ~500mb on my hard drive and around ~230mb in GitHub. I’ve made changes and added photos, but not that much to such a bit size.   As I told you, when I started this blog, my use of Git was a little bit rudimentary and didn’t used branches properly, committed a lot in the master branch and other stupidities. I was putting patches and trying things, and I even some time, I think, I duplicated all my history and fixed it somehow… Summing up, no sleep stories, that almost all of us have suffered when you are learning the ropes of a new tool.   On top of all of that, I wanted to continue to receive updates of the template from his creator, so at some point I set up a branch which upstream was the master of minimal mistakes template. Usually my workflow was:   pull the changes → merge to my develop branch → check everything is to my liking → merge to master   This setup gives me total control about what is updated in the template and how, while I still receive updates. It’s a little bit more manual than have the gem2 and overlap what I don’t like or want to customize with my code, but I can do things more granular and I can learn in the process —editing the code and see from inside how minimal mistakes’ developer, Michael Rose, changes things.   So, I run BFG in this repo in a aggressive way, looking to remove everything over 500K and even deleting all the image files in the folder /assets.   1 2 3 $ bfg -b 500K $ bfg -D \"*.{jpg,jpeg,png}\" $ bfg --delete-folders \"assets\"   The result was gorgeous. I really trimmed the size to what I have to be —around ~140mb.   The problem   The problem doing this was… I unrelated the histories of my repo with the minimal mistakes repo, in other words I have to --alow-unrelated as I did the first time I wanted to merge them again3. This happens basically, because as I mentioned above, BFG rewrites your history and doing so it changes the hashes of all the commits that BFG touches. If you’ve own all the branches that you’ve merged into your master, or other  branches isn’t really important, but this wan’t the case. Therefore, when I download again the minimal mistakes branch from its upstream and merge it again —after the --allow-unrelated— all the changes where reapplied and all the commits from that branch appeared duplicated in my history :scream:.   Solution   Sorry, but there is not a correct solution here. Or at least I don’t know one —if anyone nows, please share it with me. If you change the hash of a commit in your history that it comes from a merge and you want to merge again that branch —because there is new changes— Git isn’t going to identify those two commits as the same and they are going to be “duplicated”4. Take into account that when you change a commit, all the following commits change too, since the hash is generated taking into account the previous commits, so you generates a cascade effect that changes all the commits’ hashes in your history from that edited commit onwards.   What did I do? Clean slate                        Source: xkcd.com              Since my aim here was to reduce the size of my repo, but this duplication didn’t satisfy me, I decided to take a drastic measure. I just recloned minimal mistakes repo on my hard drive and this time I created a new branch for my changes. I rename the original branch and rename my branch as master. Copy &amp; paste my changes from my original repo, so all my previous changes are now condensed in one commit. Them push everything to my upstream repo in GitHub. Since I have a copy of my original repo —before using BFG— I considered that as my archive and I’ve uploaded as so.   My repo folder weights now ~170mb with all its branches —including minimal mistakes one with all its changes— which is more than reasonable.   Lesson learned   The lesson here is, you can do whatever you want with your repo on the condition you are the owner and the master of all the branches. If you don’t, you have to be really careful changing commits down to your tree because it’s going to be problematic.   You also have to try to adhere to Git best practices and try to keep a working tree as clean as possible. And, of course, be mindful when you add those binary files or do not add them at all unless they are really necessary.   There are also other options when you have to work with big files in Git, like LFS, which I have yet to explore.                        Source: xkcd.com                             This is not really true isfyou use some Git GUIs or Diff tools that can show you the two different versions of the file, but in the end you are going to to end with the two versions stored somewhere else. Also, depending in the type of file, Git will be able to store just the differences of the file, since it’s stored in text mode, but best practices always tell you that you shouldn’t track those files with Git, even more if your code are the one generating the file. For example, if you have a markdown code that generates a pdf file, you store and track just the markdown not the pdf. &#8617;                  I have to confess that till really recently I was using the gem and the whole code of the template at the same time. It didn’t have any downside, but it wasn’t really smart. &#8617;                  You are probably thinking: didn’t you make a branch to begin with you changes form the original template?. NO! :scream: I didn’t. I just delete all the stuff I didn’t wanted and then I begin to edit the template. After I changed to upstream repository of master to the curren repository —actually I was messing even more for sake of curiosity, but for the sake of everyone let’s keep this short— and uploaded my changes. After that, I realized I wanted updates and after more messing, so on an so forth, I ended with a branch with the upstream to minimal mistakes master. &#8617;                  They aren’t really duplicated, unless you rebase. However, when you merge, all the commits of that branch become part of your history. They are the parents of your following commits. If Git thinks that they aren’t the same commits as before, you’ll see them duplicated in your history :tada:. &#8617;           ","categories": ["Professional"],
        "tags": ["git","technology"],
        "url": "https://luispuerto.net/blog/2018/11/05/cutting-down-the-size-of-your-repo/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/big-file.jpg"
},{
        "title": "The Explorer with the People Mastery",
        "excerpt":"A couple of posts ago —or perhaps a little bit more— I was briefly explaining about the results of a personality test I took while attending a course in Finland. In that test they told me I have a personality with a role of Explorer and a strategy of People Mastery. The role determines our goals, interests and preferred activities and the strategy shows our preferred ways of doing things and achieving goals. I would like to explore —if you excuse me the repetition— a little bit more those features of my personality and discuss a little bit more if they are accurate.   Role: Explorer   Let’s begin saying that I’ve always consider myself an explorer. I like to wander and discover new things, and always yearn to find and arrive to places where no one was before or that they have been seldom visited. I’m a militan wanderer and as Sagan said, I really think it’s a characteristic that has been meticulously crafted by natural selection as an essential element in our survival. Of course, this is something that in our time and place is really difficult —unless you start looking to other planets— because although there are still really remote areas in our planet we can confidently say that humankind has reached to all places around the glove, even the most remote and extreme, if not physically, through satellite images.    Wanderers - a short film by Erik Wernquist from Erik Wernquist on Vimeo.   However, it seems that to be a explorer have much more connotations than just explore, and if I take a look to the explorer role in the 16 personalities site I come across things that I think are mostly true, with some exceptions —of course all the good things are true and all the bad are false :grimacing:. These are the ones I really thing represent myself.   Uncertainty &amp; quick thinking   It’s true that I’m really confortable with uncertainty, but I also really like to plan ahead and usually before I being to walk I have more or less a clear picture of where I want to arrive. Perhaps I really don’t know how to reach there or what is exactly the way, but more or less I like to have an idea. I also find sometimes boring to prepare too much for something. I like to be prepared, and I consider myself always prepare —if I go to the fiel I usually always carry with me a flashlight, even when I just going in the morning, you never know what is going to happen— but prepare, for the sake of prepare, it’s just boring.   It’s also true that I think quick and as a good Spaniard I’m not afraid of improvisation. You always have to adapt to the circumstances you are going to come across, even when they aren’t the best. But, again it’s always nice to have something in mind before hand and a plan b just in case. It’s always nice to have some sort of flight rules.   Gadgets &amp; persuasion   I love gadgets, complements and tools, if they are useful, or course. And when I find something I like I really want to squeeze all the juice of it. In other words, I really like to learn how really operate it.   In relation to persuasion, I really don’t know, when I really want something, I really want it, and I don’t stop till I have it. It could seem that I’ve stopped sometimes, but usually there is some craving underlying somewhere. I know that this could be good or could be bad. I’m trying to keep the good site and toss the bad.   Monotony, quitting &amp; obligations   Well, who isn’t bored by monotony? I think that everyone like some changes from time to time. However, it’s true that I really need them, and at least from time to time I need to have some adventure to feel that I’m alive. I think that is the reason I like to travel a lot, meet new people and find new things. If I don’t I usually end up feeling trapped. Don’t get me wrong, I can endure that situation just fine —remember I live up North— but I rather not.   About quitting, well, it’s more or less true, but I really think that life is like investing, if something isn’t really working, it’s better quit, take your loses and try to find a more fruitful investment. There is nothing bad into quit from something that you really don’t see any future, unless you take quitting as the norm, which I don’t. However, and as almost everyone, it’s not the first time I’ve ended staying and continuing projects even when they didn’t have good prospects or weren’t returning the investment, just because, I really wanted to finish them.   About obligations… I don’t know, don’t you feel that when something is mandatory it loses the appeal? There is a Galician saying that you can more or less translate to English like one always want what one can’t have, in other words, grass is always greener in neighbors’ lawn. Obligations are sometimes like that, when you ought to do them, they become less fun, even when they are. Anyway, I understand that, what have to be done, must be done, thus, it’ll be done.   Relaxed, freedom-loving &amp; spontaneous   Is someone out there that like to be stressed1? Yeah, it’s true, I’m relaxed and I really like to be casual, but that doesn’t mean that I’m like to be lazy, careless or nothing os the sort. I just really think that when things aren’t taken too seriously you —usually— can manage them better and —usually— you’ll end up perceiving something closer to reality. This is something that I also apply to myself —and I hope others too— I don’t like to take myself too seriously and I hate then other people take themselves too seriously. Life is a tragicomedy —now, that I think about it, this angle is really typical in the South of Europe, where we are really used to laugh to ourselves.   Other reason to be relaxed is, the social side of life. Then things are really serious they get stiff, and when they get stiff, the social component is usually lost. The social components, whether you like it or not, is the glue that make us stay together and in the end work together. In the past we could work alone, but nowadays it’s just impossible. Teamwork is a must.   I like freedom, and I love that others also have it. Specially in relations —and all human interactions are relations, more or less intimate. I like that relations really flow in a natural way and I don’t like to force anything. However, I know that relation have to worked out if we really want to have a final positive balance. For instance, friendship is like a plant, if you don’t water it —even from time to time— it usually ends dying. Freedom also makes me to don’t care what others think about me —unless the opinion really comes form someone I really care about.   As I said before, I’m not afraid of improvisation and for that reason I really think I don’t mind to be spontaneous —but not incredible spontaneous, don’t think I’m jumping around all the time. I also know when I have to adapt to the environment and when I have to adapt the environment to my needs.   Finally, I know that sometimes I have problems with something that in psicology is called delayed gratification —or at least I think I have problems— but I know that the perfect way to deal with this problem is to make divide your goals so they are reached more often and the we have gratification more often. In other words, chop those big tasks in smaller ones, that are easier to manage and quicker to reach.   Strategy: People Mastery   I’m really surprised that they have suggested that I have a strategy related to people and even more surprised that this has to do anything with any kind of mastery. I’ve been always quite shy and my mom was always pushing me to interact with people. In other words, I’m not afraid of people, but for a Spaniard perhaps I’m socially average —or even below the average. I’ve come across in my life with people much more social than me.   Nevertheless, reading the description of people mastery strategy in 16 personalities site, I, again, find things that I would identify with my personality. But again, only the good ones, of course :grimacing:.   Stress resistant   Well, I really don’t know about that and I would be really lying if I said I’m just stress proof. What is more, as I say before, I’m usually a relaxed person. It’s true that If I’m stressed, it’s something I’m not easily show off for several reasons. First of all, because stress brings stress, even more if you show your stress to your environment —a.k.a. other people— they are going to get stressed so you end up in a stress vicious circle. Everyone, has a breaking point, we are humans after all. I think that more than once I’ve been close to that point but I finally manage to return to safely from that trip.   I’m far away to be fearless when I engage with people, however, I really think that John Wayne was quite right —courage is being scared to death, but saddling up anyway. Everybody has fear when they deal with other people —in higher or lower degree— even more if they don’t know each other —it’s in our nature to be scare of the unknown. How you manage that fear is what matters. Experience is also really important.   Traveling   I have to recognize, I love traveling, but real traveling. I don’t like tourism. I make the distinction that traveling is mixing up and trying to get the most of the place you are visiting. This is rarely possible to archive if you aren’t in the place/area more than 15 days… Tourism is just visiting places for the sake of visiting. As much places you see the better. In other words, quantity vs. quality.   My favorite word is try. I love try new things to be able to have an opinion about that, even is this is just superficial in the beginning. I like to try food, to see and experience things and to know more about cultures and customs.   Confidence   Confidence is something that one doesn’t ever have enough. There isn’t such thing as too much confidence.   Now, in all seriousness, confidence is something that in some situations is better to have even when you aren’t really sure about that you are doing. Be self-confidence sometimes is half of what you need to succeed. Since you are able to see your self succeeding, you put more energy in succeeding. If from the very beginning you think you can’t do it, you’ll never do it. However, confidence, sadly, is also a currency that most of the people keep in high regard when they perceive it in other people, while the dubious are always seem as weak. This is, again, how we are build, and perhaps for a good reason —confidence people usually is positive people and you usually like to be close to people that is positive2.   Do I have confidence? Well, I don’t know, in some areas I’m pretty confidence, but in others I’m always dubious. Anyhow, I try to show as much confidence when needed, but I’m guess I’m not always able to. I hope that due to experience I can improve.   Charisma &amp; cooperation   I really don’t know if I’m charismatic or not, but I don’t consider I have charisma —someone that considers themselves to have charisma would be quite narcissist, don’t they? However, I can be quite boisterous sometimes, as any Spaniard out there in the wild. About talking and listening, I don’t know, depending with what you are comparing me. I really like to listen —or I really want to improve my listening skills— even more if what people is saying is interesting. I also think that most of the people usually don’t truly listen to other people, and this is really problematic. Listening isn’t easy at all, and of course sometimes I fail to do so, even with people I love.  Listening is a really important characteristic of any dialogue3.   About talking… I don’t know if I’m really talkative. If I have something to say about a topic, I’ll tell it for sure and I’ll try to explain myself as much and clear as possible, something I don’t always achieve.   I love cooperation and I really think it’s the way to go and in humans is more natural than competition. Probably it’s the trail that has brought us here. We can have and should have both characteristics and competition has helped us to muster up our capabilities, but at the end of the day, we always going to need to cooperate —with our team, with our workmates, friends, family, etc— to be able to compete with others. Competition isn’t always needed. Some people think that through extreme competition is how the humanity has advanced the most, in other words, war is what has helped us to get some of our most important technological advancements. Although war has been important in human development, if you just account the human advancement in science and research since the end of WWII, which has been the biggest chunk of peace in history, just pale all the rest of human history4.   I never ever have found that any part of my ego is at stake when I’m talking, dialoging, discussing or having a disagreement with anyone. I guess that I have ego somewhere else, like everyone else, but in general I don’t carry it with me —or at least I try to leave it at home. And it’s true that I like to reach agreements and I always think that in any disagreement there is a way to find a solution that more or less satisfactory to everyone involved.   In general, I don’t have or hold any grievance, resentment, grudge or anything like that… I think that at some point in my life I came across people has acted with bad faith towards me, or have taken decisions against me or my feeling. I believe that most of the time isn’t the person who is at fault but the system. Besides, at the end of the day, all those things are more harmful for your than they are for others, at least for the ones that they are aim for. As much as you wish, that voodoo doll you have in your mind, isn’t going to work, and it’s just stealing time you could be doing something else, like being happy.   And yeah, I’m probably socially idealistic and also realistic because one has to be realistic and demant the impossible. Putting aside revolutionary slogans, as good engineer, I’m also wondering how something can be improve and operate more efficiently, society isn’t an exception.   Leadership &amp; sharing   I don’t consider myself a natural leader or nothing of the sort, but I’m not afraid of leadership if I have to assume it —the punishment which the wise suffer who refuse to take part in the government, is to live under the government of worse men5. Something that has happened to me couple of times in my life. I also think that to be a leader you don’t have to be a leading position. Everyone is the leader of their own life and master of their own fate —I am the master of my fate, I am the captain of my soul. Anyhow, I know that there are people much more capable to lead than me out there and I’m not afraid of get behind them if I have to.   Leadership is quite a fuzzy term and much of a buzz work in business circles. Sadly, leaders are sometimes mistakenly associated just with executive positions and to be in command. Yet, leadership has more to do with empower others than with command others and bring people from personal visions to share visions of the future.   Of course I believe in sharing and I enjoy doing it. Do you think I wouldn’t be writing these lines I didn’t like to share? I can be independent and as only-child I developed a quite independent personality. I’m not afraid of taking one path alone if I really feel it’s the correct one. However, when you share your journey with others, the trip is much more enjoyable.                  However, it’s true that certain levels of stress in some situations can be beneficial. Stress is body’s mechanism to cope with some situation. Be all the time stressed is bad, but it’s also be all the time relaxed. &#8617;                  This is not always true, for good or for bad. Sometimes people try or want to be surrounded by pessimistic people all the time, that really cripple their ability to succeed. In some cultures this is really embedded. In other words they like to be surrounded by pessimistic people because they distrust optimistic ones, which sometimes brings an environment of constant distrusting and in the end failure, which becomes a norm. Spain is sometimes like this, even more after after the crisis and the current political situation. I’ve read more than once something like I’m not pessimistic, I’m just well informed. This in my opinion is wrong. When you think that only the worse or a bad outcome can happen, for sure it’s going to happen. The best way to defeating someone is begin to remove him from any kind of hope. &#8617;                  “To the Greeks dia-logos meant a free-flowing of meaning through a group, allowing the group to discover insights not attainable individually. Interestingly, the practice of dialogue has been preserved in many “primitive” cultures, such as that of the American Indian, but it has been almost completely lost to modern society. Today, the principles and practices of dialogue are being rediscovered and put into a contemporary context. (Dialogue differs from the more common “discussion,” which has its roots with “percussion” and “concussion,” literally a heaving of ideas back and forth in a winner-takes-all competition.)”. Excerpt From: Peter M. Senge. “The Fifth Discipline: The Art and Practice of the Learning Organization: First Edition (Century Business).”. &#8617;                  Yeah, I know this is really a complicate claim to make, and there is still some disputes out there. It’s true that a lot of advancements, really wonderful ones, were brought or based on conflicts and war. However, when you see all the wonderful achievements we’re making in science, research and technology, just because we’re freer than ever to cooperate. There are more researchers today, than have been in the past combined. Science has grown exponentially. &#8617;                  I know that isn’t the original quote from Plato —That is perhaps why to seek office oneself and not await compulsion is thought disgraceful. But the chief penalty is to be governed by someone worse1 if a man will not himself hold office and rule. It is from fear of this, as it appears to me, that the better sort hold office when they do, and then they go to it not in the expectation of enjoyment nor as to a good thing,2 but as to a necessary evil and because they are unable to turn it over to better men than themselves— but I think Emerson put much better himself. &#8617;           ","categories": ["Personal","Professional"],
        "tags": ["my personality","science","work"],
        "url": "https://luispuerto.net/blog/2018/11/13/the-explorer-with-the-people-mastery/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/explorer-header-01.jpg"
},{
        "title": "Merging GeoPackages",
        "excerpt":"The other day I had to deal with a problem I haven’t dealt before. I have to merge a bunch of GeoPackages (.gpkg) containing Finnish Forest data —downloaded from here :finland: — because they had more sense if they ended up together. The reality is, GeoPackage format is something quite new to me —well, it was approved in 2014— since I’ve been using ESRI products, where shapefiles (.shp) are more or less the standard —they are the King 👑— and if you want to go a little bit further you have to use file geodatabases. From there on you need to step up your game and have a real database engine/server like Oracle :money_mouth_face::money_mouth_face::money_mouth_face:, MS SQL :money_mouth_face:, PostGIS :elephant: —free and open source :+1:—​ and some others.   What is? and why the GeoPackage?   Well, as you can read in this article and I’ve stated above, shapefiles are the de facto standard in the geographic world. However, it’s true that could be really problematic to work with them. For starters, a shapefile isn’t just one file but at least 3 and usually they are 4 or more, which make things difficult when you want to deal with them at operating system level and outside any geographic viewer or file manager. Not to mention the other characteristics that make them a poor choice nowadays:      Small storage capacity: 2GB   Only one type of spacial features per file: points, lines, polygons, etc.   Poor data storage features as columns name, timestamps or small capacity of storage in fields.   Etc.   ESRI came out with two substitute formats, Personal Geodatabase and File Geodatabase, with problems galore. Former one was basically a Microsoft Access MDB file tuned up to store geographical data, which pose many many problems —first and most important Windows only :-1:. Latter one was an improvement, but was proprietary :lock: and ESRI never really open it up. In other words, if you don’t pay, you can access de data in read only mode.   For all these reasons OGC (Open Geospatial Consortium) came forward and try to create a real standard to store spacial data. GeoPackage was born and with a series of improvements.      It’s SQLite 3 database file so it’s more reliable and and language independent.   Multiplatform. Windows, macOS, Linux and more.   It’s just a file, not a series of files or a directory.   Can store more than one kind of data in a file, even raster files.   Can be extended over time if it’s needed an evolution.   And …. IT’S OPEN :unlock:. So you never ever are going to need to pay to access your data.   You can read a little bit more about the formats and why you should use GeoPackages :package: here.   How to merge?   Now the big question, how do you merge several GeoPackages in only one? The answer didn’t seem easy, and after a lot of rummaging  here and there I came to the conclusion that with QGIS I couldn’t perform the task, at least in the graphical interface, and GDAL and the terminal was the answer.   However, merge one by one on the terminal was a pain-in-the-tree :deciduous_tree: so I decided to merge all of them using the a for loop. The command for one file is:   1 $ ogr2ogr -f \"format\" --append destination-file origin-file    Where:      -f is for format in our case \"gpkg\".   —append is to append information fo the destination file instead of overwrite it.   The syntax for for is:   1 $ for whatever-variable in somewhere; do whatever-command with whatever-variable; done   So the final command should be:   1 $ for filename in your/folder/*.gpkg; do ogr2ogr -f \"gpkg\" -append your/other/folder/destination-file.gpkg \"$filename\"; done   Voila… you now should have a destination-file.gpkg on your/other/folder/ containing all the info of of the GeoPackages in your/folder/. In my case ~250mb GeoPackage file.   Downsides?   Yeah… one in this case. Since the info in from those grid GeoPackages was overlapping on the borders, we now have some info duplicated. I’ll explain how you can clean that info in a post in the following days.     PS/ The problem I faced the other day related to the git repo being too fat and with too-big-for-github files was related to this GeoPackage file :smiley:.   ","categories": ["GIS","Technology","Professional","Forestry"],
        "tags": ["how to","QGIS","data science","Finland","forest inventory"],
        "url": "https://luispuerto.net/blog/2018/11/19/merging-geopackages/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/geopackage-layers.png"
},{
        "title": "Trump, raking, Finland & wildfires",
        "excerpt":"Update Sunday 25th Nov 2018 at 11.35 UTC: Just to clarify a little bit. What we really need is more management, not less. We need to get the most of our forest, while conserving them and try to understand better how to the natural system that it’s works. We can’t abandon our forest to their luck in the wild now because it would be like abandoning our dog in the forest just because it’s an animal and it should know how to survive. Probably it won’t. We also need to assume that, as animals, we are part of the ecosystem and we need to play a role too. What we really need to figure out is what is our role. I assure you that, as any other animal out there, our role isn’t going to be a passive one.   TL:NR: This post was in the beginning much sorter, but sometimes ideas have life in their own. I’m sorry to not be able to be more concise.     Since people has been talking about this non-stop, I’m a forester, I live in Finland and I’ve been in the US, lets have a post about this.   As you probably already know, this year has been a really nasty wildfire season in US and recently a really devastating wildfire occurred, Camp Fire, which has left a incredible track of destruction, desolation and pain on the population that suffered its effects. President Trump decided to pay a visit to the place of the devastation and he decided to make this statement:   https://www.youtube.com/watch?v=nwL6GWYg34M   Well… first of all, I really don’t understand why this man don’t take the opportunity to just being silent or at the very least, don’t talk about something he —clearly— doesn’t have an idea how it works. This phrase sparked a myriad of responses, critiques, mockery and memes in US, in Finland and all over the World. News outlets even asked the president of Finland if he and Trump were talking about raking.   After that statement, he also decided to tweet —oh no … here we go— another statement related to “forest management”, on which of course he is an expert.   There is no reason for these massive, deadly and costly forest fires in California except that forest management is so poor. Billions of dollars are given each year, with so many lives lost, all because of gross mismanagement of the forests. Remedy now, or no more Fed payments! &mdash; Donald J. Trump (@realDonaldTrump) November 10, 2018   So, let’s try to respond, with a little bit of rational thinking, to this non-sense. Please, bear also in mind that although I’m a Forest Engineer, and I do have formal education in wildfires1, I’m not, by no means, an expert in the topic. I know the basics, how wildfires usually spread, how to manage fighting operations and, of course, how to manage the forest to decrease the risk of wildfire happening.   Do Finns rake their forest?   I really don’t know what Trump and Niinistö were talking about, but I hope, I really hope, they weren’t talking about raking the forest floors and they were spending they time in more productive topics.   NO!!! Finns, and no-one else, rake the forest floors… they don’t even rake the leaves on the street where we live since they use leaf blowers to keep the street clean2.                        McLeod Rake              However, it’s true that Finns take care of their forest with great dedication since they’re an important part of Finnish economy and culture, they are the most forested country in Europe. Normal logging operations are carried out and, of course, maintenance operations as: thinnings, clearings, extractions, afforestations, reforestations, etc. You have to bear in mind that the majority of Finnish forest (96%), is classified as semi-natural forest, in other words they are managed or have been managed at some point in their history. Around 60% of the forest is owned by private individuals and the 9% by forest companies, so around 70% of the Finnish forest is private, while 26% is owned by the state. In other words, most of the forest in Finland is productive forest for wood and usually forest companies, forest associations or private individuals take care of their piece of the forest in one way or another and with or without the help of the state. Unless the forest is under some sort of protection, dead trees and wood are usually extracted and the amount of fuel is kept as low as possible.   Yet, there are some people that really rake the forest with a forest rake, but I doubt they can be found in Finland. The forest rake is called McLeod and it’s sometimes use by fire combatants3 to create defensive lines agains wildfires. They remove debris and vegetation and create a stripe of mineral soil where the fire can be stopped by lowering its intensity.   Is California / US forest comparable to Finnish forest?   Well, although they share for sure some characteristics and I’m not an expert in Californian forest, they are pretty different. The US Pacific Northwest is closer to the Finnish forest, thought. But you have to keep in mind that most of Finnish forest, if not all, is boreal forest while not even the Pacific Northwest forest hold this biome. This make these forest hardly comparable. You have to think Finnish forest is composed mostly by 3 tree species4 while American / Californian forest have dozes of different tree species.   Another interesting difference is orography. While California has a vast diversity of terrains and orography, Finland is mainly flat —like a board. Of course, Finland possesses some hilly features, mostly located in the North, but nothing comparable to the mountainous terrain you can find in California, and more specific where this tragedy happened, in the lower parts of Sierra Nevada.   Last, but not less important, is the climate. While California has around 10 different climates, being the most commons the mediterranean5 and the desert ones. Meanwhile, Finland has —again— 3 different kinds of climates, being the vastly most common the subarctic. This is of extreme importance because most of the year the Finnish forest is wet —either under the snow or really moisturized. That doesn’t mean that there isn’t a wildfire risk in Finland, just that the fire regime is different. If the conditions are met, wildfire risk increase, and sadly and due climate change, the conditions are met more often each year6. However, this doesn’t mean that wildfires will happen, even more if you follow a correct prevention forest management and you try to reduce the fuel in the forest as much as possible as they seem to been doing in Finland using prescribed fire. These bring us to the next question.   Has there been poor management?   Wildfire fighting   This is really difficult to say and even more when you don’t have all the data, and I don’t —and I won’t— have it, and really few people will. The wildfire is right now under investigation and they are even still fighting it. I really doubt that there is anyone really responsible for it and/or the casualties, and I also doubt that blame would be the answer. I just can give all my support to the victims and of course all the combatants. Wildfires are really really complicated beast and even more when they reach certain size, as this one has been.   Forest management   Well, again, I’m not an expert on what kind of management have been applied in Californian forest and I don’t really know anything in detail about it. However, we can draw some facts here that will probably help us to understand a little bit better the situation.   Those aren’t forest fires   First of all, those wildfires aren’t exactly forest fires. As this piece in The New York Times points out, the fire has cause such an incredible damage, lost of life and properties basically because it run over a wildland-urban interface: places where communities are close to undeveloped areas, making it easier for fire to move from forests or grasslands into neighborhoods. To put in another way, fire spread over places where people live in really close contact with forest and nature. You can check Google Maps aerial photo, or better yet, the Camp Fire’s Structure Status, from California Department of Forestry and Fire Protection, to get an idea of how destructive the fire has been and how was the structure of the landscape was burned.   PG&amp;E   Although the investigation is still ongoing and we don’t know anything for sure yet, some people —they even have begun a lawsuit— have pointed fingers to the Pacific Gas and Electric Company as the possible responsible of the ignition of the fire. We can read in Wikipedia:      The fire started at sunrise on Thursday, November 8, 2018, and was first reported at 6:33 a.m. PST, near Pulga, California, near Camp Creek Road in Butte County, California. Soon after the ignition of the Camp Fire, initial attack firefighters were dispatched to a report of a brush fire under Pacific Gas and Electric Company (PG&amp;E) power lines near Poe Dam on the Feather River. Arriving 10 minutes later, the first units on scene observed rapid fire growth and extreme fire behavior due to low humidity and high winds in the area.    You can see the approximate location of the fire ignition here.   Besides, this seems that it isn’t the first time PG&amp;E equipment has been found as the responsible of igniting a forest fire although they are trying to distance themselves from the events.   Power lines have to be really clean and they are usually cleared every year to minimize the risk of fire. Trees are also checked so no branches are in danger to touch the line and crease a spark, a fire and a power outage. However, all of this is costly and require a lot of time, and humans, to review and clean those power lines stripes.   Cal Fire   US sometimes is really messy when it comes to who has to do what and who is the responsible to this and that. Some times responsibilities fall under the Federal Government and sometimes under the State Governments and sometimes under both and other times no one knows what is going on… Don’t be surprised because this is the bread and butter of almost any federal state / country —Spain is almost a federal state and we have this problems too— even more if it’s old —in Spain boundaries of forest, municipalities, properties, regions, etc are so messy that I have a headache only to remember it— and even more if the state is big —EU is as big, or more, than US, and its becoming to have this kind of issues too.   Cal Fire (The Department of Forestry and Fire Protection) seems to be the responsible for the forest management in California. But they seems to be mainly focused in Fire Fighting, representing more than the 80% of the agency budget, which is not even close to be enough to maintain this wildfire regime.   Besides, we have to take into account that the fire started inside the limits of the Plumas National Forest which is under the jurisdiction of US Forest Service.   US Forest Service                        Smokey the Bear              The US Forest Service has a really long tradition of fighting wildfires and I would say that most of the wildfire science and techniques has been developed by this US Government agency. They are also probably the organization that most forest science produces in the World7 including wildfire management. US citizens should be proud of all the people working at the agency because they do a really an incredible job.   However, US Forest service seems that has been in decline in the last decades, with shifts in policy, vision, aims, organization, etc… and without a clear mandate from the government. Traditionally, the US Forest service had an approach to forestry from the natural resource management, but from the 60s on this approach begun to shift to a more conservation vision, and different families with different approach begun to appear. Society also has changed, and we all are now more worried about environmental issues. All of these have created a more hostile environment to manage forests, increasing the numbers of stakeholders involved in the decision-making processes for managing. Where there were just the USF Service employees, the communities close and relaying in those forest resources and the timber industry, now there are a plethora of individuals and organization that want —and probably should— to have a say in the managing processes and activities.   This is isn’t something good or bad per se, just it’s, and reflects that, what we demand as society from our natural resources has shifted, but it has increase the time and resources needed to produce the same amount of decisions, plans, solutions and other products. Before, we just needed wood, now need more products, some of them even as intangible as “feeling good because we are taking care of Earth” or “a beautiful forest”. I really think that the environmental movement from the 60s and 70s was really positive, meaning for a lot of people the awakening to the idea that we need to take care of our planet. This was probably part as a result of policies from US Forest Service and National Parks Service —among others— that little by little begun to democratize nature and approach it in a gentle manner in National Parks and National Forest. We in the end begun to think that we need to take care of it.   Nevertheless, sometimes hell is paved with good intentions and this has brought us to a gridlock situation where forest’s health has decrease and fuel has built up. To add up, although the budget of the service has been increased in latter years, all that increase has been assigned to wildfire management — while it is not even enough— while the other areas of the service has been stalked and degreasing budget due to inflation, and seems that further budgetary cuts are planned.   I really recommend to read this article to have a broader vision about the US Forest Service situation and history.   Climate Change   A lot of people have also been pointing fingers to climate change as a direct cause of the increase and virulence of wildfires. I would say that they are right and they don’t.   Although the existence of Climate Change is something undeniable and it’s more than probable they play role in increase of wildfires, it isn’t the only factor in the increase and it shouldn’t be used as an excuse in my opinion. It’s true that the unusual dry and hot conditions, that seems to be driven by climate change, increase the amount of dry and fine fuels in forest and converts it in a tinderbox. However, you have to take into account that there are forest in really diverse climate conditions and places under different management regimes and some of them end up charred and some others don’t. If we look at the below figures, you can see that it’s really interesting that in Spain where most wildfires occur, is where precipitation is higher and the temperatures are milder. It’s also where one of the most forested area of Spain is located, but they are other areas with plenty of forest and they don’t get burned. We also have to take into account that wildfires in Spain have decreased.                                                                                                                          Wildfire incidence, mean precipitation and mean temperature in Spain. Source: 1 &amp; 2       Management here is a key, and also culture. Part of the Green Spain has a culture of using fire to clear forest to create pasture. How forest are managed and how the property and gains are share among local populations also play a role. Most the wildfires in Spain have a human cause, and a lot of them are intentional. Quarrels, bitterness, grievance, resentment, grudge, etc are common causes of a lot of fires. Not to mention derange people.   US Congress, The President &amp; California   US Forest Service and Cal Fire, depend directly from US Congress, The President of US and the State of California. Those agencies depend on them for organization, mandate and budget. In other words, although they are, or try to be, fairly independent agencies, government still set the general guidelines and set the budget they can and should spend. They also create and enforce the legislation that those agencies has to follow and enforce. Those agencies are just tools of their respective governments to fulfill the purpose of manage and preserve forest and protect forest and population from wildfires.   Conclusion   The conclusion was quite clear from the very beginning, one shouldn’t talk about something that doesn’t have a minimum knowledge about an Trump shouldn’t be an exception of that rule of thumb. In the very end, the US Government, from whom he is one of the highest responsible now, is the real responsible. Is he going to fire himself?   What he should be doing is to try to increase the budget to fight forest fires and start the preparations for restore the area. And of course, prepare the relief fund for the victims. It’s pretty clear to me that decrease funding isn’t the way to go. US Forest Service, Cal Fire, and all the rest of the agencies involved in managing natural resources —in the US and in the rest of the World— have ahead the titanic task of managing those resources in a world threatened, and in constant change, as a consequence of Climate Change. Those are vital for the future of Humanity and need to figure out the best and most efficient way to manage them to still enjoy them while keeping them safe. We are going to need more funds and personnel than ever to fulfill that task and we need to switch from a strategy of just defending those resources from hazards to a more efficient policy—not only economically but in all other terms— of preventing those hazards. Sometimes, hazards can’t be predicted and the worst can —and perhaps should— happen, so we also need to work to make those resources —and the communities living and relying on them— as resilient as possible.   As a society, we also need to rethink our approach to natural resources and what we really want to demand from them and from the agencies in charge of taking care of them. Sometimes I wonder if the democratization of nature has been positive. On one had, I really appreciate the interest of the people on taking care of our blue planet and this interest and concern in really needed to demand policies that change things and help to preserve those resource. On the other hand, we end many times in gridlock situations that last forever and in clashes of interest that many times are driven by misconceptions of what nature really is and should be manage. Neither is just a resource that should be sold and use for profit, nor is that idilic place that fairy tales have build in our head and where everything is perfect and set in place for our enjoyment and aesthetic reasons. We have to understand also that perhaps sometimes nature works in a quite counterintuitive way and that we should adapt our policies and mindsets to our current understanding of the ecosystems. Nature isn’t always pretty or nice, and beauty is really a human concept.   In the end, natural resources should be, and need to be, used with respect. The best way to preserve something is to use it with care and respect. Putting them into a glass urn for our enjoyment is just going to make things worse.                  In my studies in Spain, where fighting wildfires are an important part of being a forester, even more in my region where have the highest density of wildfires in Spain, I took a couple of courses about defending the forest and fighting wildfires. After that, I took also a couple of curses in the topic, with one being a Fire Management Course in the North of Sweden. &#8617;                  Yes, municipalities and building owners pay someone to take care of this problem, in the same way someone comes to clean the snow of the street and your building door. &#8617;                  I say sometimes because, as you can read in the Wikipedia, it’s a quite awkward tool to transport and to deal with. I think I’ve only seen one once and most of the forestry people doesn’t even know that this tool exist. In other words, it’s quite rare. &#8617;                  Scotch pine (Pinus sylvestris), spruce (Picea abies) and birch (Betula sp although mostly Betula pendula). &#8617;                  It’s quite funny because California it’s one of the few places in the World outside of the Mediterranean basic with a Mediterranean Climate &#8617;                  Those conditions are hotter and drier summers. You have to think that the floor of the boreal forest is composed mainly by fine debris and litter, lichens and mosses. All those elements are quite vulnerable to dry out under high temperatures and become a high flammable fuel. However, this temperatures has to continue for a while to really transform the forest into a tinderbox. I remember when I was taking my Fire Management course in the North of Sweden that we have programmed a practical exercise where we have to burn a really small portion —probably less than 10x10 meters— of the forest. We have to really wait, and be on call from the instructors, to be able finally perform the exercise because the weather was so humid —it wasn’t even raining— that the forest ground was really wet and not even using driptorches was possible to start the fire. &#8617;                  Which is normal, if you take into account that is probably the forest organization with the biggest budget and with the biggest number of employees. They take care of almost the half of the forest of North America, so they can easily produce that quantity of science, which is really nice. I hope that we can create something similar in the EU. &#8617;           ","categories": ["Forestry","Opinion","Politics"],
        "tags": ["Finland","wildfires","forest management","USA"],
        "url": "https://luispuerto.net/blog/2018/11/23/trump-raking-finland-and-wildfires/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/raking-forest.jpg"
},{
        "title": "Cleaning a GeoPackage after merging",
        "excerpt":"The other day I was talking about how you can merge several GeoPackages files in a single GeoPackage —just because you wanted or needed that way. Problem is, sometimes some of the data or information that those GeoPackages contain could be redundan. In my particular case, some information was redundan because the GeoPackages contain forest data at stand level that is also at UTM10 Finnish’s grid1 level. Since life is imperfect, some stands are between two grid quadrants, so a decision2 was made to have them in both —or more3— of the quadrants.   So we needed to remove those duplicated stand, among other things to leave the GeoPackage ready to be analyzed in R. Most of the scripts we used are in the GeoProcessing tools from the Vector menu in QGIS3.                        GeoProcessing Tools in QGIS3              Removing the duplicated stands   First of all, we need to remove the duplicate stands from our data. This can be performed with the  dissolve script where the standid was used as a Unique ID field.                        Dissolve script in QGIS3              Removing the stands outside of our area of interest                        Stands overflowing the border              As I told you, there are stands that overflow the grid representing our area of interest. In the previous step we just removed the duplicated stands, but the stands in the outside border of the area of interest weren’t duplicated but part of them were outside, as you can see in the left image. Since we just want the forest area inside of the grid we have to clip those stands using the grid.                        Clip script in QGIS3              Now, we have a perfect straight borders.   Intersecting the stands and the grid   Now, we’ve removed the duplicated and overflown, we want to cut the stands by the grid. We are going to to perform our analysis taking the grid’s quadrants as main units, so we need to cut the stands totally straight everywhere not just in the exterior border. In other words, we need to divide the stands that intersect with the gird lines. To make that possible we are going to use intersection script                        Intersection script in QGIS3              After the intersection was performed, the resulting layer is going to have the information from both intersected layers. In other words, each of the stands have a column now where is stated to witch quadrant of the grid belongs.   Updating the Area   After the intersection with the the grid the areas of the border stands needed to be updated, they aren’t the same any longer since they’ve been divided. To do so we can use the QGIS field calculator.                        Field Calculator in QGIS3              We created a new field and used the formula area($geometry)/10000 to fill it and divided the result by 10000 to get the result in hectares.   You can know more about this functions from the QGIS3 help. For the area function.      function area     Returns the area of a geometry polygon object. Calculations are always planimetric in the Spatial Reference System (SRS) of this geometry, and the units of the returned area will match the units for the SRS. This differs from the calculations performed by the $area function, which will perform ellipsoidal calculations based on the project’s ellipsoid and area unit settings.     Syntax     area(geometry)     Arguments     geometry polygon geometry object     Examples     area(geom_from_wkt(‘POLYGON((0 0, 4 0, 4 2, 0 2, 0 0))’)) → 8.0    And for the $geometry function.      function $geometry     Returns the geometry of the current feature. Can be used for processing with other functions.     Syntax     $geometry     Examples     geomToWKT( $geometry ) → POINT(6 50)    Layer housekeeping                        Refactor Fields script in QGIS3              After the intersection and the creation of the updated area field, a little bit of housekeeping was needed —in my opinion— in the final layer. It’s always good to keep layers tidy and with just the information you need. Too much information that you don’t need just create clutter, which make difficult to read the useful info. To tidy up the layer we used the Refractors fields script on the QGIS Vector Layer toolbox.   Adjoining tables   In these GeoPackages there are additional info that it’s contained in the adjoined tables. Some of that info it’s going to be also duplicated, but you can’t —or at least I don’t know how— clean that duplicated info in QGIS3. I couldn’t find any script to remove duplicate entries in a table.   All these info it’s going to be used in R to perform analysis, so it’s probably must more easier to clean those tables there with something like this. This leads us the last step.   Linking and using the GeoPackages in R   This was the last step, but to know more about this you can check Olalla’s post.                  A little bit more of info about the Finnish grid can be found here 🇫🇮. Basically the grid numbers refer to the scale of the sheet, so UTM10 mean the sheets are in 1:10000 scale. This comes from the era when papers maps were the only thing available so the country has to be gridded in some way. Usually these sheets are just subdivisions of the Universal Transverse Mercator coordinate system. The scale also mean how detailed is the cartography inside of the sheet. So in this case your cartography is 1:10000 detailed. &#8617;                  I don’t know if the decision was good of bad, but I guess it was the best —and a middle road— solution. In the end, the criteria to include a stand in one or another quadrant could come off being really fuzzy. It was probably munch easier to select all the features contained within and intersecting with each of the grid’s quadrants. &#8617;                  I imagine that chances are some stands are in up to four quadrants at the same time, if they are in one of the corners or intersections of the grid. &#8617;           ","categories": ["Forestry","GIS","Professional"],
        "tags": ["Finland","forest inventory","data science","QGIS"],
        "url": "https://luispuerto.net/blog/2018/11/24/cleaning-geopackage-after-merging/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/finnish-forest-stands.png"
},{
        "title": "How I publish my blog: Using Continuous Integration with Jekyll",
        "excerpt":"As I’ve mentioned before, this blog is build as a static website with Jekyll, which allow me not to worry about data bases, logins or anything of the sorts when I’m publishing and managing the web. I use the free tier of GitHub Pages as hosting, which has its downsides and upsides. The main upside is, it’s really convenient and free. On the other hand, I’m limited to certain Jekyll plugins if I want to use GitHub Pages building capabilities. This can be circumnavigate if you use CI (Continuous Integration) to build your site.   What is Continuous Integration?   You can read more about this in the Wikipedia —and in Travis documentation— but CI is basically —and I’m summarizing a lot here— just testing your code before you integrate —merge— it with the master branch, so you know that you aren’t breaking anything —serious. Usually what you do is building your copy and then you merge it with the master branch. However, building is a pain, and even more if you have to do it all the time —continuously. For this reason, the CI services were born.   Usually a CI works as it follows —again simplified:      It’s usually related / connected to your online repo and when you push commits to the repo the webhooks are triggered and CI comes into action.   CI download the last commits of your repo to a machine —usually virtual— with an OS you can specify and begin tu run a series of scripts1 to test and to build your software from the last commit you’ve pushed.   If those scripts give an error, so your code can’t be build, report to you as failing and —bad luck— you have to review what you have done because it’s not working.   If the scripts exit without an error —you’re lucky— and now a couple of things can happen.            Just nothing else. You are notify there is not errors in your code —so you are free to merge.       The code is automatically deployed.                    It’s merged into another branch —usually master.           It’s deploy to a server or something else.                           All of these steps have a lot of sense in GitHub, GitLab and other repo online services where webhooks can be triggered even with a pull request, so you can know beforehand if someone else code is worth to review in an even of a pull request.   That’s more of less the official function of the CI. However, it can do much more than that. For example, in the Homebrew repos/taps it builds the bottles and deploys them to a hosting so you can latter download them. It can build software and apps and deploy it to your website. Or like in our case can build your Jekyll site and deploy to GitHub Pages.   How?   There are several options out there as CI services, from Travis —with is one of the most common one— to Jenkins or, if you use GitLab, their own CI. I’m going to talk about Travis, because it’s the one I’m using myself and usually is the easier option. You can use Travis for free on the condition your project is open source.   Give access to Travis   First of all, you should go to the Travis website and authorize them to have access to your repos in GitHub. Usually, you just need to login with your GitHub account; they will ask for permission and if you just want to use Travis in all your repos or in just some of the repos. I have it set for only the repos I’m interested on because otherwise Travis can be trigged just pushing a commit to GitHub2.   Configure your build   Once you have given access to Travis to the repo where you have your Jekyll page, you need to configure your build. You configure your build with the file .travis.yml in the root of your repo.   Here is mine right now:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # .travis.yml # This file should be at the root of your project # language: ruby rvm:  - 2.5.3 cache:   bundler: true   # directories:   #   - $TRAVIS_BUILD_DIR/tmp/.htmlproofer before_install:   - gem install bundler gemfile: Gemfile script:   - if [ \"$TRAVIS_BRANCH\" = \"master\" ]; then       bundle exec jekyll algolia;        JEKYLL_ENV=production bundle exec jekyll build;        bundle exec htmlproofer ./_site --alt_ignore \"/.*/\" --allow_hash_href --http-status-ignore 999 --disable-external;       fi    - if [ \"$TRAVIS_BRANCH\" = \"develop\" ]; then       JEKYLL_ENV=production bundle exec jekyll build;        bundle exec htmlproofer ./_site --alt_ignore \"/.*/\" --allow_hash_href --http-status-ignore 999 --disable-external;       fi  branches:   only:     # Change this to gh-pages if you're deploying using the gh-pages branch     - master     - develop env:   global:   - NOKOGIRI_USE_SYSTEM_LIBRARIES=true # speeds up installation of html-proofer  sudo: false # route your build to the container-based infrastructure for a faster build  deploy:   provider: pages   skip-cleanup: true   github-token: $GITHUB_TOKEN  # Set in the settings page of your repository, as a secure variable   keep-history: false   local_dir: _site   # repo: luispuerto/luispuerto.net # in case I wanted to build my page in other repo   target-branch: gh-pages   on:     branch: master   You can learn more about how to create your own configuration file on Travis Documentation, but I’ll walk you through my config:      language: The language we are going to use in the build. In our case is Ruby, because Jekyll is build in Ruby.   rvm: is the version of ruby we want to use. I’m using the last one —at this moment— 2.5.3. I don’t know if you can set it up to use the last stable one, but some people argue that it isn’t a good idea and you should set the version so it match the one in your machine —which has sense   cache: You can cache content that usually don’t change. In our case we are caching the bundle install because usually the gems we need don’t change that often.   before_install: Before we install the gems we are going to need to build our page we need to install the bundle.   gemfile: Location of the gemfile, so Travis knows what gems should install.        script: This is the script to build. In ruby the default script is your rake file, but you can define anything else here —like a shell script which is much more simple. As you see I have a couple of entries here that run depending on the branch it’s pushed.              When I push master I run Algolia, build the site and then past the htmlproofer.       When I push develop I just build the site and past the htmlproofer.           Please remember to build your site with JEKYLL_ENV=production because if you don’t some features aren’t going to show in the final deployment.       branches: This define which branches trigger the build. In my case master and develop, but you can add/remove whatever you want.   env: You can define environment variables and in my case since I’m using htmlproofer it’s recommended to use NOKOGIRI_USE_SYSTEM_LIBRARIES as environment variable.   sudo: We don’t need sudo, since we don’t need customizations.   Deploy   This is the last part of the config file, which is going to deploy the final built product —your website— to your repo again but in a different branch —gh-pages in my case. Travis has a section in its documentation about deployment in GitHub Pages.   Just a couple of notes:      on: States the branch is going to trigger the build it’s the previous test exit with no errors. In other words, only when you push commits to master Travis is going to deploy your site. If you run Travis in other branches the deployment isn’t going to happen.   target_branch: This is where is going to be deployed your site. In my case I have a branch only for deployments gh-pages which I don’t even have locally. Be aware that Travis push with --force to that branch so it’s going to overwrite the history in that branch leaving just one commit.   repo: You can deploy in a different repo if you want, just define username and name of the repo.   local_dir: Folder from the build that is needed to be deployed, _site in my case.   keep-history: If you don’t want Travis to push with --force you can set this option to true and it’ll will keep the history of your branch.   github-token: You need to provide Travis with a GitHub personal access token with the public_repo or repo scope, depending if you repo is public or private. This token should be private and the best way use it is to configure it in your repository settings in Travis. If you don’t provide the token correctly, Travis isn’t going to be able to deploy.   skip-cleanup: You need to set this up as true if you don’t want Travis to delete everything after it finish the built, which wind up deploying nothing.   provider: pages since you are deploying to GitHub Pages.   Why?   Perhaps you are wondering why perform the building in your different branches and so. Well, have three permanent branches: master, develop and writing. I usually work on develop or writing and then I merge to master. I also use Git flow for making changes in the source code of the site, and I’m thinking to try to implement it also for writing post and pages.   In other words, I think it’s a good idea to test that everything add up before I merge to master when I’m working on features. Writing, it’s much more simple, and usually the errors are just that some link isn’t correctly set, which is easy to fix. I sometimes use htmlproofer locally when I’m writing so I know everything is OK before I merge to master.   LFS?   At this moment, I have my configuration exactly like you can see above. However, while I was writing this post, I just discovered that you can use LFS with Travis —of course you can, why wouldn’t you? You can’t use LFS with GitHub Pages though, but I wonder if you implement the build of your site with Travis while using LFS, you’ll make it possible. Anyhow, you need to take into account the bandwidth limitations of LFS and it’s probably that, every time you build with Travis you’ll use part of your bandwidth.   I’ll try to write a post in the near future about what is LFS (Large File Storage), but summing it up a lot, it’s just a system that allow you to manage better large binary files with Git, storing them in alternative storage instead of just tracking them with Git.                  These scripts can be whatever you want —more or less. &#8617;                  IIRC to trigger Travis you also need to have a .travis.yml file in the root of your repo but I’m not sure. Anyhow… sometimes I have repos in GitHub that have files .travis.yml and I don’t want they run in CI. In the end you can do whatever you think it’s more convenient for you. I don’t think it’s wise to try to build every time you are pushing to your repo and then receive an email of your build is failing!. &#8617;           ","categories": ["Technology","Personal"],
        "tags": ["jekyll","blogging","CI","travis"],
        "url": "https://luispuerto.net/blog/2018/11/26/how-i-publish-my-blog-ci/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/jekyll+travis.png"
},{
        "title": "How I schedule post in Jekyll",
        "excerpt":"The other day I was explaining how I publish my blog using CI. Today I want to explain how I schedule post in my blog while still involving CI.   You know that Jekyll is great, but sometimes it’s lacking some features, like be able to schedule post. I don’t think it’s a dealbreaker, but it’s definitely helpful. Sometimes I write a couple of post at once, just because they are part of a series, or a single post that is split because it’s really big, or just because I like to… However, I don’t want to publish those post right away so I would like to schedule them.   With Jekyll you can’t schedule post because when you build your site, it’s static —isn’t this the whole point of having a static site?— so unless you build your site again it isn’t going to change. So, even if you set a date in the future in the yaml front-matter nothing is going to happen unless you rebuild the site on that date or after that date. Other option is to build with with the --future tag, but then those post will show when you build which it’s going to be probably too early.   There are several solutions out there to schedule, like this, this other or that one. However, most of those are too complicate for my taste, or you need to have something on AWS, etc.   Since I have a Raspberry pi at home, it’s connected to internet all the time and all the time it’s on, I can schedule cron jobs on it to run scripts. On the other side, you can trigger your site rebuild on Travis using their API. I’m already doing that with a cron job to rebuild my site everyday at 8 in the morning. Cron it’s really handy and allow you a lot of different setups. For instance, you can run a script every minute, every six hours, once a week, once a month, a year… etc.   What is cron?   Just in case you don’t know or you are so lazy that you can’t check the Wikipedia, I’m going to sum up for you what is cron.   Cron is just a scheduler for jobs that most of UNIX machines —linux and mac for example— have. It’s really basic, but really handy if you know more or less how to program it. I recommend you to learn how to use it because can make your life quite easier.   How do you access to Travis API   On travis documentation you can find more options, but this is the basic script you need to run on cron —or on your terminal— if you want to access Travis API.   1 2 3 4 5 6 7 8 9 10 11 12 13 #!/usr/bin/env bash body='{ \"request\": { \"branch\":\"master\" }}'  curl -s -X POST \\    -H \"Content-Type: application/json\" \\    -H \"Accept: application/json\" \\    -H \"Travis-API-Version: 3\" \\    -H \"Authorization: token YOUR-TOKEN\" \\    -d \"$body\" \\    https://api.travis-ci.com/repo/your-github-username%2Fyour-repo-name/requests   You can get your Travis token on your preferences page on Travis page. You also need to set your-github-username and your-repo-name in the url. Remember you also need to make your script executable.   1 $ chmod +x path/to/your/script.sh   Running jobs in cron   If you want to add jobs to cron you just need run in the terminal   1 $ crontab -e   and you default text editor will open. There you can add jobs taking into account the following rules:   1 2 3 4 5 6 7 8 9 ┌───────────── minute (0 - 59) │ ┌───────────── hour (0 - 23) │ │ ┌───────────── day of the month (1 - 31) │ │ │ ┌───────────── month (1 - 12) │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday; │ │ │ │ │                                   7 is also Sunday on some systems) │ │ │ │ │ │ │ │ │ │ * * * * * command to execute # You can put a comment here too.    In command to execute is where you put the route to your script.   For example, if I wanted to run my building script on Nov 26th at 6.07 am I have to set up the following line in cron:   1 7 6 26 11 * /root/scripts/cron-builds-jekyll/luispuerto-net.sh   You can set up as much lines you want in cron, so you can set up multiple post at once for different dates and so.   Note: Please mind in what time zone is your blog and in what time zone is your Pi —or whatever you are using to schedule post— because if they aren’t in the same time zone you need to take into account when you schedule your cron job. For example, I have my blog in UTC but my Pi is with me in Finland —EET— so I need to subtract two hours in winter and three in summer —mind the DST too if it’s still happening.   When I’m out home   When I’m out of home and I still want to program my post I still use my raspberry at home, since I can connect to it using a VPN connection I have set it up on the Raspberry. I recommend you to set it up a VPN if you haven’t done it yet.   Other options   If you don’t have a Raspberry, there are other options so you can run cron jobs online. However, I haven’t tried any of those solutions and I don’t know how well suited are for running scripts.  ","categories": ["Technology","Personal"],
        "tags": ["jekyll","blogging","CI","travis","raspberry pi"],
        "url": "https://luispuerto.net/blog/2018/12/03/how-i-schedule-post/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/jekyll+travis.png"
},{
        "title": "Happy New Year 12019 🎊",
        "excerpt":"I want to wish you all a   Happy New Year 12019                        The 12019 Human Era Calendar from Kurzgesagt. Source: Standard.tv              And make you remember that…   We’ve Made An Enormous Progress In The Last 13000 Years                        From hunters and gatherers to programmers. Source              From Hunters And Gatherers To Programmers And Space Explorers                        Voyager leaving the Solar System. Source              And Although The Way Till Here Haven’t Been Easy                        Is war over? Source              There Are Always Reasons   To Be Optimistic            And To See The Beautiful Side Of Life            And Remember That You Get   “More Bonus Points If You Help To Build A Galactic Human Empire”.     PS/ This is our 12018 in review:                        12018 in review             ","categories": ["Personal"],
        "tags": ["New Year"],
        "url": "https://luispuerto.net/blog/2018/12/31/happy-new-year-12019/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/The-Year-12019.jpg"
},{
        "title": "My other personality trails",
        "excerpt":"                     General idea of what it seems I’m              I’ve been trying to dissect my personality using the results on the 16personalities test in previous posts and I would like to finish talking about how they think my big 5 trails are.   As you can see in the picture in the left. They think I have:      Mind: Extraverted (78%) vs. introverted (22%).   Energy: Observant (58%) vs. intuitive (42%).   Nature: Thinking (53%) vs. feeling (47%).   Tactics: Prospecting (51%) vs. judging (49%).   Identity: Assertive (76%) vs. turbulent (24%).   I would like to talk a little bit about each of the trails and see how I really fit —or I think I fit— on those results.   Mind: Extraverted vs. Introverted                        Extraverted person. Source: 16personalities              In their opinion, I’m extraverted (78%) vs. introverted (22%), which means I’m a highly extraverted person. However, and as they point out at the end of the description about this trail, one shouldn’t confuse introversion / extraversion with introspection. One person can be quite introverted and at the same time be reflective. On the other hand one can be extraverted and introspect, which actually I think I’m.   I really never have considered myself an extraverted person, at least for the standard we have in Spain as an extraverted individual. I see myself more introverted than extroverted, but since I live in Finland, and I have had some experience living in other places, I need to make the following question: compare with whom am I extraverted or introverted?   Experience has make me more extroverted. When I was a kid, I was much more introverted than I’m right now, but with the past of time and the experiences I have had in my life, I’ve, little by little, learned to be less fearful of the outside world. In other words, you lose the fear to fail when you finally understand that usually it isn’t such a big deal and it’s part of the process of learning and living. In the end, it gives you confidence in yourself and you become more extraverted.   On the other hand, and reading directly from their description of the trail, they mention that extroverts are not as sensitive to outer stimuli and need to seek them out in order to gain a kind of functional equilibrium and to perform well, which is somehow true in my case. I really need to have social interaction to survive —who doesn’t?1— and I miss the social life I get when I’m in the South of Europe or in the on the other side of the pond, where social life is different —more dynamic— than in the North of Europe.   They also mention that extroverts are willing to take the lead, push the limits for them and for others and that they feel they can handle any chalenge, which I would say that it’s more or less true in my case. Yet, I really don’t know if I can attribute that to my nature, personality or, again, to experience. Regarding to taking the lead, I prefer rather not, even more if I feel there are someone more willing, and qualified2, than me. In relation of taking any challenge, it’s also true that I’m usually quite eager, but it isn’t the first time that I go with overconfidence. Live an learn they say. Seems that extraverts are much more proactive in experiencing (and relying on) the world around them, which somehow I can also relate to me, since I like places full or energy and activities and social life, in other words, I like cities3.   In the end, we have to take into account that this seems to be just a measure of how much a external stimuli a person can handle. So, it isn’t probably the same concept that people have of extroverts —someone shouting around— or introverts —someone that it’s only able to look to their shoes.   Finally, something called my attention in the description:      Introverts are more likely to dislike coffee and energy drinks.    Are most of the Finnish people extraverted because they love caffeine? I doubt it.   Energy: Observant vs. Intuitive                        Observant person. Source: 16personalities                 …how you see the world and what kind of information you focus on. It may seem like your decisions are the most important, but a decision is only as good as the understanding that backs it up.    It seems that I’m in between two worlds here… they consider me observant (58%) vs. intuitive (42%), and more or less it’s true in my opinion. I think this has a really positive side, because you can end with the best of two worlds.   I’m able to dream, to wonder, to make questions and to think how to improve the world around me. And even sometimes, I do it too much. However, and at the same time, want to take into account reality, the current state of matters, data and facts and being concrete on those thoughts. In other words, I’m a realistic dreamer4.   Nature: Thinking vs. Feeling   In this camp, it seems that I’m also between two worlds, thinking (53%) vs. feeling (47%.   Thinking people are usually rational and follow logic; they tend to use their brains more than their hearts. On the other hand, feeling people, are more compassionate towards other and passionate defending their believes. It seems that I’m more or less in the middle.   I’m a quite logical person, and try to be also analytical. I think that System Thinking is a great tool to understand the world and that more or less everything has a systemic answer and solution. I usually, don’t like to draw conclusions soon and I like to check all the sides of a problems, looking for the root cause. This perhaps has to do with me being an engineer, since I really think this is the most efficient way to operate.   At the same time, and as a good Spaniard, I quite passionate, and when I feel —and I usually know with data and facts— that something is someway different as usually people think it’s or it can be solved in some other better way as usually is, I will vehemently defend those truths until their last consequences. You’ve probably guested it… my favorite color is red.   Tactics: Judging vs. Prospecting                        Prospecting person. Source: 16personalities              This is the last trail where I’m between two waters since I’m prospecting (51%) vs. judging (49%). And provably the one I disagree more with the results.   They say that judging people like to have everything planned and always have a plan B, C, D —and so on and so forth— that they prefer clarity, they like checklists and that they really follow an order —and law— which usually translates into that they have a strong work ethic too. In other words, they aren’t flexible.      …if you see someone walk for five minutes so that they could get to the other side of the street using a crosswalk, they are much more likely to be a Judging type.    On the other hand, prospectors are much more flexible and relaxed when dealing with expected and unexpected challenges, they like to have their options open and it’s difficult to commit to something forever for them, since in the future it could turn out to be an inferior option. They usually focus in their happiness.      they always be able to come up with something better to do.    This really make me to have mixed feeling, because in on one side I’m not really that stiff person as a judging people seem to be. I definitely like to have my options open although I usually have a contingency plan at hand in case something happens. In other words I like to have something prepare to deal with the unexpected even though I’m not afraid of dealing with it and improvise if it’s necessary. And of course, I have a strong work ethic and I’m perfectly able to commit. What is more, I usually think I commit to much to others.   Identity: Assertive vs. Turbulent   To finish, it seems that I’m assertive (76%) vs. turbulent (24%) in the camp of identity. Yet, I don’t fully agree with that percentages, I think they reflect some of my personality and even in some cases, on things I really want to improve.   I feel that I’m quite stress resistant, as I’ve mentioned earlier, and also consider myself fairly self-assured and even-tempered. I’m reasonably satisfied with my life and I’m not afraid of taking challenges or afraid of the ones that, for sure, I’m going to come across in my life —sooner or later. I also think that it’s usually no point into analyze what you have done in the past and even more regret your decisions. Sure, you need to learn from your past, from what you have done and figure out why things turned out the way they turned. But you shouldn’t attach your past to your ankle like a ball and chain in a way that it cripple your future. The important is the present and future and what you are going to do on and with them.   Besides, I’m someone that is always eager to improve and I always want to learn new things. I’m quite perfectionist and I’ve needed to learn that sometimes you don’t need to be that perfectionist to get things done. Sometimes that perfectionism has made perhaps not to push harder to reach my goals in life —if you aren’t going to be able to reach perfection, what is the point on trying?. Lucky, it’s something I’ve learned to deal with, using productivity techniques such as pomodoro technique, agile, getting things done and similar. Productivity is something I’m been always interested on. At the end of the day, I’m an engineer, I really like to optimice and improve things around me, including myself, of course.                  Finns :finland:. Or they say so. But I don’t believe it. &#8617;                  I don’t consider myself specially qualified to lead. I like to learn about it and I think that most people have the wrong concept of leadership —the commanding one— while leading is a really complex position  that has to do more with helping others to find their own way and perhaps pointing the direction —but this can easily be confuse with forcing to go in that direction— than with command. &#8617;                  Yeah, I love cities, specially big cities, where people is really social and I always get mesmerized by the city lights. However, please don’t get me wrong, I love to explore, to camp away in the wild —not in a camping if possible— and enjoy the outdoors. If I didn’t, I couldn’t be Forest Engineer, but as Madona said: I don’t like cities but I love New York. &#8617;                  Recently I’ve been reading Utopia for Realist, and the book open with a quote from Oscar Wilde: “A map of the world that does not include Utopia is not worth even glancing at, for it leaves out the one country at which Humanity is always landing. And when Humanity lands there, it looks out, and, seeing a better country, sets sail. Progress is the realization of Utopias.” &#8617;           ","categories": ["Personal","Professional"],
        "tags": ["my personality","science","work"],
        "url": "https://luispuerto.net/blog/2019/01/08/my-other-personality-trails/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/personality.jpg"
},{
        "title": "Analyzing my personality. Bottom line",
        "excerpt":"This is just the final post of the series of post about my personality. It’s been quite fun, but really hard, analyzing myself —or try to— from the starting point of the 16pernalities test. I wanted to write the final conclusion as a separate post mainly because I want it to encroach all the other three post and I don’t think this bit really fit at the bottom of any of the others, even when It’s going to be small.   The main reason I’ve done this exercise —analyze myself— is improvement. If one doesn’t know well oneself, how one can really improve and try to be a better version of oneself? First time I read the test I really thought that it was more or less shenanigans and I still hold that opinion, since no test is going to give you, ever, a real picture of yourself.   However, I think it’s probably a great tool to know myself better and to walk me through the strong and weak point of my personality. Whether the test is really accurate in framing your personality, it create a interesting template or structure to dip you deeper into yourself. I never thought before in some of the points that the description of my personality raised and, of course, I never thought in evaluating them. So I really think it’s worth it.   The second great reason is sharing. I really wanted to share my personality with the rest of the world. Not because I really think I’m that important to anyone to give a damn about how my personality is, but as an exercise of openness. Usually, when someone talks about you, and they are more or less accurate, we get concerned, angry, even mad at them and try to hide as soon as possible.   That’s a perfectly natural behavior, since we are afraid to get hurt by others if we’re too exposed, but it’s something that has to change, even more if the description isn’t done with malice. When we share with others how we are and why, we are making it easy for them to see us, understand us and in the end communicate. Trying to understand others and make it easy to understand us make us a better humans. Trying to find out what are the views of others and why and how they reached them it’s a much better start for a dialogue, instead of just fighting those views when they aren’t close to ours. Sharing what you are and how you see the world, it’s a great place to begin with.   Sharing your flaws doesn’t expose you to others, just make you human and a better and more hones person. No one is perfect1… Let’s try to find out about each other more often, so we can understand each other better, work better and live better.                  What is more… what is perfection? Perfection is a human concept that at the end of the day has nothing to do with reality. Some that for someone could be perfect, to others can be totally imperfect. The same for different situations. Someone can be just perfect for someone, but totally imperfect for other one. &#8617;           ","categories": ["Personal","Professional"],
        "tags": ["my personality","science","work"],
        "url": "https://luispuerto.net/blog/2019/01/20/analyzing-my-personality-bottom-line/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/16personalities.png"
},{
        "title": "What have I been doing these last years?",
        "excerpt":"Warning! This is a very long post and it’s probably boring for your. Read it at your own risk!   Since some people has asked me this question once in a while, I thought it was going to be a good idea to write a small1 post about what I’ve been doing these last years since I left Spain. I think I’m going to give it the shape of an informal resume / curriculum where I’m going to talk, superficially, about each of the things I’ve been doing.   First of all, I really want do differentiate between quality and quantity time. Quality time for me has, more or less, the meaning everyone gives to it on the English language.      Quality time (QT) is an informal reference to time spent with close family, partners or friends that is in some way important, special, productive or profitable. It is time that is set aside for paying full and undivided attention to the person or matter at hand. It may also refer to time spent performing some favorite activity.    I would add that, for me, it isn’t only that but also time that I spend on myself… doing whatever I enjoy and I think it’s going to be positive for me, even if it’s in the long run. They are also those kind of activities that are really difficult to quantify, but they stand out for their quality.   On the other hand, in my opinion there is the quantity time concept, of which I couldn’t find a formal definition, but I would define as all that other time that you can usually quantify —hours, days, weeks, months or years— doing something specific. Working, learning getting a diploma2, volunteering… This is usually the kind of things that people put in their curriculums because they can be undoubtedly quantified —they can be weighted— so other people usually can take them into account —and measure you in some way.   Quality vs. Quantity   Now, the important question. What do you prefer to have? Quality or quantity in your life? It’s pretty clear that if you could, you would like to have both. However, you can’t always have both.   You also have to understand that since I learned about Optimistic Nihilism3 philosophy, I think this is the correct way to understand the universe. However, as they say in the video please they it with a grain of salt we I don’t know any more about human existence that you do        If this is our one shot at life, there is no reason not to have fun and live as happy as possible. Bonus points if you make the life of other people better. More bonus points if you help build a Galactic Human Empire.                         It is like riding the bicycle of life, knowing that the road ends at an inevitable lethal fall, knowing that riding it on has no fundamental reason but you ride on, you ride on the road to enjoy the journey, to look around you and just feel like you’re the product of million of years of complexity, it’s like looking at the trees around you and saying, “Holy shit, I love this. This is freaking awesome.” Source: Quora              Quality Time   Enjoying my life   This is first and foremost important thing I’ve been doing, enjoying my life, and I think you should too! We are clearly here to enjoy our life, not to suffer. I’ve been doing all the things I explain in this post —and some others— and I’ve been enjoying them. No regrets. Perhaps some of the things haven’t been as productive, as good, or didn’t turn out as expected, however, in the end, all of them, have been positive because they’ve been part of this trip that is life, and I’ve learned from them.   Discovering &amp; developing myself   Some things I’ve been doing have been really positive for my personal development, I’ve been discovering part of myself that I didn’t know till now and I’ve been even discovering parts of my personality that I didn’t know I have. All of these, help me to be a better person, better professional and be more productive.   Learning   Part of that discovery and personal development are related to learning. Learning new skills, and learning that you can learn new skills. We are natural learners, but for some reason some people decide that they don’t want to learn anymore. They end up stuck in life.   Improving my English   All this time I’ve been improving my English since I’ve been living in an international environment and I’ve tried to travel as much I could —and still try. I’ve been living for almost 5 years in Finland, where almost all the time in my daily life I have to use English for communicating with others. Before coming to Finland I was living 6 months in Sweden where the same happened. During the period we’ve been living in Finland we moved for 5 month to the west coast of the United States where my English had an incredible boost. I tried to speak with people as much as possible —without weird anyone out— and one my fondest memories is talking and discussing with our landlord and housemate for really long hours about the most diverse topics: politics, life, science, philosophy, Europe, America, forestry, ecology, life…   Learning other languages   Since I live in Finland, I’ve been learning Finnish and I am able to speak a little bit. Not enough to maintain a normal and regular conversation for sure, but it has serve me well in a variety of situations when I’ve needed it. Learning Finnish, has been a really interesting experience. It’s in different language family, so you usually have to start from scratch since you aren’t going to have any reference or resemblance with your mother tongue —unless you are Estonian, Hungarian or part of the small Fino-Ugric family. In my particular case, my mother tongue is Spanish and pronunciation of Finnish isn’t a pain in the tongue, more the other way around, but grammar is totally different and really difficult. Besides of what painful has been learn the little Finnish I speak, it’s been a really change of perspective on thinking. Languages are a way of thinking and Finnish is a really particular one, usually really concise.   I also leaned a little bit of Swedish while I lived in Sweden —which was much easier than Finnish— but I’ve almost forgotten all of it. I liked Swedish because it’s a really nice language to listen, really rhythmic and even sometimes, funny.   Coding &amp; computer skills   All these years I’ve been improving my computer stills a lot. I’ve been introducing to coding, and now I’m able to code in R, and I’m beginning to code in other languages as Rubi, CSS, HTML or Liquid. Proof of that is this website that I’ve little by little changing from the original template to what you see it now to adjust to my necessities. I’ve even add new functionalities.   I’ve been also learn to use Git, and I use it almost everyday to keep track of my work. Also, I’ve increase incredible the use of the shell and the command line, where I’ve also write scripts from time to time when I need them, either to analyze LiDAR data using Fusion —or any other software— or to fulfill my necessities in macOS or Linux.   I’ve been learning about Linux too, since I have a couple of Raspberry Pis which I use for different tasks at home, like have a VPN server, or NAS, audio server, VPN router… or whatever it comes to my mind.   Reading   Reading is another way of learning about stuff, but usually in a more relaxing way —usually when you are close to go to bed. In the last years I’ve been able to reconcile with reading and I’ve been able to increase the number of books I’ve been reading little by little. Internet it’s great, but in my opinion sometimes, or most of the times, lacks the substance that books usually have. Returning to reading has improved my ability to concentrate, which was eroded by internet. In the last year, I as able to read read 7 books, but I’ve started the year strong and in the last month I read two.   I’ve been increasing my readings and I plan to increase even more since little by little I find the internet more boring everyday. Perhaps, I’m getting old, or perhaps is it has lost its nobility. Don’t get me wrong, I still spend a lot of time reading on the internet and it’s still my primary source of fresh information. Nevertheless, I started to despise Facebook —and all its ecosystem— and most of the other social networks. I still use Twitter quite a lot, and I’m a Reddit fan, but they lack features to create a really meaningful communication of ideas and it’s difficult to create quality content those platforms.   The Fifth Discipline   One of books I read these last years is “The Fifth Discipline”, an all-time classic of organizational management, and in some regards much more, since it can lead to a different view of the world. I discovered The Fifth Discipline thanks to the CEO of the last organization I was working in Spain. I really think it’s a great book and of course an interesting way to see organizations and humans interactions.   I begun to feel interest for management, organizational science and productivity also while I was in Spain. There I learned that you can have all the resources of the world to create or do whatever you want, but if you haven’t assembled a good and effective team, you aren’t going to succeed. Teams are made or people, not computers, office furniture, buildings, gadgets, or factories. Most managers doesn’t understand that people are the basic building blocks of their organization, so people is to what you have to play attention to. Humans aren’t a resource, but an investment, that in the end will really pay off. Senge really understand this and explain it much better than me in his books.   But, what is The Fifth Discipline? System Thinking, the ability of seeing the world in a systemic way. It integrated the other four: Personal mastery, Mental Models, Building a Shared Vision and Team Learning. I don’t have the time here to explain any of them, and I really don’t think it’s the place. If you want to know more, I really recommend to read any of the Senge’s books.   Enjoying my life with the person I love   This perhaps is going to sound corny, naïve, innocent or even foolish. I don’t know, but it’s what it’s. My partner and I met each other more than 15 years ago in a lost corner of the middle-south of Spain. For 10 years we maintain an on-distance relationship. It was a great relationship, with its good and bad things, but it was on distance. When I decided to move to Sweden, it was also because I wanted to start a new life with her, living under the same root. We got lucky and she got her dream position as a researcher in Finland and we decided to move here. I somehow wanted to enjoy and take back part of the time we were apart in Spain and try to share as much time as I could with her. We’ve been partners for life and I hope we will continue this way till the end of our lives. I’ve been really happy to be able to share all this time with her and travel together to the far side of the world. What an adventure!   Traveling and discovering the World   One of the things I’ve been also doing a lot is, traveling. No, I haven’t been doing tourism. I don’t like that, I don’t want to collect places on an album. I really want to have experiences and try to know the places I go as much as I can. Explore the terrain. I’ve been really blessed that I’ve been able to do it during these years.   The Nordics   As I’ve mentioned before, I’ve been living in Sweden and now I live in Finland, but I’ve in all the other Nordic Countries, but Iceland. When we moved to Finland and we knew that it was going to be permanent so we decided to move my old Volkswagen Golf from Spain to our place in Finland. We decided not to take any ferry and we crossed Denmark, Sweden from South to North and Finland from North to South till where we live.   Besides, we’ve been traveling all around the Nordics for business or leisure. We been in Lapland and in North Cape as well as the Lofototen islands and the North part of Norway, where we’ve been camping. We’ve camped also in Urho Kekonen,  Koli and Patvinsuo National Parks. And of course, we’ve hiked all around North Karelia and visited other parts of Finland. I even has witnessed a moose hunting on a weekend trip to the North of Finland.   Europe   As I mentioned previously, at some point we decided to move the my old Volkswagen Golf from Spain to Finland, so we drove all across Europe. It was a really wonderful —and tiring trip— in which we crossed, France, Belgium, Netherlands, Germany and part of the Nordics and watched Europe pass by our windows.   I’ve been also Visiting Vienna and Zurich lately, two cities that I really love. I really love Center Europe, its architecture and of course its people.   USA   In 2017 we lived for five months in the Pacific Northwest of the USA. More specifically in the city of Corvallis.   For us was a wonderful experience that allowed us to discover the USA, specially the West part. For me it wasn’t the first time in the USA, since I was living for a month in NYC in 2008 although you can argue that NYC isn’t exactly the USA.   Returning to Corvallis, we have the opportunity to discover America, or at least that part of America. We lived in a nice wooden old house close to the center of the city and had really interesting housemates, landlord and neighbors. Our neighborhood was like taken from one of those American movies of the 80s. Houses with lawns and streets full of trees.   We also bought a car and cruise America a little bit in our spare time. We visited several National and State parks, where we enjoyed the American wilderness, and drove the Oregon Coast up and down several times.  Just to mention some of the places we were… Florence Dunes, Redwoods National Park, Crater Lake National Park, Silver Falls State Park, McKenzie Pass… and of course we went East to visit Archers, Rocky Mountain, Grand Teton and Yellowstone National Parks. We saw past by our window the Eastern Oregon with its High Desert, the Utah desert, the Wyoming shrub steppe and we crossed the Rocky Montains. Crossing the Rocky Mountains was specially adventurous since we did it partly at night4. I will never forget the shadow of the Colorado river gorge agains the night ski while we rolled up the highway with the long semis overpassing us and the opposite lanes almost overlay one over the other.   Coming across of interesting people   Not everything while traveling has been just being mesmerized by wonderful landscapes, of course. I been doing something even more important, meeting people along the way. Some of them wonderful and some of them less wonderful. With some of them I had a long relation, and with others I just exchange few words. With some of them I still keep contact, and some of them are just memories. Some of them were natives, some of them were fellow travelers.   Meeting people is what makes traveling fun. Without people, places would be just empty and people is what make places wonderful or horrible. Talking with people is what make you see things differently and what really tell you the story of the place. From our trip to the USA we keep pretty fond memories of people, who usually were pretty social, and we are still in contact with most of the ones we interacted in our daily lives.   Quantity time   Learning Finnish   When I first arrived to Finland I started to attend to the Finnish Intensive courses the unemployment office provided as a way to learn the language and the culture. I was attending to those courses for a total of one year and a half during the first 3 years. Each course was around 500 hours during five or six months and they included language internships in a Finnish company or organization.   I have to say that I actually did learn Finnish —A 2.1 level— but not all the Finnish I wanted or I would like to have learned for the amount of time and effort in invested. However, there was a bright part on it, I meet people, I knew about their lives, the reasons why they ended up here, and and share quite interesting experiences with all of them.   Would I repeat those courses? no! but I have to say that the experience was worth it.   Master   Since Autumn 2013 I’ve been enrolled in the iGEON online Master of Science and I was coursing it normally until I arrived to Finland. Then, I started with the Finnish courses that absorbed most of the time, my energy and my morale. Little by little I left it aside… because I wanted to do other things, like learning on my own, doing an internship to get some hands-on-knowledge, traveling or just enjoy some time with my loved ones and taking care of them. In other words, I got stuck for a while and my progress halted.   However, I’m resuming it right now again and I plan to finish it on 2020 or earlier if I can.   Learning on my own   I’m a really curious person and I like learning as a way to improve myself. During these years I’ve been learning and improving mostly about computers and programing, but also about other things. For example, how to bake bread and pizza as it should be.   In the computer science department I’ve been mostly improving my skills scripting in environments like Unix shell or Windows Batch. I also learned how to program in R and some statistics on my own, and I continue doing it and wanting to improve my coding. I’ve learned how to use Git and I learned how to create static websites with Jekyll, using HTML, CSS &amp; Sass, Liquid templating, Continuous Integration and some Ruby and JavaScript.. I’ve been also toying with Raspberries and using them as a home servers for R Studio server, VPN server, NAS, cloud storage, wireless audio streaming, ad blocker, internet speed logger and many others.   Internships   I’ve done a total of four internships, three of them related to my Finnish language courses and the last one was just a 6 month internship sponsored by the employment office.   Metsäkeskus   On the internship of my first Finnish language course I decided to go to Metsäkeskus so I could learn about Finnish forest and practice some Finnish at the same time.   The experience was really positive and I keep really fond memories of the people working there that I was able to interact with my little Finnish. I was lucky and my internship supervisor spoke English, as well as the director of the office, so we could workout something productive.   I visit the field a couple of times with my supervisor and other people from the office and I could witness on first hand how Finnish forest was managed. I also learned how the GIS of Metsäkeskus worked, how it was totally interrelated with the forest inventory in real time and how the customers could access to the data to take decisions about their own Forest.   Part of my duties there were create a online map with the information from the Metsäkeskus’ forest inventory data, which was all in Finnish and I have to translate and understand on my own. I processed and analyzed all the data and lately we published it on the Metsäkeskus site, where it was for a while.   UEF   From my second language course onwards, I decided carry on my internships at University of Eastern of Finland, where I could learn about LiDAR and how to process it. I had some contact with LiDAR before but I never ever processed it that intensively.   I download and analyzed the LiDAR data of the whole Finland using Windows Batch Scripts and the Fusion software from the US Forest Service. Then, I storage and manage the outcome in a geodatabases and it could be accessed much faster than in its raw format. I also analyze LiDAR data using R and the LidR package, using parallelization processing, to yield biomass estimation form the LiDAR data.   Researching   One of the outcomes of the internships at the UEF was a scientific publication in the iForest journal that I actively helped to write and soon will be published since it’s been already accepted. During the writing process I learned how to read and write science and how hard is researching and learn to do it.   Researching is a career that I’ve been toying with a little bit. I am very excited about the possibility of continue doing research and to further developed research related activities. I’ve been always curious about science and technology and I think I have a rational and analytical mind that could server me well in in scientific activities. However, I have a too much respect for science and I don’t like the stiffness of academia, but I don’t think both things could be a problem.   Coaching   During these last years I’ve been also “coaching” other people. When I say coaching I mean mainly giving advice —when I’ve been asked for it— professional or personal one. Please, don’t think that I’m that arrogant and condescending to think my advice is better than anyone else’s that I’ve been giving it “freely” all around to everyone. I usually don’t mind anyone business, but mine. Howeer, when I see my close ones troubled I usually ask what is going on and if I think can share some kind of wisdom or experience I have that can help, there I go.   My working experience has served me well in this field since I worked in high stressful environments that allowed me realize of the importance of the team and that learn how to manage people in your team it will really make the difference. This made me really interested in the topic and I’ve tried to learn as much as I could. I’m proud to say that I still keep a really good relation with all my previous team-mates, that were over, under or lateral to me, and that some of them became friends later on. Some of them told me they felt that I wasn’t there to command but to help them to fulfill their job, which I tried to do as much I could.   On this matter, I tried to support my wife as much as I possible in her endeavor of get a PhD and more than once, specially towards the end, I gave her a pep talk5 to boost her up as much as possible.   Volunteering   I feel that the idea and purpose of volunteering is quite different between Europe and America, mainly North America, or at less that is my perspective being Spanish and living in Finland. I feel that almost every North American has volunteering in something at some point of their live, while in Europe is something more rare, or that at least is more related to political activism —like conservationism, ecology, or something similar— than to general volunteering. Almost none of the people is close to me and live in Spain has volunteering to anything at any given point of their lives. I guess that it’s related to what is socially expected and to the different perspective about the State and what is its role.   Anyhow, I’ve been doing some volunteering during these last years, but probably not as much for the North American standards.   Cultural integration volunteering   During the first years we lived in Joensuu a friend of us created a small group of people —sort of a club or association— to help to develop a little bit the cultural life of Joensuu. We meet regularly in one of the rooms of the regional museum and developed some activities together like art expositions, international food activities or photo sessions with locals and internationals to try to tell a little bit their stories. We also developed some cultural activities for refugees and their children, like an exposition of the children’s drawings about war, so we could create some awareness about what was going on in Syria.   Teaching to code   My wife and I organized an activity to teach code to kids on fifth grade on the CEIP José Calvo Sotelo in Madrid inside of the Hour of Code initiative. It was really interesting to see how kids were really interesting in computers, beyond playing computer games, and how they can think out of the box to solve problems in different ways. It was a really boost in creativity.   Carousel   It’s funny how to you can end up in the most interesting places sometimes just by pure chance. While we were living in the US I ended up helping to restore an old carousel in a town close to the one we were living and it was one of the most positive experiences in my life.   Before we traveled to the US West Coast, we were looking for a place to live and we ended up contacted by a nice couple because they perhaps had some place for us to stay. It didn’t workout —because their place was too far away of the center of our activities and we didn’t planned to have a car— but we ended up befriending.  They offered us a lot of support and help during all our stay there and they are one of the best people we came across in our lives. It’s funny because the main reason they contacted us is because we were living in Finland. She has a Finnish ancestry and they were curios about Finland.   Anyhow, he was volunteering restoring this old carousel and they asked me if I wanted to go with him. Right away. I came across a really nice bunch of old school people that were carrying out a really complex engineering project, just for fun! I couldn’t ask for more. It was great to help them to install the floor, review the mechanics, install the animals, or just to put some bulbs in the carrousel. However, the best was to be in contact with them and know a little bit of their stories. They were living in the surroundings all their lives and they were part of the 50s and 60s generation that witnessed America’s splendor. They were really kind to me and, even when I arrived in the last stages of the project, they didn’t hesitate to put my name with theirs hidden in one the beans of the carrousel, like any other member of the team. I even appear in a PBS documentary about the restoration and in some photos in the local newspaper about the topic. I think I was exotic.   Entanglement   Of course I don’t have my life that extremely compartmentalized. All of these things and much more have been and are entangle… mixed together as in any other life. Lives are organic and usually their parts can’t be put in different boxes because they end up dying. Of course you can dissect lives, like any other system, to understand them better, but they are a whole and they need to be understood that way. Looking at just parts doesn’t make sense.      When you split an elephant in two, you do not have two small elephants which you can take care of…    Is all of these just bull**it?   Perhaps… or perhaps not. What it’s true is we are usually much more of that we manage to show to the rest of the people, for good or bad. I wanted to be a little bit more transparent and show that although curriculums could be good tools, but most times they left out of the picture a lot of what a person is. That is problematic. Anyway, no tool is perfect and even this post is just part of the story.                  Both of us know that this post isn’t going to be small. &#8617;                  I don’t think that getting a diploma —going to the university to get some degree of formal education— fully equated to learning and gather knowledge. I know a lot of people that has several diplomas, even PhDs but they barely know anything. &#8617;                  It’s pretty clear Optimistic Nihilism isn’t a real philosophical term but a casual lingo. But perhaps you can learn a little bit more about the term in this answer in Quora. &#8617;                  The original idea was to camp by the Colorado close to to Arches National Park, but it was too hot. &#8617;                  A pep talk is: Informal A speech of exhortation, as to a team or staff, meant to instill enthusiasm or bolster morale. Source. But could be also this. &#8617;           ","categories": ["Personal","Professional"],
        "tags": ["life","developing"],
        "url": "https://luispuerto.net/blog/2019/02/03/what-Ive-been-doing-these-last-years/",
        "teaser":"https://luispuerto.net/assets/images/blog/2018/existential-nihilism-header.jpeg"
},{
        "title": "Discovering Hypothes.is",
        "excerpt":"I just discovered Hypothes.is which is a service to annotate and highlight articles, posts, pdfs or whatever text you want and find all over the web and in any site. In their own words:      The Hypothesis Project is a new effort to implement an old idea: A conversation layer over the entire web that works everywhere, without needing implementation by any underlying site.    All their tools are open source and free, and they can be checked here, which make the services even more attractive to me. They’re also a non-profict organization, if you are wondering if there any comercial interest in the tools they’re developing. In addition, they provide outline.com, which is similar to hypothes.is but transforming the article in a readable version without all the clutter websites usually have.   You can start using them right away, creating yourself an account in hypothes.is and firing up either hypothesi.is or outline in any web you want using the lightweight bookmarklets to share your annotations and highlights with others. You can find a outline bookmarklet below. Please note… when you are highlighting and annotating in outline, you are just annotating there. Your comments aren’t going to be share in the main article.                        hypothes.is highlighting and annotation tool              The tool is really easy to use, you just click in the bookmarklet and small buttons in the side pops up.  Then, you just select text and a pop up will ask you if you want to highlight or annotate. You can also view the comments, notes and highlights from others users of the tool, which is even more interesting, and you can also comment in those annotation and reply to other annotations. Finally, it seems that it’s possible to create groups, so you can share your notes with others privately or publicly.   As you can see in the image below, outline do exactly the same, but in a clean interface that helps you to read the article without distractions.                        outline interface with hypothes.is highlights              Web developers can also implement this tool in their webs, so when users select text the annotation and highlight tool pops up in a really medium-like way without the need to trigger it using the bookmarklet. I was thinking to implement it in my site, and I even was toying with it in a branch, but I finally decided to leave you the option of using it or not through the bookmarklet. Mainly for two reasons:      First of all. You choose. I don’t want to impose anything into any one.   Second of all. I think the implementation is too intrusive to have it all the time there. You can implement it in two ways, classic or clean.            The classic way makes the sidebar always present through its  buttons and somehow interfered with the design of the site. Mostly on mobile.       The clean way doesn’t show you anything until you select text, but them you are missing some features such us: marks of where the annotations are, or the ability to turn the annotations on and off. This wouldn’t bee a problem if users can bring all the features triggering the bookmarklet, but it doesn’t work like that.           Perhaps in the future I will integrate it in someway if they improve how it blend with webs or if they for example develop something to transform the notes into comments.   I really like the idea of being able to have a layer of notes and highlights all across the web. It’s a great idea. I hope that the idea evolved in a sort of organization like the Wikimedia Foundation and it becomes a standard all across the web. I really think it’s a great tool to share information with others, ever more if it’s related to research and science or education.  I encourage you to use the tool and share your notes with others.   ","categories": ["Professional","Technology"],
        "tags": ["research","tools"],
        "url": "https://luispuerto.net/blog/2019/02/10/discovering-hypothesis/",
        "teaser":"https://luispuerto.net/assets/images/blog/2019/HypothesisBannerGoogleForms.png"
},{
        "title": "No message, no connection in LinkedIn",
        "excerpt":"I think this is something that almost anyone with a LinkedIn account has suffered or happened to them… an invitation to connect that comes out of the blue from someone you don’t know. No message, or just the standard message, no explanation, nothing… just an invitation in your LinkedIn inbox from someone, usually with a happy face —maybe a fake one— that perhaps is in a similar field as yours —or not— and they live close to you —or they don’t. I hate it, because I need to figure out who this person is and what they want.   What are they? Are they bots? Are they mining personal information? Are they just people that can figure out how social interactions should work? Did they click in my profile by mistake? What is going on here? A really don’t understand :confused:.                        Simply no words              Ok… after a brief search on internet about the topic, now I understand… They are LIONs1 —LinkedIn Open Networkers— in other words they are people which only goal is to grow their LinkedIn’s network —to huge numbers— so it’s easier for them to connect to other people really interesting for them in the future and appear in more people’s searches. LinkedIn only allow you to connect to other people when you are up to 3 degrees of separation from that person and the same for searchers, you only show up when you are up to 3 degrees of separation away. So when you have a huge network you have more probability to be at 3 or less degrees of separation those contact you really want to connect with. You can find longest explanation here and here.   Don’t get me wrong. I’m nothing special, just a professional looking for opportunities and someone who is, of course, really glad to meet and connect with other people. I think LinkedIn could be a great place to do it. The problem is that you just don’t poke, or connect, people randomly without knowing anything about them, without telling them why do you want to know more about them and/or telling a little bit about you. If you sent me a message —in or out LinkedIn— telling me why you want to connect with me —just a line, or a couple if you want— then I will be more than happy to connect with you. On the condition that you have a real and meaningful reason you want to connect. If you send me a standard message or you tell me that you just want to connect, it has the same effect as it you tell me nothing.   Can you imagine that you are in a bar, a conference or just in the street and you decide to give your business card to random people and passersby? With no reason at all? Telling to all of them the same artificial phrase without trying to learn anything about them. Would you take their card? Would you keep it? No I guess no, because it doesn’t have any sense unless you are human billboard and giving handouts to everyone without any filter, which most probably will end up in the garbage. Aren’t you a seasoned professional that you really want to stand out other the rest? Don’t you really want to look professional?                        C’mon guys!!!! The option to “add a note” to that connection request is just there              On the other hand, just imagine for a second. You are in the same bar, conference or just in the street (in some event or something) and someone approach you, introduce himself, tell you that knows you from XYZ, and they would love to know more about your work and they hand you their card. I’m sure your perspective changes a lot about that person and you are provably going to remember them.                        This is the problem, the LinkedIn recommendations panel              The problem seems to be related to the Recommended for you panel on My Network section. If you click on one of those connect buttons, it seems that a direct invitation to connect is sent to that profile, without asking you if you want to add a note —yes please! I’m wondering now if this isn’t a bug, but a feature. In other words, does Microsoft want you to have as much connections as possible? What for?   I don’t care what are the current LinkedIn dynamics, or if they have changed since Microsoft has bought it. In my opinion it’s plainly wrong to send invitations to connect with people you don’t know out of the blue and without any message. Instead of just clicking on that connect button in My Network section, you can check the LinkedIn’s profile of the person is suggested to you and in their profile you can click on connect —if you consider the person is worth to connect with— and add a note to your invitation to connect —YAY! :thumbsup:. Even if I know you beforehand, in real life or just because we’ve been exchanging emails —or whatever— you should add a note to refresh my memory or send a message outside of LinkedIn. Memories fail! :thinking:   As this guy states, if you don’t add a message to your invitation you are showing:      That you are indistinguishable of all the other invitations.   You are disingenuous, that I’m going to include you in my network just because you send me an invitation.   You lack of creativity and you are lazy since you decided not to even check my profile —and contact me from there, either in LinkedIn using a note to your invitation or using my contact info— and if you have done so, you decided explicitly not to write a note :thumbsdown:.    The problem is worsen by LinkedIn, since you can’t ask to someone that is asking you to connect with exactly why they want to connect with you, since that is a premium :money_mouth_face: feature to send messages to people outside your network. So people don’t want to pay, which is normal with that prices, and just try to connect with others in the free tier. Perhaps LinkedIn should allow to at least reply to those invitations with a message asking for more info.   I know that sending a message, check the profile or research a little bit sometimes is a lot of effort, but I really think it’s worth it. I have this web, where you can find my contact information and contact me in several different ways. I also have my contact information in my LinkedIn profile. There is no excuse to send me a message telling me who you are and why are you interested in me.   NO MORE RANDOM INVITATIONS, please!                  To be honest, and after giving it a thought, I don’t think they are even that, because I bet an arm that most of them don’t know what a LION is. I didn’t know until I wrote this piece and I doubt most people in LinkedIn know about that. They just connect because they know they are going to get something on the long run or just because LinkedIn is suggesting them to do so. They just click in connect under your profile. &#8617;           ","categories": ["Professional","Technology"],
        "tags": ["LinkedIn","social media","networking"],
        "url": "https://luispuerto.net/blog/2019/02/21/no-message-no-connection-in-linkedin/",
        "teaser":"https://luispuerto.net/assets/images/blog/2019/linkedin-logo-header.png"
}]
